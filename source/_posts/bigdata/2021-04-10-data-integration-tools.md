---
layout: post
title: Data Integration
categories: [big data]
description: 
keywords: data integration
catalog: true
multilingual: false
tags: big data
date: 2021-04-10
---


## 数据集成简介
企业总是有从各个系统里集成数据的需求, 数据集成就是一门技术用来提供企业层面的统一和一致的数据视图. 数据集成的目的是维护数据源整体上的数据一致性, 解决企业数据孤岛问题, 提高信息共享和利用的效率. 数据集成的核心任务是将有关联的异构数据源集成到一起, 使用户能够透明地方式访问这些数据.

## 数据集成的分类
从集成方式的角度来分, 数据集成可分为:
- 点对点集成
- 总线式集成
- 离线批量集成
- 流式数据集成

### 点对点数据集成
点对点一般采用接口方式对接, 适合连接对象比较少的情况, 具有开发周期短, 技术难度低的优势. 问题是当连接对象变多, 连接路径将会指数级增长, 效率和维护成本会变大. 主要的维护成本在于不能集中管理和监控接口服务, 如果交换协议不一致将会遇到开发困难.
点对点的集成是紧耦合的，当一个连接变化时，所有与其相关的接口程序都需要重新开发或调试。

### 总线式数据集成
总线式数据集成是通过在中间件上定义和执行集成规则，其拓扑结构不再是点对点集成形成的无规则网状，而主要是中心辐射型的（Hub型）星型结构或总线结构.
总线结构通过与点对点集成架构相比，采用总线架构可以显著减少编写的专用集成代码量，提升了集成接口的可管理性。不同连接对象如果连接方式有差异，可以通过总线完全屏蔽掉，做到对连接对象透明，无需各个连接对象关心。通过总线结构，把原来复杂的网状结构变成简单的星形结构，极大提高了硬件的可靠性和可用性。
- 第一代总线集成工具: EDI电子数据交换系统
企业直接按照通用的消息格式发送信息，接收方也需要按统一规定的语法规则，对消息进行处理，并引起其他相关系统的EDI综合处理。一般EDI用于企业间交换交易凭证和发票等数据, 是无纸化办公的实现.

- 第二代总线集成工具: ESB企业服务总线
ESB的使用标志着企业的应用集成进入了SOA时代（SOA是一种面向服务的集成架构）。SOA架构的其主要特征是基于一系列Web标准或规范来开发接口程序，包括UDDI、SOAP、WSDL、XML、REST，并采用支持这些规范的中间件产品作为集成平台，从而实现了一种开放而富有弹性的应用集成方式。
ESB是对web服务（WebService）的注册、编排和管理。
WebService是一种跨编程语言、跨操作系统平台的远程调用技术，是web的一种标准。可以理解为：WebService是一个应用程序向外界暴露了一个能通过Web调用的API接口，我们把调用这个WebService的应用程序称作客户端，把提供这个WebService的应用程序称作服务端。客户端进行服务的远程调用前，需要知道服务的地址与服务有什么方法可以调用。
因此，WebService服务端通过一个文件（WSDL）来说明自己家里有啥服务可以对外调用，服务是什么，服务中有哪些方法，方法输入的参数是什么，返回值是什么，服务的网络地址是什么，通过什么方式来调用等。
WSDL是一个基于XML的语言，用于描述WebService及其函数、参数和返回值，它是WebService客户端和服务器端都能理解的标准格式。

### 离线批量数据集成
在传统数据集成的语境下，离线批量数据集成，通常是指基于ETL工具的离线数据集成，ETL即数据的提取（Extract)、转换(Transform)和加载(Load)。
ETL的实现有多种方法，常用的有三种：
  - 第一种是借助ETL工具：例如：Informatic、IBM CDC、talend、kettle、Nifi等，借助工具可以快速的建立起ETL工程，屏蔽了复杂的编码任务，提高了速度，降低了难度，但是缺少灵活性。
  - 第二种是SQL编码实现：SQL的方法优点是灵活，提高ETL运行效率，但是编码复杂，对技术要求比较高。
  - 第三种是ETL工具和SQL组合实现：综合了前面二种的优点，会极大地提高ETL的开发速度和效率。

### 流程数据集成
流式数据集成也叫流式数据实时数据处理，通常是采用Flume、Kafka等流式数据处理工具对数据库进行实时监控和复制，然后根据业务场景做对应的处理（例如去重、去噪、中间计算等），之后再写入到对应的数据存储中。

### 网络数据集成
网络数据集成也叫网络数据采集，指通过网络爬虫或网站公开API等方式从网站上获取数据信息的过程。
网页爬虫，即一种按照一定的规则，自动地抓取互联网信息的程序或者脚本，一般分为通用网络爬虫和聚焦网络爬虫两种。网页爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。
聚焦爬虫的工作流程较为复杂，需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接并将其放入等待抓取的URL队列。
网页爬虫支持文本文件、图片、音频、视频等非结构化数据、半结构化数据从网页中提取出来，存储在本地的存储系统中。
目前网络上有许多网页爬虫，Octoparse、WebCopy、HTTrack、Getleft、Scraper等.

### 表格总结

|-----------------+------------+-----------------+----------------|
| 方式 | 优点 | 缺点  | 备注  |
|-----------------|:-----------|---------------|---------------|
| 点对点 | 1.开发周期短 2.技术难度低 | 1.维护成本高 2.紧耦合 3.接口协议局限 | 基本不选择 |
| 总线式 | 1.集中管理  2.多协议 3. 解耦和结构星型化 4.可声明 | 1.存在单点问题,需要高可用 2.开发成本比点对点高 | 适合API类型的数据集成 |
| 离线批量 | 1.吞吐量大 2.可自定义数据模型 3.可集成数仓 | 1.需要一定编码能力 2.数据有延迟 3.ETL一般只支持关系型数据 | 适合数仓系统 |
| 流式数据集成 | 1.实时 2.吞吐量达每秒百MB 3.支持NoSQL | 1.架构较复杂 2.需要devops能力 |一般使用Kafka |
|-----------------+------------+-----------------+----------------|