---
layout: post
title: 集成学习
categories: [machine learning]
description: 集成学习总结
keywords: machine learning
catalog: true
multilingual: false
tags: ML, ensemble
date: 2022-11-07
---


## 集成学习的本质
集成学习的本质是集众之长得出一个更好的结果。专业术语来讲是集成多个单独的`classifier`(一般是弱学习者`weak learner`)来获得一个更优的`classifier`.
集成学习最大的好处是可以解决高偏差和方差问题。采用多个分类器可以增加模型的复杂性来解决方差问题，而只要分类器足够不相干，统计规律告诉我们这样可以减少偏差。

### 偏差和方差的识别
一般模型的`error = Bias**2 + Variance + inreducible error`, 实际应用中我们一般使用学习曲线和验证曲线来评估他们。`validation curve`是不同超参数下，算法可以达到的性能。在每个超参数下，我们使用`K-fold`交叉验证，存储样本内的性能和样本外的性能，然后计算并画出样本内和样本外的性能的平均和标准差值。最后通过比较相对和觉得的性能，我们可以计算出偏差和方差的大小。


### 集成学习的关键---足够多样的分类器
基本上有两种方式可以得到足够不同的分类器，一种是使用不同的算法，另一种是使用相同的算法在随机子集。


### Bagging
