<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="spark, pandas, arrow">
    <meta name="description" content="官方文档解读">
    <meta name="author" content="Kopei">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-redefine.png">
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://kopei.github.io/2020/07/22/bigdata-2020-07-23-pyspark-for-pandas/"/>
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-logo.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-logo.svg">
    <meta name="theme-color" content="#005080">
    <link rel="shortcut icon" href="../../../../images/redefine-logo.svg">
    
    <title>
        
            Pandas UDF and Function Api in Spark |
        
        Kopei&#39;s Home
    </title>
    
<link rel="stylesheet" href="css/style.css">

    <link rel="stylesheet" href="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/css/fonts.css">
    
    <link rel="preconnect" href="https://evan.beee.top" crossorigin>
    <script id="hexo-configurations">
    let REDEFINE = window.REDEFINE || {};
    REDEFINE.hexo_config = {"hostname":"kopei.github.io","root":"/","language":"en","path":"search.xml"};
    REDEFINE.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#005080","avatar":"/images/redefine-avatar.svg","favicon":"/images/redefine-logo.svg","article_img_align":"center","right_side_width":"200px","content_max_width":"1000px","nav_color":{"left":"#f78736","right":"#367df7","transparency":35},"hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_image":{"light":"https://evan.beee.top/img/wallhaven-wqery6-light.webp","dark":"https://evan.beee.top/img/wallhaven-wqery6-dark.webp"},"title_color":{"light":"#fff","dark":"#d1d1b6"},"description":"两仪立焉 朋来无咎"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_block":{"copy":true,"style":"mac"},"pjax":{"enable":true},"lazyload":{"enable":true},"version":"1.1.0","friend_links":{"columns":2}};
    REDEFINE.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
    
    <link rel="stylesheet" href="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/css/fontawesome.min.css">
    <link rel="stylesheet" href="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/css/brands.min.css">
    <link rel="stylesheet" href="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/css/solid.min.css">
    <link rel="stylesheet" href="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/css/regular.min.css">
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="menu-wrapper">
    
    <div class="menu-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Kopei&#39;s Home
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="pc">
                <ul class="menu-list">
                    
                        
                            <li class="menu-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="../../../../index.html"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="menu-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="../../../../archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        ARCHIVES
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="menu-drawer">
        <ul class="drawer-menu-list">
            
                
                    <li class="drawer-menu-item flex-center">
                        <a class="" 
                        href="../../../../index.html"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-menu-item flex-center">
                        <a class="" 
                        href="../../../../archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                ARCHIVES
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">Pandas UDF and Function Api in Spark</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="../../../../images/redefine-avatar.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Kopei</span>
                            
                                <span class="author-label">article</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="pc">2020-07-23 00:00:00</span>
        <span class="mobile">2020-07-23 00:00</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="../../../../categories/big-data/">big data</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="../../../../tags/big-data/">big data</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h2 id="Apache-Arrow-in-PySpark"><a href="#Apache-Arrow-in-PySpark" class="headerlink" title="Apache Arrow in PySpark"></a>Apache Arrow in PySpark</h2><p>Spark可以使用<code>Apache Arrow</code>对python和jvm之间的数据进行传输， 这样会比默认传输方式更加高效。<br>为了能高效地利用特性和保障兼容性，使用的时候可能需要一点点修改或者配置。</p>
<h2 id="为什么使用Arrow作为数据交换中介能够提升性能？"><a href="#为什么使用Arrow作为数据交换中介能够提升性能？" class="headerlink" title="为什么使用Arrow作为数据交换中介能够提升性能？"></a>为什么使用Arrow作为数据交换中介能够提升性能？</h2><p>普通的python udf需要经过如下步骤来和jvm交互：</p>
<ul>
<li>jvm中一条数据序列化</li>
<li>序列化的数据发送到python进程</li>
<li>记录被python反序列化</li>
<li>记录被python处理</li>
<li>结果被python序列化</li>
<li>结果被发送到jvm</li>
<li>jvm反序列化并存储结果到dataframe</li>
</ul>
<p>所以python udf会比java和scala原生的udf慢。<br>但是使用pandas udf可以克服数据传输中需要的序列化问题，关键是使用了Arrow. spark使用arrow把JVM中的Dataframe转为可共享的buffer, 然后python也可以把这块共享buffer作为pandas的dataframe, 所以python可以直接在共享内存上操作。<br>以上，我们总结一下，使用arrow主要有两个好处：</p>
<ol>
<li>因为直接使用了共享内存，不在需要python和jvm序列化和反序列化数据。</li>
<li>pandas有很多使用c实现的方法， 可以直接使用。</li>
</ol>
<h2 id="Spark-DataFrame和Pandas-DataFrame的转化"><a href="#Spark-DataFrame和Pandas-DataFrame的转化" class="headerlink" title="Spark DataFrame和Pandas DataFrame的转化"></a>Spark DataFrame和Pandas DataFrame的转化</h2><p>首先需要配置spark, 设置<code>spark.sql.execution.arrow.pyspark.enabled</code>, 默认这个选项是不打开的。<br>还可以开启<code>spark.sql.execution.arrow.pyspark.fallback.enabled</code>来避免如果没有安装<code>Arrow</code>或者其它相关错误。<br>Spark可以使用<code>toPandas()</code>方法转化为Pandas DataFrame; 而使用<code>createDataFrame(pandas_df)</code>把Pandas DataFrame转为Spark DataFrame.</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">spark.conf.set(&#x27;spark.sql.execution.arrow.pyspark.enabled&#x27;, &#x27;true&#x27;)</span><br><span class="line"></span><br><span class="line">pdf = pd.DataFrame(np.random.rand(100,3))</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(pdf)</span><br><span class="line"></span><br><span class="line"># 使用arrow把spark df转化为pandas df</span><br><span class="line">result_pdf = df.select(&quot;*&quot;).toPandas()</span><br></pre></td></tr></table></figure></div>

<h2 id="Pandas-UDF-矢量UDF"><a href="#Pandas-UDF-矢量UDF" class="headerlink" title="Pandas UDF(矢量UDF)"></a>Pandas UDF(矢量UDF)</h2><p><code>Pandas UDF</code>是用户定义的函数， Spark是用arrow传输数据并用pandas来运行<code>pandas UDF</code>， <code>pandas UDF</code>使用向量计算，相比于旧版本的<code>row-at-a-time</code>python udf, 最多增加100倍的性能. 使用<code>pandas_udf</code>修饰器装饰函数，就可以定义一个<code>pandas UDF</code>.对spark来说，UDF就是一个普通的pyspark函数。<br>从spark3.0开始， 推荐使用python类型(<code>type hint</code>)来定义pandas udf.<br>定义类型的时候，<code>StructType</code>需要使用<code>pandas.DataFrame</code>类型， 其他一律使用<code>pandas.Series</code>类型。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from pyspark.sql.functions import pandas_udf</span><br><span class="line"></span><br><span class="line">@pandas_udf(&quot;col1 string, col2 long&quot;)</span><br><span class="line">def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -&gt; pd.DataFrame:</span><br><span class="line">  s3[&#x27;col2&#x27;] = s1+s2.str.len()</span><br><span class="line">  s3[&#x27;col1&#x27;] = &#x27;sss&#x27;</span><br><span class="line">  return s3</span><br><span class="line">  </span><br><span class="line">df = spark.createDataFrame(</span><br><span class="line">      [[1, &quot;a string&quot;, (&quot;a nested string&quot;,)]],</span><br><span class="line">      &quot;long_col long, string_col string, struct_col struct&lt;col1:string&gt;&quot;)</span><br><span class="line">      </span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line">df.select(func(&quot;long_col&quot;, &quot;string_col&quot;, &quot;struct_col&quot;)).printSchema()</span><br><span class="line"></span><br><span class="line">df.select(func(&quot;long_col&quot;, &quot;string_col&quot;, &quot;struct_col&quot;)).show()     </span><br><span class="line">+--------------------------------------+</span><br><span class="line">|func(long_col, string_col, struct_col)|</span><br><span class="line">+--------------------------------------+</span><br><span class="line">|                              [sss, 9]|</span><br><span class="line">+--------------------------------------+</span><br></pre></td></tr></table></figure></div>

<h3 id="Series-to-Series-类型的UDF"><a href="#Series-to-Series-类型的UDF" class="headerlink" title="Series to Series 类型的UDF"></a>Series to Series 类型的UDF</h3><p>当类型提示可以被表达为<code>pandas.Series -&gt; pandas.Series</code>时，称为<code>Series to Series</code>UDF<br>这种类型的<code>pandas UDF</code>的输入和输出必须要有相同的长度， PySpark会把数据按列分成多个batch, 然后对每个batch运行<code>pandas UDF</code>, 然后组合各自的结果。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import pandas as pd</span><br><span class="line">&gt;&gt;&gt; from pyspark.sql.functions import col, pandas_udf</span><br><span class="line">&gt;&gt;&gt; from pyspark.sql.types import LongType</span><br><span class="line">&gt;&gt;&gt; def multiply_func(a: pd.Series, b:pd.Series) -&gt; pd.Series:</span><br><span class="line">...     return a*b</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; multiply = pandas_udf(multiply_func, returnType=LongType())</span><br><span class="line">&gt;&gt;&gt; x = pd.Series([1,3,4,5])</span><br><span class="line">&gt;&gt;&gt; df = spark.createDataFrame(pd.DataFrame(x, columns=[&#x27;x&#x27;]))</span><br><span class="line">&gt;&gt;&gt; df.select(multiply(col(&#x27;x&#x27;),col(&#x27;x&#x27;))).show()</span><br><span class="line">+-------------------+</span><br><span class="line">|multiply_func(x, x)|</span><br><span class="line">+-------------------+</span><br><span class="line">|                  1|</span><br><span class="line">|                  9|</span><br><span class="line">|                 16|</span><br><span class="line">|                 25|</span><br><span class="line">+-------------------+</span><br></pre></td></tr></table></figure></div>

<h3 id="Series迭代器-gt-Series迭代器-类型的UDF"><a href="#Series迭代器-gt-Series迭代器-类型的UDF" class="headerlink" title="Series迭代器 -&gt; Series迭代器 类型的UDF"></a>Series迭代器 -&gt; Series迭代器 类型的UDF</h3><p>当类型提示可以被表达为<code>Iterator[pandas.Series] -&gt; Iterator[pandas.Series]</code>时，称为<code>Iterator[Series] to Iterator[Series]</code>UDF.</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from typing import Iterator</span><br><span class="line">import pandas as pd</span><br><span class="line">from pyspark.sql.functions import pandas_udf</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; pdf = pd.DataFrame([1,2,3], columns=[&#x27;x&#x27;])</span><br><span class="line">&gt;&gt;&gt; df = spark.createDataFrame(pdf)</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">DataFrame[x: bigint]</span><br><span class="line">&gt;&gt;&gt; @pandas_udf(&#x27;long&#x27;)</span><br><span class="line">... def plus_one(iterator: Iterator[pd.Series]) -&gt; Iterator[pd.Series]:</span><br><span class="line">...     for x in iterator:</span><br><span class="line">...             yield x+1</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; df.select(plus_one(&#x27;x&#x27;)).show()</span><br><span class="line">+-----------+</span><br><span class="line">|plus_one(x)|</span><br><span class="line">+-----------+</span><br><span class="line">|          2|</span><br><span class="line">|          3|</span><br><span class="line">|          4|</span><br><span class="line">+-----------+</span><br></pre></td></tr></table></figure></div>

<h3 id="多个Series迭代器-gt-Series迭代器-类型的UDF"><a href="#多个Series迭代器-gt-Series迭代器-类型的UDF" class="headerlink" title="多个Series迭代器 -&gt; Series迭代器 类型的UDF"></a>多个Series迭代器 -&gt; Series迭代器 类型的UDF</h3><p>当类型提示可以被表达为<code>Iterator[Tuple[pandas.Series,...]] -&gt; Iterator[pandas.Series]</code>时，称为<code>Iterator[Tuple[pandas.Series,...]] to Iterator[Series]</code>UDF.</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from typing import Iterator, Tuple</span><br><span class="line">&gt;&gt;&gt; @pandas_udf(&#x27;long&#x27;)</span><br><span class="line">... def multiply_two_cols(</span><br><span class="line">...     iterator: Iterator[Tuple[pd.Series, pd.Series]]) -&gt; Iterator[pd.Series]:</span><br><span class="line">...     for a,b in iterator:</span><br><span class="line">...             yield a*b</span><br><span class="line">... </span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df.select(multiply_two_cols(&#x27;x&#x27;,&#x27;x&#x27;)).show()</span><br><span class="line">+-----------------------+</span><br><span class="line">|multiply_tow_cols(x, x)|</span><br><span class="line">+-----------------------+</span><br><span class="line">|                      1|</span><br><span class="line">|                      4|</span><br><span class="line">|                      9|</span><br><span class="line">+-----------------------+</span><br></pre></td></tr></table></figure></div>

<h3 id="Series-gt-Scalar-类型的UDF"><a href="#Series-gt-Scalar-类型的UDF" class="headerlink" title="Series -&gt; Scalar 类型的UDF"></a>Series -&gt; Scalar 类型的UDF</h3><p>当类型提示可以被表达为<code>pandas.Series -&gt; Scalar</code>时，称为<code>Series to Scalar</code>UDF.<br>Scalar具体的类型必须是原生python类型如int, float等等， 或者是numpy的数据类型如numpy.int64, numpy.float64<br>这种UDF可以被用于<code>groupBy(), agg(), pyspark.sql.Window</code>.</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from pyspark.sql import Window</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df = spark.createDataFrame([(1,1.0), (1,2.0),(2,3.0),(2,4.0),(2,10.0)], (&#x27;id&#x27;,&#x27;v&#x27;))</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">DataFrame[id: bigint, v: double]</span><br><span class="line">&gt;&gt;&gt; @pandas_udf(&#x27;double&#x27;)</span><br><span class="line">... def mean_udf(v: pd.Series) -&gt; float:</span><br><span class="line">...     return v.mean()</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; df.select(mean_udf(&#x27;v&#x27;)).show()</span><br><span class="line">+-----------+</span><br><span class="line">|mean_udf(v)|</span><br><span class="line">+-----------+</span><br><span class="line">|        4.0|</span><br><span class="line">+-----------+</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df.groupby(&#x27;id&#x27;).agg(mean_udf(&#x27;v&#x27;)).show()</span><br><span class="line">+---+-----------------+</span><br><span class="line">| id|      mean_udf(v)|</span><br><span class="line">+---+-----------------+</span><br><span class="line">|  1|              1.5|</span><br><span class="line">|  2|5.666666666666667|</span><br><span class="line">+---+-----------------+</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">DataFrame[id: bigint, v: double]</span><br><span class="line">&gt;&gt;&gt; df.show()</span><br><span class="line">+---+----+</span><br><span class="line">| id|   v|</span><br><span class="line">+---+----+</span><br><span class="line">|  1| 1.0|</span><br><span class="line">|  1| 2.0|</span><br><span class="line">|  2| 3.0|</span><br><span class="line">|  2| 4.0|</span><br><span class="line">|  2|10.0|</span><br><span class="line">+---+----+</span><br><span class="line">&gt;&gt;&gt; w = Window.partitionBy(&#x27;id&#x27;).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)</span><br><span class="line">&gt;&gt;&gt; df.withColumn(&#x27;mean_v&#x27;, mean_udf(&#x27;v&#x27;).over(w)).show()</span><br><span class="line">+---+----+-----------------+                                                    </span><br><span class="line">| id|   v|           mean_v|</span><br><span class="line">+---+----+-----------------+</span><br><span class="line">|  1| 1.0|              1.5|</span><br><span class="line">|  1| 2.0|              1.5|</span><br><span class="line">|  2| 3.0|5.666666666666667|</span><br><span class="line">|  2| 4.0|5.666666666666667|</span><br><span class="line">|  2|10.0|5.666666666666667|</span><br><span class="line">+---+----+-----------------+</span><br></pre></td></tr></table></figure></div>

<h2 id="Spark的Pandas函数API"><a href="#Spark的Pandas函数API" class="headerlink" title="Spark的Pandas函数API"></a>Spark的Pandas函数API</h2><p>Spark有一些函数可以让python的函数通过pandas实例直接用在spark dataframe上。内部机制上类似<code>pandas udf</code>, jvm把数据转成arrow的buffer, 然后pandas可以直接在buffer上操作。但是区别是，这些函数api使用起来就像普通pyspark api一样是作用在dataframe上的, 而不像udf那样作用于一个<code>column</code>. 实际使用的时候，一般是<code>DataFrame.groupby().applyInPandas()</code>或者<code>DataFrame.groupby().mapInPandas()</code></p>
<h3 id="Grouped-Map-api"><a href="#Grouped-Map-api" class="headerlink" title="Grouped Map api"></a>Grouped Map api</h3><p>Spark的dataframe在<code>groupby</code>后使用普通的pandas函数， 如<code>df.groupby().applyInPandas(func, schema))</code>， 普通的pandas函数需要输入是pandas dataframe, 返回普通的pandas dataframe. 上面这写法会把每个分组group映射到pandas dataframe.<br><code>df.groupby().applyInPandas(func, schema))</code>过程其实分为三步， 典型的<code>split-apply-combine</code>模式：</p>
<ul>
<li><code>DataFrame.groupBy</code>分组数据</li>
<li>分组的数据映射到pandas dataframe后，apply到传入的函数</li>
<li>组合结果成一个新的pyspark Dataframe<br>使用groupBy().applyInPandas(), 用户需要做两件事：</li>
<li>写好pandas函数</li>
<li>定义好pyspark dataframe结果的schema<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def subtract_mean(pdf):</span><br><span class="line">...     v = pdf.v</span><br><span class="line">...     return pdf.assign(v=v-v.mean())</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; df.groupby(&#x27;id&#x27;).applyInPandas(subtract_mean,schema=&#x27;id long, v double&#x27;).show()</span><br><span class="line">+---+------------------+                                                        </span><br><span class="line">| id|                 v|</span><br><span class="line">+---+------------------+</span><br><span class="line">|  1|              -0.5|</span><br><span class="line">|  1|               0.5|</span><br><span class="line">|  2|-2.666666666666667|</span><br><span class="line">|  2|-1.666666666666667|</span><br><span class="line">|  2| 4.333333333333333|</span><br><span class="line">+---+------------------+</span><br></pre></td></tr></table></figure></div></li>
</ul>
<h3 id="Map-api"><a href="#Map-api" class="headerlink" title="Map api"></a>Map api</h3><p>也可以对pyspark dataframe和pandas dataframe做map操作，<code>DataFrame.mapInPandas()</code>是对当前的DataFrame的取一个迭代器映射普通到pandas函数。这个普通pandas函数必须是输入输出都是pdf.</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def filter_func(iterator):</span><br><span class="line">...     for pdf in iterator:</span><br><span class="line">...             yield pdf[pdf.id == 1]</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; df.mapInPandas(filter_func, schema=df.schema).show()</span><br><span class="line">+---+---+</span><br><span class="line">| id|  v|</span><br><span class="line">+---+---+</span><br><span class="line">|  1|1.0|</span><br><span class="line">|  1|2.0|</span><br><span class="line">+---+---+</span><br></pre></td></tr></table></figure></div>

<h3 id="Co-grouped-Map-api"><a href="#Co-grouped-Map-api" class="headerlink" title="Co-grouped Map api"></a>Co-grouped Map api</h3><p>这个api可以使两个pyspark dataframe组合后使用pandas函数</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],</span><br><span class="line">    (&quot;time&quot;, &quot;id&quot;, &quot;v1&quot;))</span><br><span class="line"></span><br><span class="line">df2 = spark.createDataFrame(</span><br><span class="line">    [(20000101, 1, &quot;x&quot;), (20000101, 2, &quot;y&quot;)],</span><br><span class="line">    (&quot;time&quot;, &quot;id&quot;, &quot;v2&quot;))</span><br><span class="line"></span><br><span class="line">def asof_join(l, r):</span><br><span class="line">    return pd.merge_asof(l, r, on=&quot;time&quot;, by=&quot;id&quot;)</span><br><span class="line"></span><br><span class="line">df1.groupby(&quot;id&quot;).cogroup(df2.groupby(&quot;id&quot;)).applyInPandas(</span><br><span class="line">    asof_join, schema=&quot;time int, id int, v1 double, v2 string&quot;).show()</span><br><span class="line"># +--------+---+---+---+</span><br><span class="line"># |    time| id| v1| v2|</span><br><span class="line"># +--------+---+---+---+</span><br><span class="line"># |20000101|  1|1.0|  x|</span><br><span class="line"># |20000102|  1|3.0|  x|</span><br><span class="line"># |20000101|  2|2.0|  y|</span><br><span class="line"># |20000102|  2|4.0|  y|</span><br><span class="line"># +--------+---+---+---+</span><br></pre></td></tr></table></figure></div>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li>Post title：Pandas UDF and Function Api in Spark</li>
        <li>Post author：Kopei</li>
        <li>Create time：2020-07-23 00:00:00</li>
        <li>
            Post link：https://kopei.github.io/2020/07/22/bigdata-2020-07-23-pyspark-for-pandas/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="../../../../tags/big-data/">#big data</a>&nbsp;
                        </li>
                    
                </ul>
            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="../../23/bigdata-2020-07-24-from-pandas-to-spark/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">从Pandas到Spark</span>
                                    <span class="post-nav-item">Prev posts</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="../../10/bigdata-2020-07-11-create-dataframe-from-rdd-in-spark/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">Spark RDD转成Dataset的两种方式</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            

            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments">&nbsp;Comments</i>
    </div>
    

        
            
 
    <div id="waline"></div>
    <script data-pjax 
        src="//evan.beee.top/js/waline.js"></script>
    <script data-pjax>
        function loadWaline() {
            Waline.init({
                el: '#waline',
                serverURL: 'https://kopie-comments-9snhujxxf-kopei.vercel.app',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">Pandas UDF and Function Api in Spark</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Apache-Arrow-in-PySpark"><span class="nav-text">Apache Arrow in PySpark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8Arrow%E4%BD%9C%E4%B8%BA%E6%95%B0%E6%8D%AE%E4%BA%A4%E6%8D%A2%E4%B8%AD%E4%BB%8B%E8%83%BD%E5%A4%9F%E6%8F%90%E5%8D%87%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="nav-text">为什么使用Arrow作为数据交换中介能够提升性能？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-DataFrame%E5%92%8CPandas-DataFrame%E7%9A%84%E8%BD%AC%E5%8C%96"><span class="nav-text">Spark DataFrame和Pandas DataFrame的转化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pandas-UDF-%E7%9F%A2%E9%87%8FUDF"><span class="nav-text">Pandas UDF(矢量UDF)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Series-to-Series-%E7%B1%BB%E5%9E%8B%E7%9A%84UDF"><span class="nav-text">Series to Series 类型的UDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Series%E8%BF%AD%E4%BB%A3%E5%99%A8-gt-Series%E8%BF%AD%E4%BB%A3%E5%99%A8-%E7%B1%BB%E5%9E%8B%E7%9A%84UDF"><span class="nav-text">Series迭代器 -&gt; Series迭代器 类型的UDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E4%B8%AASeries%E8%BF%AD%E4%BB%A3%E5%99%A8-gt-Series%E8%BF%AD%E4%BB%A3%E5%99%A8-%E7%B1%BB%E5%9E%8B%E7%9A%84UDF"><span class="nav-text">多个Series迭代器 -&gt; Series迭代器 类型的UDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Series-gt-Scalar-%E7%B1%BB%E5%9E%8B%E7%9A%84UDF"><span class="nav-text">Series -&gt; Scalar 类型的UDF</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark%E7%9A%84Pandas%E5%87%BD%E6%95%B0API"><span class="nav-text">Spark的Pandas函数API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Grouped-Map-api"><span class="nav-text">Grouped Map api</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Map-api"><span class="nav-text">Map api</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Co-grouped-Map-api"><span class="nav-text">Co-grouped Map api</span></a></li></ol></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>



        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2017</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-regular fa-computer-classic"></i>&nbsp;&nbsp;<a href="/">Kopei</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        VISITOR COUNT&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        TOTAL PAGE VIEWS&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a></span>
                <br> 
            <span class="theme-version-container">THEME&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v1.1.0</a>
        </div>
        
        
        
            <div id="start_time_div" style="display:none">
                2017/01/01 00:00:00
            </div>
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
        
            <script async data-pjax defer>
                function odometer_init(){
                        let el = document.getElementsByClassName('odometer');
                        for (i = 0; i < el.length; i++) {
                            od = new Odometer({
                                el: el[i],
                                format: '( ddd).dd',
                                duration: 200
                            });
                        }
                }
                odometer_init();
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="unfolded-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="fa-regular fa-arrow-up"></i>
            </li>
        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="folded-tools-list">
        <li class="right-bottom-tools tool-toggle-show flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    


</main>



<script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/utils.js"></script><script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/main.js"></script><script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/layouts/menu-shrink.js"></script><script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/tools/go-top-bottom.js"></script><script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/tools/dark-light-toggle.js"></script>


    <script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/tools/local-search.js"></script>



    <script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/tools/code-block.js"></script>



    <script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/layouts/lazyload.js"></script>



    <script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/tools/runtime.js"></script>
    <script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/layouts/odometer.min.js"></script>
    <link rel="stylesheet" href="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/css/odometer-theme-minimal.css">


<div class="post-scripts pjax">
    
        <script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/tools/toc-toggle.js"></script><script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/libs/anime.min.js"></script><script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/layouts/toc.js"></script><script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/plugins/tabs.js"></script>
    
    
</div>


    <script src="//evan.beee.top/projects/hexo-theme-redefine/v1.1.0/source/js/libs/pjax.min.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            REDEFINE.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            REDEFINE.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            REDEFINE.refresh();
        });
    });
</script>



</body>
</html>
