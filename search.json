[{"title":"OceanDB小结","url":"/2017/11/13/Aliyun-oceanDB/","content":"阿里自研的分布式关系型数据库主要特点\n支持SQL92和高度兼容Mysql，有一个类似mysql的sql语法，但是有一些限制。\n多个副本，分布在多区域，可抵御单机、机架及机房故障。\n准内存数据库。\n底层Paxos协议，通过3个以上节点投票保持数据强一致。\n支持跨行跨表事务\n\n","categories":["database"],"tags":["db, aliyun"]},{"title":"Aliyun OSS 小结","url":"/2017/11/14/aliyun-oss/","content":"OSS的原理对象存储将数据当成对象, 可以通过对对象的HTTP verb进行增删改查操作. oss的bucket是一个平铺的大文件夹(看起来你可以像传统文件系统那样有目录层级,但是实际上是平的),里面存储的文件名可以当成key, 文件为value, 这样仿佛oss又是一个键值对存储,但是有微小的差别: oos还可以有元属性, 存储大数据有优化但不保证数据强一致性.  \n块存储和对象存储的比较块存储是把数据存在存储介质上, 存储介质以 块 的形式(比如一块是512B)将数据分割存储. 而对象存储是将数据当成一个对象. 它们的区别可以在更新文件时清楚看到, 当更新一个文件的一行数据时, 块存储只会更新那一行数据存在的某一块, 而对象存储是更新整个文件; 同时块存储是服务器,操作系统紧紧耦合关系, 而对象存储是一个单独的可以通过API控制的服务.\n对象存储和文件存储的比较对象存储和文件系统在接口上的本质区别是对象存储不支持和fread和fwrite类似的随机位置读写操作，即一个文件PUT到对象存储里以后，如果要读取，只能GET整个文件，如果要修改一个对象，只能重新PUT一个新的到对象存储里，覆盖之前的对象或者形成一个新的版本。\n使用限制\n冷备份恢复读取需要1分钟\n上传5G的数据需要采用断点续传，但不能大于48.8TB\n上传同名文件会覆盖原有文件\n删除文件无法恢复\n\n","categories":["storage"],"tags":["oss, aliyun"]},{"title":"Git如何同步fork的仓库","url":"/2017/12/14/%E5%90%8C%E6%AD%A5Forked%E4%BB%93%E5%BA%93/","content":"场景实际工作可能需要fork别人的代码另做开发，但是又有需求希望同步源仓库的更新。以下实例将演示如何同步源仓库代码。\nMac, Linux, windows通用\n首先需要配置git remote 到源仓库\n\nblockchain git:(f03d28f) ✗ git remote -vorigin\thttps://github.com/dvf/blockchain (fetch)origin\thttps://github.com/dvf/blockchain (push)blockchain git:(f03d28f) ✗ git remote add upstream https://github.com/dvf/blockchainblockchain git:(f03d28f) ✗ git remote -vorigin\thttps://github.com/dvf/blockchain (fetch)origin\thttps://github.com/dvf/blockchain (push)upstream\thttps://github.com/dvf/blockchain (fetch)upstream\thttps://github.com/dvf/blockchain (push)\n\n\n拉取原仓库更新的提交\n\nblockchain git:(f03d28f) ✗ git fetch upstreamremote: Counting objects: 77, done.remote: Total 77 (delta 23), reused 24 (delta 23), pack-reused 53Unpacking objects: 100% (77/77), done.From https://github.com/dvf/blockchain [new branch]      dvf/bug-fix  -&gt; upstream/dvf/bug-fix [new branch]      dvf/hash-fix -&gt; upstream/dvf/hash-fix [new branch]      dvf/tests    -&gt; upstream/dvf/tests [new branch]      master       -&gt; upstream/master\n\n\n切换到本地master\n\nblockchain git:(f03d28f) ✗ git checkout masterM\tblockchain.pySwitched to branch &#x27;master&#x27;Your branch is up-to-date with &#x27;origin/master&#x27;.\n\n合并upstream&#x2F;master到local master\n\nblockchain git:(master) git merge upstream/masterUpdating f03d28f..4010cf3Fast-forward .gitattributes                                       |  63 +++++++++++++++ .travis.yml                                          |  12 +++ README.md                                            |   2 + blockchain.py                                        |  22 +++--- csharp/BlockChain.Console/App.config                 |  10 +++ csharp/BlockChain.Console/BlockChain.Console.csproj  |  63 +++++++++++++++ csharp/BlockChain.Console/Program.cs                 |  12 +++ csharp/BlockChain.Console/Properties/AssemblyInfo.cs |  36 +++++++++ csharp/BlockChain.sln                                |  43 ++++++++++ csharp/BlockChain/Block.cs                           |  19 +++++ csharp/BlockChain/BlockChain.cs                      | 226 +++++++++++++++++++++++++++++++++++++++++++++++++++++ csharp/BlockChain/BlockChain.csproj                  |  73 +++++++++++++++++ csharp/BlockChain/Node.cs                            |   9 +++ csharp/BlockChain/Properties/AssemblyInfo.cs         |  36 +++++++++ csharp/BlockChain/Transaction.cs                     |   9 +++ csharp/BlockChain/WebServer.cs                       |  78 ++++++++++++++++++ csharp/BlockChain/packages.config                    |   5 ++ tests/__init__.py                                    |   0 tests/test_blockchain.py                             | 104 ++++++++++++++++++++++++ 19 files changed, 811 insertions(+), 11 deletions(-) create mode 100644 .gitattributes create mode 100644 .travis.yml create mode 100644 csharp/BlockChain.Console/App.config create mode 100644 csharp/BlockChain.Console/BlockChain.Console.csproj create mode 100644 csharp/BlockChain.Console/Program.cs create mode 100644 csharp/BlockChain.Console/Properties/AssemblyInfo.cs create mode 100644 csharp/BlockChain.sln create mode 100644 csharp/BlockChain/Block.cs create mode 100644 csharp/BlockChain/BlockChain.cs create mode 100644 csharp/BlockChain/BlockChain.csproj create mode 100644 csharp/BlockChain/Node.cs create mode 100644 csharp/BlockChain/Properties/AssemblyInfo.cs create mode 100644 csharp/BlockChain/Transaction.cs create mode 100644 csharp/BlockChain/WebServer.cs create mode 100644 csharp/BlockChain/packages.config create mode 100644 tests/__init__.py create mode 100644 tests/test_blockchain.py\n\n看一下git branch tree\n\n\n","categories":["misc"],"tags":["misc"]},{"title":"Customer Experience","url":"/2017/12/06/customer-experience/","content":"What is customer experience?\nInformation architecture \nto a user, the interface is the system\nwhat is persona？\n\nmethod workflow\npre-discovery\ninternal analysis\nstakeholder interview\n\n\ndiscovery\nuser interview\ncontextual analysis\ncompetive analysis\nadjacent experience\n\n\naudience analysis\nvalidate design guide\npain points\nuser journey\n\n\nUX design for 3 rounds\nInformation Architecture\ninteration architure\nglobal navigation\n\n\nVisual design for 3 rounds\ncolor\ncontrol display systems\nvisual order\n\n\n\n","categories":["misc"],"tags":["misc"]},{"title":"冬季嵊泗3日游行程安排","url":"/2017/12/26/%E5%B5%8A%E6%B3%97%E8%A1%8C%E7%A8%8B/","content":"Day One\n9：30AM出发， 上海黄浦区旅游集散中心，外马路1588号。 四号线1号口出来就是。\n下午1点到李柱山码头，打车到酒店，预计50元。地址：菜园镇金沙路101号。记得领取酒店免费两张基湖沙滩门票\n吃中饭， 可以在700米附近的大排档，也可以在酒店\n下午时间多可以去大悲山和东海渔村，坐公交，菜园-五龙线。时间少可以在基湖沙滩看日落。\n晚饭在酒店附近吃排档。\n本岛巴士\n微信预订船票到花鸟岛, 但是不能预订从小花园到花鸟的船票。\n注意天气情况，风大可能无法出船！\n\nDay Two\n早上6点起，酒店早饭，7：30赶到菜园码头开船到花鸟岛，9点到达。14点返回本岛。15：30到菜园码头。\n码塔线，从码头开始一直到花鸟灯塔全长约5公里\n午饭在路上解决\n\n\n逛一下菜园镇，然后到大悲山（如果第一没去）或者基湖沙滩。\n返回酒店\n\nDay Three\n4点起床，到六井谭看2018年第一缕日出，然后和尚景区，南长涂。镇上吃完中饭，打道回府。\n\n备注\n必带物品\n手套，围巾，帽子等御寒防风物品\n可带暖宝宝\n\n\n可带\n铲子\n水桶\n鱼竿\n\n\n\n","categories":["misc"],"tags":["misc"]},{"title":"SED and AWK","url":"/2018/04/02/sed-and-awk/","content":"前言SED和AWK都是平时会用的Linux命令，正好看到NYU的一个PPT, 觉得不错，做一下总结。\nSEDsed是一个流式、非交互的文本编辑器。类似grep, sed会读取一行文本然后查找一个pattern模板,再根据匹配与否做相应的动作。sed是可以改变文件内容的。sed作为非交互文本编辑器需要以脚本执行命令，有一个叫ed的交互式编辑器可以执行相同的命令。sed也可以认为是一个unix过滤器（如果不改变原输入）。所有的sed指令会在每一行执行，如果某一行执行了一行命令，后续还有命令会在这个改变的行继续执行指令。\n\n优点：正则、快速、简洁\n缺点：难以记忆、不能回退、不能前向引用、不能处理数字、语法笨拙\n\nsed脚本结构[address[,address]][!]command[arguments]sed脚本由一些命令组成，每一个命令由最多两个可选地址（address)和一个指令action组成, 如上所示（多个action可以用{}包住）。当一行输入时，sed读取一行命令，把它存入 pattern space 检查address是否匹配当前行，如果满足就会执行指令，(如果没有address,命令会在每行执行).如果不满足就直接跳到下一行命令。当某一行执行完所有命令，sed就会把当前处理过的行输出（处理过的行在pattern space里）。然后再读取下一行，循环执行命令直到文件最后一行。\naddress一个address可以是一个行号或者pattern模式(&#x2F;pattern&#x2F;), 如果是pattern那么它必须是正则表达式(basic regular expression)。$指最后一行。如果有两个address, 那么命令会在这两个address之间执行（包括这两行）。可以使用！取反操作。address!action\nActionsed的指令是一个单独的字母[s,a,i,c,d,p,y,q]。下面是一些例子：\n\n6d #删除第六行\n&#x2F;$^&#x2F;d #删除所有空行\n1,10d #删除1-10行（含）\n1，&#x2F;^$&#x2F;d  #删除第一行以下的所有空行\n&#x2F;^$&#x2F;,$d  #删除第一个空行到结尾\n&#x2F;^ya*y&#x2F;,&#x2F;[0-9]$&#x2F;d # 把yay, yaay,yaaay等，并且以数字结尾的行删除。但使用多个指令时，有一些语法要求，如下。第左括号必须是行结尾，右括号必须是单行结尾。\n\n[/pattern/[,/pattern]]&#123;  action1  action2  action3&#125;\n\n打印 [address[,address]]p用来打印pattern space的内容，可以配合-n使用，如果不指定-n, 但是指定p,结果会打印两次！\n替换[address(es)]s/pattern/replacement/[flags], flags可以是[n,g,p],n代表替换几次，g代表pattern space中的全局替换,p代表打印.sed的replacement可以使用几个特殊字符：\n&amp;代表所有在pattern中匹配的文本部分，用这个&amp;就可以在replacement中引用pattern中匹配的部分。比如：\n\n# user=&amp;uidXsed -e &#x27;s/user=&amp;uid/user=&amp;sysuserid./g&#x27;user=user=&amp;uidsystemid.X\n\n\\n代表了在pattern中\\(,\\)中匹配的部分，用\\n可以在replacement中使用。\n替换例子：\n\n&quot;the UNIX operating system ...&quot;s/.NI./wonderful &amp;/&quot;the wonderful UNIX operating system ...&quot;---cat test1first:secondeone:twosed &#x27;s/\\(.*\\):\\(.*\\)/\\2:\\1/&#x27; test1seconde:fisrttwo:one---\n叠加 插入 修改 这三者的语法类似，需要更新的文本写在语句第二行. insert是在pattern space前面插入文本，append是在pattern space后面增加文本[address]a\\   #appendtest  [address]i\\   #inserttest[address(es)]c\\   #changetest\n\ninsert例子\n\n/&lt;Insert Text Here&gt;/i\\Line 1 of inserted text\\\\        Line 2 of inserted textLine 1 of inserted text             Line 2 of inserted text&lt;Insert Text Here&gt;\ny是sed的逐字替换，[address[,address]]y/abc/xyz\nq是退出读取新的行，sed &#39;110q&#39; filename读取110行\n\nhold spacesed的临时交换空间\nsed语法sed [-n][-e][&#39;command&#39;][file...]sed [-n][-f scriptfile][file...]-n只打印print命令指定的内容。-e后面跟command, 可以多条-e用于指定多个command, -f后面指定脚本，在脚本的第一行使用#n, 等同于-n-i直接在原输入文件做修改。\nsed的特殊字符sed中$.*[\\]^必须转义，除非这些字符在[…]中，而()&#123;&#125;+?|有特殊的作用，转义的话就变成特殊用法了。如果需要插入环境变量，sed的命令需要使用 双引号。\nAWKawk是三个发明者的名字首字母, awk的发明是为了提供一个可编程的过滤器来处理文本和数字，awk是一种pattern-action 语言，这点和sed一样。但是和sed处理行不同，awk处理的是字段。(当然它们输入都是读取一行)nawk是awk的新标准，用于大型awk程序，gawk是nawk的GNU版本。awk可以从文件、重定向、pipe和标准输入建立输入。awk的语言有点像C,但是会自动处理输入，字段分割，初始化内存和管理。数据类型支持字符串和数字，不需要变量类型声明。awk是一个非常优秀的原型语言，你可以一行一行加上程序逻辑直到满足需求。\nawk比sed好的地方\n方便数字处理\n变量和控制流\n方便找到行中的字段\n灵活的打印功能print\n内置算数和字符函数\n类C语法\n\nawk的语法结构awk的语法由三部分组成：\n\n可选的BEGIN部分，用于先于文本输入执行逻辑\npattern-action部分，根据输入数据，如果有pattern匹配，就采取对应的action。awk不会改变原输入文件, 可以用&gt;重定向输出文件。\n每一个awk程序必须有一个pattern或者一个action, 或者两者都有。如果没有指定pattern, 默认pattern是匹配所有的行; 没有指定action,默认action是打印当前记录。patterns通过简单文本展示，action需要大括号{}包住来区分两者。\nPatterns模式， 是一个选择器，它决定了后续的action是否执行。pattern可以是正则表达式（”&#x2F;&#x2F;“包住)，关系（大于等于）或者字符串匹配，”!”感叹号反向匹配，或通过”&amp;&amp;”或”||“任意组合。BEGIN和END是一个特殊的pattern, 用于初始化和总结。\nActions行动。action可以是一组类C表达式，也可以是数字或字符串表达式、赋值或格式化输出字符串流。action会在匹配的每一行执行，如果pattern没有提供，action会在每一行执行，如果action没有提供，所有pattern匹配的行会被打印到标准输出。action和pattern需要用有无{}来区分(有{}就是action)。\n\n\n可选的END部分，用于文本处理完后进一步执行逻辑\n\nls |awk &#x27;BEGIN &#123;print &quot;all html file&quot;&#125;/\\.html$/ &#123;print&#125;END &#123;print &quot;There we go!&quot;&#125;&#x27;\n\n运行awk的方式可以有三种方式执行awk命令：\n\nawk ‘program’ input_file(s)\n程序和输入文件通过命令行提供\n\n\nawk ‘program’\n程序通过命令行提供，输入通过标准输入\n\n\nawk -f program_file input_file(s)\n程序是从文件读入的\n\n\n\n变量awk可以定义变量，不需要声明类型。\nBEGIN &#123; sum = 0 &#125;&#123; sum++ &#125;END &#123; print sum &#125;\n\n条目(Records)awk一行是一条记录，默认记录条目分隔符是换行符（\\n)，但是可以是任何其他正则表达式。通过在BEGIN设置RS(record seperator).NR是一个变量记录着当期条目的序号。\n字段（Fields)awk里每一行输入都会被分成字段，FS(field separator)可以指定分隔符，默认是空格。通过awk -F（分隔符）指定分隔符。$0是整行，$1是第一个字段，一次递推。只有字段可以被通过$表示，变量无法这么做。\nawk输出\n打印某个字段， action写成&#123;print $1, $3&#125;可以打印第一和第三个字段，默认采用空格分割。\nNF(Number of Fields), 当前字段序号，&#123;print NF, $1, $NF, $(NF-2)&#125;打印当前字段序号，第一个字段，最后一个字段, 倒数第三个字段。\n可以对着$1做算术计算， &#123;print $1*$2&#125;\n打印行号， &#123;print NR, $0&#125;\n在打印的字段周围加一些字符，&#123;print &quot;total pay for&quot;, $1, &quot;is&quot;, $2*$3&#125;\nprintf. awk也提供格式化输出，printf(format, val1, val2, val3), {printf(“total pay for %s is $%.2f\\n”, $1, $2*$3)}, 注意空格和换行需要手动输入，awk不会帮你插入。\n\n选择器（selecton)awk patterns十分适合选择某些行做处理，比如\n\n$2 &gt;= 5 &#123;print&#125;比较选择\n$2*$3 &gt; 50 &#123;printf(&quot;%6.2f for %s\\n&quot;, $2*$3,$1)&#125; 计算\n$1 == &quot;NYU&quot;通过文本选择$2 ~ /NYU/\n$2 &gt;= 4 || $3 &gt;= 20组合比较\nNR &gt;= 10 &amp;&amp; NR &lt;= 20通过行号\n\n算术和变量awk变量是数字类型或字符类型，用户定义的变量是不能用$的（unadorned), 默认用户定义的变量初始化为null, 值为0. 变量有这些：\n\n$0, $1, $NF\nNR\nNF\nFILENAME. 当前输入文件的名称\nFS\nOFS. 输出字段分隔符\nARGC&#x2F;ARGV. argument count&#x2F;argument value array\n这两个变量用于从命令行得到参数awk的运算符一共有如下这些：\n\n\n赋值。 &#x3D;\n关系比较。 &#x3D;&#x3D;， &gt;&#x3D;, &lt;等等\n逻辑比较。 ||， &amp;&amp;， ！\n算术运算符。+，-，&#x2F;, %\n\n$3 &gt; 15 &#123;emp=emp+1&#125;END &#123;print emp, &quot;employees worked more than 15 hrs&quot;&#125;\n\n处理文本awk处理文本的强大之处是可以方便地把数字和字符转换。\n\n字符串拼接&#123;names=names $1 &quot; &quot;&#125; END &#123;print names&#125;\n\n内建函数awk有一些内建函数，比如length（wc简化版），&#123;nc = nc+length($0)+1&#125;; 比如substr(s,m,n), 生成字符s的子集，位置从s的m到n.\n\n算术函数。 sin, cos, atan, exp, int, log,rand,sqrt\n字符函数。length,substr, split\n输出。 print, printf\n特殊用途。system（&quot;clear&quot;)执行unit命令。exit立即从输入退出，进入END区。\n\n控制流if$2 &gt; 6 &#123;n=n+1; pay=pay+$2*$3&#125;END &#123;if (n&gt;0) print n else print &quot;no&quot;&#125;\n&#123; i= 1  while (i&lt;= $3)&#123;    printf(&quot;\\t%.2f\\n&quot;, $1*(1+$2)^i)    i = i+1  &#125;&#125;\ndo &#123;  statement1&#125;while(expr)\n&#123;for (i=1; i&lt;=$3;i=i+1)printf(&quot;\\t%.2f\\n&quot;, $1*(1+$2)^i)&#125;\n\n数组（Arrays)awk的数组不需要声明。数组的下标可以是数字和字符。\n&#123; line[NR] = $0 &#125;END &#123;  for (i=NR; i&gt;0; i=i-1)&#123;    print line[i]  &#125;&#125;\n&#123; for (v in array)&#123;    print array[v]  &#125;&#125;\n","categories":["linux"],"tags":["linux"]},{"title":"Subscription Mode","url":"/2023/01/08/Subscription-Mode/","content":"Zien’s Subscription Mode最近看了左霆的《订阅经济：创造可持续增长的未来》，有颇多的想法。整体上这边书介绍了订阅经济的商业模式和可行性, 我对第二部分的内容关于如何建立“订阅文化”比较感兴趣，因为它教你如何去建立公司文化、组织架构和其他种种可能遇到的问题及挑战。当然，作为一本主要推销作者个人和公司的书籍，对于“How”这一部分作者还是浅尝辄止，有些点不够深入。可能正是这样，读者意识到了问题但是不知道怎么去做，才更有可能去购买他公司的产品吧~顺便说一下，他家的产品是订阅管理系统，主要功能有:\n\n自动化订阅管理\n报告和分析\n订阅产品管理\n\n订阅模式的财务指标获客成本、生命周期客户价值、年经常性收益、每位客户带来的平均收益\nIT运营转变IT基础设施需要是以客户为中心建立的，而非围绕产品销售建立的。所以传统的IT组织架构需要往PADRE方向转变，他们分别是，后面会有详细介绍。需要支持的是这个架构比较适合互联网企业，大型非互联网集团公司可能需要相应调整，但是整体方向不变。\n\nPipeline渠道\nAcquire获客\nDeploy部署\nRun运营\nExpand扩展\n\nInnovation: always in Beta敏捷产品开发的一个关键就是在最终产品上线前有客户和利益相关者参与开发过程，目标是了解异常情景，帮助实现质量保证。学习gmail那样始终处于beta版本，始终让客户参与开发提升产品功能和质量。\ncustomer-centric客户作为永远的创新伙伴, 创新不会凭空发生，它是一个概念在一段时间内持续迭代的结果。让产品永远处于测试状态中。\nRules for subscription services个体互动高于流程和工具、可运行的软件高于详尽文档、客户合作高于合同谈判、响应变化高于遵循计划。\n营销4P的改变产品、价格、渠道、促销（4P）将随着产品向服务转变也相应做出改变。\n对渠道商的改变以工作坊、白皮书、研讨会等方式培训渠道商订阅模式的通识教育，原有大订单以年度维护计划等额外服务的形式进行。教会经销商如何经营长期的合作关系，而不是每年加签一份合同。一个可行的时间表：前三个月关注转化率，接下来的六个月关注使用率，最后三个月为续费及相关的潜在销售准备打包服务。并且将这些数据分享给经销商。\n对促销的改变需要在社交网络上讲一个成功的故事，特别是把你的服务和用户放进一个更广的社交故事之中。先是气势恢宏的商业转型故事，接下来是市场故事，最后才是产品故事。你想创建一个你自己版本的艺术馆，其目标就是带领参观者按照次序走完“三间屋子”。“第一间屋子”根本不涉及你的公司，它介绍的是你公司所诞生的背景，它讲述的是你在广阔的商业世界中所观察到的一切，恰恰是这些，让你的业务应运而生。只有当你建立了背景之后，才能走进“第二间屋子”，并阐明价值——基于角色和行业的客观效益。现在，你可以再深挖一点儿，给出具体的基于角色的建议、行业趋势及相关案例研究。最后，“第三间屋子”是关于产品自身。\n定价的改变定价与潜在客户无法预算或预估的某个使用参数联系起来。两种基本定价模式：\n\n消费驱动型增长（使用提升），以量取胜\n功能驱动型增长（服务创新），理想的情况是，假如你提供青铜&#x2F;白银&#x2F;黄金等级服务，70%的订阅用户都在白银和黄金类别中。\n\n销售资产转移模式转向长期的关系。对于订阅模式，销售更需要做的事：\n\n获得更多的客户\n提升这些客户的价值\n更长久地拥有这些客户\n\n需要采取多种策略来维持增长。他们分别是：\n\n选择合适的初始客户。如选择小型一些的企业，他们作为初始用户，不会像大公司那样需要面面俱到的功能和服务，导致本企业无法支持而倒闭。同时刚开始控制销售团队规模，让这些销售和客户建立长期关系而不是基于拿到佣金。\n降低客户流失率。降低客户流失率一个很关键的问题在于用户采纳程度，所以需要团队培训客户更好地使用产品。同时配合销售组合，通过广泛的解决方案解决广泛的问题的能力可以降低客户流失率。\n扩大销售团队。扩大销售团队时需要注意提升混合销售模式（传统销售模式和自助式销售），并且提升自动化来帮助内部销售流程运作。\n通过追加销售和交叉销售实现增值。追加销售指功能更丰富且价格更高的升级服务。交叉销售是指销售额外的服务，提供更加全面的解决方案策略，希望拥有交叉销售能力的订阅式企业，需要不断增加新的服务、特点、功能和内容，从而吸引客户从服务中获得更多价值。在成熟的订阅服务中，追加销售和交叉销售平均可以带来20%的收入。同时还有其他好处，包括降低客户流失率，从而降低你的获客成本。负责管理现有客户的销售可以叫“客户成功经理”。\n进入新的细分市场。\n走向国际。需要考虑第一，监管相关的内容——营业执照、税收、数据驻留要求；第二，支付相关的内容——替代支付网关、当地货币、信用卡等等；第三，店铺本身——人力资源、员工聘用等等。\n将收购带来的增长机会最大化。\n优化定价与打包组合。订阅式企业需要通过定价持续优化收益\n\n订阅经济利润表年度经常性收入Annual recurring revenue和月度经常性收入Monthly recurring revenue是订阅经济利润表核心。具体公示表现为：\nARRn+1 = ARRn - Churn + ACV(Annual contract value)\n因为订阅经济认为营销和销售的投入会提升下一年ACV,同时它们也不会作为经常性成本，而是增长成本，而增长成本对未来的经常性收入有直接的帮助。所以ACV = sales&amp;marketing * rate, 对于企业而言，如何提升销售和营销的投入对未来订阅合同的转化率至关重要。简化的订阅利润表为：\nPADRE&#x2F;PPM运营模型以客户为中心的组织架构可以由8大子系统有机组成，由PADRE+3个基本部分PPM组成。公司的组织不一定完全按照这个模式设立，但是这8个子系统是必须有的基础，并且可以跨功能协作。\n\nPipeline渠道。子系统的主要目标是建立市场意识，并转化为需求。具体渠道包括网络和社交媒体，公共关系，事件营销。\nAcquire获客。就是所谓的“买家之旅”。潜在客户是如何做决策的？他们成功的标准是什么？他们还有什么替代解决方案？什么可能是他们反对的？他们必须向谁进行确认？他们的伴侣？家人？老板？首席财务官？还是团队？\nDeploy部署。\nRun运营。持续运营能力对于客户的体验至关重要。\nExpand扩展。。你需要你的订阅用户能够做三件事：留下、增长、支持。三个基础部分包括：人、产品和钱。基于PADRE模型，运营公司就知道要关注渠道覆盖、销售数据、新采用的商标、客户保留统计数据、扩展速度，并且分享给公司每个人。建立PADRE这种文化确保有利于组织发展的知识不会过于分散。\n\n","tags":["Subscription mode,Zien"]},{"title":"FHIR-Resource-Definition","url":"/2023/02/20/FHIR-Resource-Definition/","content":"FHIR资源定义\n本文主要记录FHIRv4.3.0的资源Resource内容是如何定义的。实际数据交换时，资源可以用不同的格式来表示: XML、JSON、Turtle和UML. 未来批量数据Bulk Data Formats的定义也将发布. \n\nResource DefinitionFHIR资源指一个规格标准定义如何展示和健康相关的某个概念, 当前版本一共有146种资源，主要包括临床信息，人员与组织，财务，安全和术语。资源可以用几种不同的方法来定义:\n\n一种是层级表格，以逻辑视图的方式展示内容, 简称逻辑表Logical table\nUML 统一模型语言\n片假XML语法\n片假JSON\n片假Turtle\n\n除了上述语法，还有其他格式可以使用，包括W3C schema, Schematron, JSON Schema和StructureDefinition.\nLogical Table 逻辑表逻辑表以树形结构来表示资源，并固定几个特定列， 如名称、标志、类型等。相对其它表示方式，用表格的形式显得更加直观和整洁。\n\n\n\nColumn\nContent\n\n\n\nName\n资源中元素的名字，如XML中manifest清单，JSON&#x2F;RDF的属性名。有些名字结尾有[x]代表下面详细的介绍。同时名字前面还有一个图标标记类型。\n\n\nFlags\n一组标记表示实现时该如何处理该元素。比如这个元素必须被支持或它是可以选择的。\n\n\nCard\n基。 该元素在资源中允许出现的次数的下限和上限。\n\n\nType\n元素的类型。注意，元素的类型有两种含义，取决于该元素是否有定义的子元素。如果元素有子代，那么元素有一个匿名的类型，由子元素给定具体的类型。如果元素没有子代，那么该元素可以被预置的类型所指定。\n\n\nDescription&amp;Contraints\n元素的描述，约束的细节。比如那些情况适用编码的元素。\n\n\n表一. 逻辑表固定字段\n\n举例:\n对于Type类型的图标：\n\n代表一个基础资源。\n资源中的元素，同时此元素还可以定义子元素\n此元素可以有多个不同的类型\n代表基础类型元素，基础类型都以小写字母开头\n元素的数据类型描述了其它的元素，称为复合类型，复合类型用大写字母开头\n子元素可以引用另一个资源\n此元素和同一个资源内另一个元素内容一样\n引入切片集合, 具体见profile中定义.\n复杂嵌套扩展\n简单扩展，只扩展一个值没有嵌套\n复杂修改扩展\n 简单修改扩展\n逻辑概貌的根\n\n对于Flag标志的图标\n\n?!: 这个元素是一个正在建设的元素\nS: 这个元素必须被支持\nΣ: 此元素是汇总集合的一部分\nI: 此元素定义了约束或被约束\n《A》抽象类型\n《I》此资源是接口定义\nTU此元素是试用状态\nN该元素的标准状态为规范性，既正式状态。\nD该元素的标准状态是起草阶段。\n\n其它注意项\n\n资源和元素是大小写敏感的\n任何元素是基础类型的，它会有一个value的属性来表示具体的数值\n元素都有一个基数，来表示这个元素会出现或者必须出现多少次\n元素复用方面，如果子元素和另一个子元素有相同的数据类型，那么可以这样表示：see[name]括号中name是另子元素的名称。\n每个元素名都会在数值字典里有正式的定义名，通过超链接找到对应的关系。\n有些元素可能会有id的属性用来内部引用，上面的例子没有显示出来这个id。\nFHIR的元素永不为空。如果有一个元素在资源里出现，那么它要么必须有值，要么它的子元素定义，或者其他扩展。\n基础类的元素是所有资源共有的，所以不会在表格中展示。这些共用的元素在基类Resource和DomainResource中定义。元素的数据类型表示一般是type类型直接体现的，但是有两个特例：\n如果元素支持多个类型（名字结尾[x]）, 那么类似可以是一系列选项，用|来分割。\n如果一个类型是Reference或canonical, 那么数据类型将直接列出可能的引用或者profile链接。如果链接到profile, 参考的类型可能被profiled， 比如元素的实例必须遵循特定profile或者一组profile列表。特定的url使用&#123;&#125;来表示。\n\n数据类型的选择有些元素可能有多个数据类型的选择，这样的情况需要使用name[x]来定义元素名，[x]指定实际使用的数据类型。如果想要某个元素只重复一次数据类型，那么它的基数只能是1. \n格式的序列化可以使用如下方式序列化资源：\n\nJSON\nXML\nRDF(Turtle)\n\n系统必须在Capability Statement声明它支持的格式. 如果一个服务器收到它不支持的格式请求，那么需要返回406 Not Acceptable. 如果客户端post一个不支持的格式，那么返回416 Unsupported Media Type.比较推荐的做法是支持JSON和XML格式，以适用于不同的技术栈。RDF比较适用于数据分析而不是数据交换。\n批量数据格式（建设中）当FHIR需要交换批量数据如1000条以上数据的时候，批量数据格式就可以用上了。目前建议的支持格式有：\n\nND-JSON(New line delimited JSON)\nGoogle Protobuf\nApache Parquet&#x2F;Avro\n\n外部链接\nFHIR Resource Definition\n\n"},{"title":"Health Data Security Guide","url":"/2023/02/21/health-data-security-guide/","content":"\nThis article is a translation of Chinese national standard GB&#x2F;T39725-2020.\n\n0. IntroductionHealth data includes personal health data and health related data obtained from processed system. With the vigorous development of health application, “Internet + health” and intelligent medical care, various new businesses and applications are emerging, and health data are facing more and more security challenges in all stages of the life cycle, and security issues are frequently occurring. Since the security of health data is related to the safety of patients’ lives, personal information security, social public interest and national security, in order to better protect the security of health data, regulate and promote the integration and sharing of health data, development and application, and promote the development of health business, this health data security guideline is formulated. \n1. ScopeThis standard gives the security measures that health data controllers can take when protecting health data.This standard is suitable for guiding health data controllers in the security protection of health data, and can also be used as a reference for health, cyber security-related authorities and third-party assessment organizations to carry out security supervision, management and assessment of health data. \n2. Normative ReferenceThe following documents are essential to the application of this document. Where a reference document is dated, only the dated version applies to this document. Where the reference document is not dated, its latest version (including all the change orders) applies to this document. \n\n\n\nNational standard document code\nCategory\nDocument Name\nApplicable\n\n\n\nGB&#x2F;T 22080—2016\nInformation Security tech\nInformation Security Management System\nRequired\n\n\nGB&#x2F;T 22081—2016\nInformation Security tech\nInformation Security Control Practice Guide\nGuide\n\n\nGB&#x2F;T 22239—2019\nInformation Security tech\nBaseline for Classified protection of cybersecurity\nRequired\n\n\nGB&#x2F;T 25069\nInformation Security tech\nTerminology\n\n\n\nGB&#x2F;T 31168\nInformation Security tech\nSecurity capability requirements of cloud computing services\nRequired\n\n\nGB&#x2F;T 35273\nInformation Security tech\nPersonal information security specification\nGuide\n\n\nGB&#x2F;T 35274—2017\nInformation Security tech\nSecurity capability requirements for big data services\nRequired\n\n\nGB&#x2F;T 37964—2019\nInformation Security tech\nGuide for de-identifying personal information\nGuide\n\n\nISO 80001\n\nApplication of risk management for IT-networks incorporating medical devices\n\n\n\n3. Terms &amp; Definition3.1 Personal health dataElectronic data that, alone or in combination with other information, can identify a specific natural person or reflect the physical or mental health of a specific natural person Data.Note: Personal health data includes an individual’s past, present or future physical or mental health, received health services and health services paid for, etc. See Appendix A.\n3.2 Health dataPersonal health data and health-related electronic data obtained by processingExamples: Overall group analysis results, trend forecasts, disease prevention and control statistics, etc., obtained after processing group health data.\n3.3 Health service professionalA person authorized by a government or industry organization to be qualified to perform specific health work.Example: Doctors.\n3.4 Health serviceServices provided by a health professional or paraprofessional that have an impact on a health condition.\n3.5 Health data controllerAn organization or individual who can determine the purpose, manner, and scope of health data processing, etc.Example: Organizations that provide health services, health insurance agencies, government agencies, health scientific research institutions, individual clinics, etc.\n3.6 Health information systemA system for capturing, storing, processing, transmitting, accessing, and destroying health data in a computed form.\n3.7 Limited data setPersonal health data sets that have been partially de-identified but still are able to identify the corresponding individual and therefore need to be protected.Example: Removal of identifiers directly related to the individual and his or her dependents, family members, and employers from health data.Note: Limited datasets may be used for scientific research, medical&#x2F;health education, and public health purposes without the authorization of the individual.\n3.8 Notes of treatmentObservations, reflections, treatment, discussions, and conclusions recorded by health professionals in the course of providing health services.Note: Treatment notes have intellectual property attributes and are the property of the health professional and&#x2F;or his or her organization.\n3.9 DisclosureThe act of transferring and sharing health data to specific individuals or organizations, as well as publicly releasing it to unspecified individuals, organizations or society.\n3.10 Clinical researchScientific research activities aimed at exploring the causes, prevention, diagnosis, treatment and prognosis of diseases, initiated by medical institutions, academic research institutions and&#x2F;or medical and health-related companies, with patients or healthy people as research subjects.Note: Clinical research belongs to a branch of medical research.\n3.11 Completely public sharingOnce data is released, it is difficult to recall and is generally released directly and publicly via the Internet. See [GB&#x2F;T37964—2019, 3.12].\n3.12 Controlled public sharingControl the use of data through data use agreements. See [GB&#x2F;T37964—2019, 3.13].\n3.13 Enclave public sharingShared within the physical or virtual territory, data cannot flow outside the territory. See [BG&#x2F;T37964—2019, 3.14].\n4. Abbreviations.Following are the abbreviations.\n\nACL: （Access Control Lists）\nAPI: （Application Programming Interface）\nAPP: （Application）\nDNA: （Deoxyribonucleic Acid）\nEDC:  (Electronic Data Capture)\nGCP：（Good Clinical Practice）\nHIS: （Hospital Information System）\nHIV: （Human Immunodeficiency Virus） \nHL7: （Health Level Seven）\nID：（Identity）\nIP: （Internet Protocol）\nIPSEC：（Internet Protocol Security）\nLDS：（Limited Data Set Files）\nPIN: （Personal Identity Number） \nPUF：（Public Use Files）\nRIF: （Research Identifiable Files）\nRNA: （Ribonucleic Acid）\nSQL: （Structured Query Languages）\nTLS：（Transport Layer Security）\nUSB：（Universal Serial Bus）\nVPN: （Virtual Private Network）\nXSS：（Cross-site scripting）\n\n5. Security TargetHealth data controllers are advised to adopt reasonable and appropriate management and technical safeguards to achieve the following objectivesa) Ensure the confidentiality, integrity and availability of health data.b) Ensure the legality and compliance of the health data use and disclosure process to protect the security of personal information, public interest and national security.c) Ensure that health data meets business development needs while meeting the above security requirements.\n６. Classification6.1  Data CategoryHealth data can be classified asa) Personal attribute, is data that alone or in combination with other information, can identify a specific natural person.b) Health status, is data that reflects or is closely related to the health condition of an individual.c) Medical application data, is data that reflects medical care, outpatient, inpatient, discharge, and other medical services.d) Health payment data, are data related to the costs involved in services such as health or insurance.e) Health resource data, are those data that capture the capacity and characteristics of health providers, health programs, and health systems.f) Public health data, are data related to public utilities that are relevant to the public health of a country or region.\nThe specific content of each type of data is shown in Table 1. The data elements, data sets, value codes and other related standards used in the health information domain can be found in Appendix B.\nTable 1\n\n\n\n\nData Category\nScope\n\n\n\nPersonal attribute\n1) Demographic information, including name, date of birth, gender, ethnicity, nationality, occupation, address, employer, family member information, contact information, income, marital status, etc. 2) personal identification information, including name, ID card, work permit, residence permit, social security card, images that can identify the individual, health card number, hospitalization number, various types of examination and test-related bill numbers, etc. 3) personal communication information, including personal telephone numbers, mailboxes, account numbers and associated information, etc. 4) personal biometric information, including genetic, fingerprint, voice print, palm print, ear, iris, facial features, etc. 5) personal health monitoring sensor ID, etc.\n\n\nHealth status\nChief complaint, current and past medical history, physical examination (signs), family history, symptoms, test and examination data, genetic counseling data, health-related data collected by wearable devices, lifestyle, gene sequencing, transcription product sequencing, protein analysis and measurement, metabolic small molecule testing, human microbial testing, etc.\n\n\nMedical application data\nOutpatient (emergency) medical records, inpatient medical prescriptions, examination and test reports, medication information, course records, surgery records, anesthesia records, blood transfusion records, nursing records, admission records, discharge summaries, referral (hospital) records, informed information, etc.\n\n\nMedical payment data\n1) Medical transaction information, including medical insurance payment information, transaction amount, transaction records, etc. 2）Insurance information, including insurance status, insurance amount, etc.\n\n\nHealth Resource data\nBasic hospital information, hospital operation data, etc.\n\n\nPublic health data\nEnvironmental health data, epidemic disease data, disease monitoring data, disease prevention data, birth and death data, etc.\n\n\n6.2 Data GradingHealth data can be classified into the following five levels based on the level of data importance, risk level, and the level of possible damage and impact on the individual.(a) Level 1: Data that can be used completely public. This includes data that can be accessed through public channels, such as hospital names, addresses, telephone numbers, etc., which can be directly disclosed to the public on the Internet.(b) Level 2: Data that can be accessed on a large scale. For example, data that cannot identify individuals and can be used for research and analysis by doctors in each department upon application for approval.(c) Level 3: Data that can be accessed on a medium scale and may cause moderate damage to individual subjects if disclosed without authorization. For example, data that has been partially de-identified, but may still be re-identified, is limited to use within the scope of the authorized project team.d) Level 4: Data that is available for access use on a small scale and may cause a high degree of harm to the individual if disclosed without authorization. For example, data that can directly identify an individual is restricted to access and use by health professionals involved in healthcare activities.(e) Level 5: Data that is accessible only to a very small extent and under strictly limited conditions, which, if disclosed without authorization, may cause serious damage to the individual subject. For example, details of specific diseases (e.g., AIDS, STDs) are restricted to access by treating health professionals and require strict controls.\n6.3 Roles ClassificationFor a data-specific scenario, the relevant organizations or individuals can be classified into the following four types of roles. Any organization or individual can only be classified into one of these roles for a specific data, a specific scenario or a specific data processing behavior.(a) Personal health data subject (hereinafter referred to as “subject”): the natural person identified by personal health data.(b) Health data controller (hereinafter referred to as “controller”): See Definition 3.5. To decide whether an organization or individual can determine the purpose, manner, and scope of health data processing, it is possible to consider    - 1) Whether the processing of health data is necessary for the organization or individual to comply with a law or regulation.    - 2) whether the health data processing is necessary for the organization or individual to perform its public functions.    - 3) whether the health data processing is decided by the organization or individual itself or jointly with other organizations or individuals.    - 4) Whether the health data processing is authorized by the individual or by the government authorize to the organization or the individual.\nThe organization or individual who jointly decides the purpose, manner, and scope of a data processing is the joint controller.(c) Health data processor (hereinafter referred to as “processor”): An organization or individual that collects, transmits, stores, uses, processes or discloses health data in its possession on behalf of the controller, or provides the controller with services related to the use, processing or disclosure of health data. Common processors include: health information system providers, health data analysis companies, and assisted treatment solution providers.(d) Health data users (hereinafter referred to as “users”): Organizations or individuals who do not belong to the subjects, controllers or processors, but make use of health data for specific scenarios of specific data.\n6.4  Data Flow ScenariosBased on the data flow between different roles, the data exchange usage scenarios can be divided into the following six types, as shown in Figure 1.a) Subjects-&gt;controller data flow.b) Controller-&gt;subjects data flow.c) Controller&lt;-&gt;controller intra data flow.d) Controller&lt;-&gt;processor data exchange.e) Inter-controller data exchange.f) Controller -&gt; users data flow.\n\nFigure 1 \n\n6.5 Data Public WaysThe types of data public can be divided into completely public sharing, controlled public sharing, and enclave public sharing, which correspond to different de-identification requirements and are handled by the regulations of GB&#x2F;T37964–2019. Common forms of data openness and their applicable types of sharing are shown in Table 2.\n\n\n\nWays of open\nDescription\nApplicable type of sharing\n\n\n\nPublic website\nStatistical aggregated data or anonymized data are available to the public and can be downloaded and analyzed by themselves.\nCompletely public\n\n\nFile share\nGenerated files from data systems and push them to SFTP interface devices or applications, or share them using mobile media\nControlled public\n\n\nAPI interface\nData is provided between systems through request-response, with the data system providing real-time or quasi-real-time data service application interfaces to specific users, with the demand-side system initiating the request and the data system returning the required data, e.g. through the Webservice interface\nControlled public\n\n\nOnline query\nQuery the web page provided by the data system for relevant information\nCompletely public sharing of data (anonymous queries). Controlled public sharing (user query)\n\n\nData analysis platform\nThe platform provides system environment, analytical mining tools, and de-identified sample data or simulation data. Platform users use shared or dedicated hardware and data resources, can deploy their own data and data analysis algorithms, and can query the data and analysis results within their authority. All the original data of the platform cannot be exported; the output and download of analysis results must be approved by the approval before they can be exported to the public\nenclave public sharing\n\n\n7. Use and Disclosure PrinciplesIt is appropriate for the controller to follow the below principles in the use or disclosure of health data.\n\na) When using or disclosing personal health data, it is appropriate for the controller to inform the subject and obtain the subject’s authorization (except in the case of b below); all communications are appropriate in plain language and contain specific information about the content of the data to be disclosed or used, the recipient of the data, the purpose of the data and how it will be used, the duration of the use of the data, the rights of the data subject, and the protective measures taken by the controller. The use or disclosure of personal health data must not exceed the scope of the individual’s authorization. If it is necessary to use or disclose beyond the scope due to business needs, it is appropriate to obtain the consent of the subject again.\nb) A controller may use or disclose personal health data without the subject’s authorization in the following cases.\nWhen providing the subject with his or her own health data.\nWhen treatment, payment or health service is provided.\nWhen required by public interest or laws and regulations\nwhen the limited data set is used for scientific research, medical&#x2F;health education, or public health purposes.In the above cases, the controller may rely on legal and regulatory requirements, professional ethics, ethical and professional judgment to determine which personal health data are allowed to be used or disclosed.\n\n\nc) It is appropriate for the controller to obtain authorization from the subject to use or disclose personal health data for marketing activities, except for face-to-face marketing communications between the controller and the subject. It is appropriate for the authorization for marketing activities to be communicated to the subject in a reasonable manner and for the subject to be fully informed and to give explicit and autonomous consent. The authorization should be independent and should not be a precondition for the subject to obtain any public service or medical service or bundled with other service terms. The controller is advised to inform the subject in writing, at the same time as obtaining the authorization, of its right to revoke the authorization at any time.\nd) The subject (or his authorized representative) has the right to access his personal health data or to request disclosure of his data, and the controller is advised to disclose the corresponding personal health data upon his request.\ne) The subject has the right to review and obtain a copy of his or her personal health data, which the controller may provide, for example, through file sharing or online access.\nf) If the subject discovers that the controller holds inaccurate or incomplete personal health data about the subject, the controller is encouraged to provide the subject with the means to request correction or additional information.\ng) The Subject has the right to a historical review of the use or disclosure of data by the Controller or its processors for a minimum period of six years.\nh) The Subject has the right to request the Controller to restrict the use or disclosure of its personal health data in the course of diagnosis, treatment, payment, health services, etc., as well as to restrict the disclosure of information to related persons, and the Controller is not obliged to agree to such restriction requests; however, once agreed, it is appropriate for the Controller to comply with the agreed restrictions unless required by law or regulation and in medical emergencies.\ni) Controllers may use treatment notes for therapeutic purposes and, after necessary de-identification, may use or disclose treatment notes for internal training and academic seminars without individual authorization.\nj) It is appropriate for the controller to develop and implement reasonable strategies and processes to limit use and disclosure to a minimum.\nk) Controllers are advised to confirm that the security capabilities of the processor meet the security requirements and to sign a data processing agreement before allowing the processor to perform data processing for them. Processors are advised to process data in accordance with the controller’s requirements, and processors cannot bring in third parties to assist in data processing without the controller’s permission.\nl）Before the controller provides the data to the third party controller authorized by the government, it is appropriate to obtain the relevant documents with the official seal of the government, and after the data is provided, the responsibility for data security and the security of the transmission channel shall be borne by the third party controller.\nm) The controller may use the limited data set for scientific research, healthcare services, public health and other purposes after confirming the legality, legitimacy and necessity of data use and that the user has the appropriate data security capabilities, and the user has signed a data use agreement and committed to protecting the personal healthcare data in the limited data set; the user may only use the data within the scope agreed upon in the agreement and assume the responsibility for data security, and after the use of the data is completed, it is appropriate to return, completely destroy or otherwise dispose of the data in accordance with the controller’s requirements. Users may not disclose data to third parties without the permission of the controller.\nn) If the controller obtains health-related data that does not identify the individual after aggregation and analysis of personal health data, the data is no longer personal information, but its use and disclosure are subject to other relevant national regulatory requirements.o) If the controller needs to provide the corresponding data outside of China for academic research, after the necessary de-identification process and after discussion and approval by the Data Security Committee, non-confidential and non-important data within 250 items can be provided, otherwise it is appropriate to submit it to the relevant department for approval.\np) Data that does not involve state secrets, important data or other data that are prohibited or restricted from being provided outside the country, the controller may provide personal health data to overseas destinations with the consent of the subject’s authorization and the consent of the Data Security Committee for discussion and approval, and it is appropriate to control the cumulative amount of data within 250 items, otherwise it is appropriate to submit it to the relevant department for approval.\nq) It is not appropriate for the controller to store health data in servers outside the country, and not to host or lease servers outside the country.\nr) When the controller cooperates in data development and utilization, it is appropriate to adopt the open form of “data analysis platform” and strictly control the disclosure of data use.\n\n8. Key Safety Measures8.1 Graded Key Safety MeasuresData grading can be carried out according to the needs of data protection, and different security protection measures can be implemented for different levels of data, focusing on authorization management, identity identification, and access control management. For example, the data grading and security measures from the perspective of personal information security risk are shown in Table 3. The data grading and security measures under the scenario of doctors access are shown in 11.1. The data grading and security measures under the scenario of clinical research are shown in 11.3.\nTable 3. Highlights of data grading and security measures from personal information security risks\n\n\n\nData grading\nBusiness requirements，content &amp; users\nScenarios\nFeatures and examples\nKey safety measures\n\n\n\nLevel 1\nBusiness requirements: data can be publicly released;Data Content: Certain statistical data;Users: The general public\nPublic announcement\nFor example, information on remaining inpatient beds, remaining available outpatient numbers\nNeed approval when public announces\n\n\nLevel 2\nBusiness requirements: no need to identify individuals; Data content: general demographic information, various types of medical and health service information; Data users: scientific research and education and others\nManagement, Research, Education and Statistical Analysis\nNo need to identify individuals, e.g., case analysis, various types of disease distribution statistics, epidemiological studies, disease cohort studies, etc. Examples of scenarios: clinical research, medical health education, drug&#x2F;medical device development\nIt is desirable to de-identify and control by agreement or enclave public sharing model, and it is desirable to ensure the integrity and authenticity of the data\n\n\nLevel 3\nBusiness requirements: the person to be served is personally identifiable, but the surrounding people not easily identify him&#x2F;her. Data content: Part of the personally identifiable information or code, separated from other information content, such as Zhang ××, queue number, etc. Data users: small range of people\nNotify the person to be served\nNotify service recipients on public occasions, such as outpatient call, examination call, medical examination service call, etc.\nPersonal information needs to be partially masked, and the environment and the number of recipients are limited\n\n\nLevel 4\nBusiness requirements: must accurately identify individuals. Data content: contains complete and accurate personal health data. Data users: relatively small range of personnel, audit and privacy obligations\nPersonalized service and management\nMust accurately identify individuals, such as medical services for individuals, healthcare services, infectious disease control, genome sequencing, etc. Examples of scenarios: hospital interconnection, telemedicine, health sensor data management, mobile applications, commercial insurance integration\nAs it involves personally identifiable information, strict control between the environment and the recipient is desirable, and high standards are desirable to ensure data integrity and availability\n\n\nLevel 5\nBusiness requirements: necessary for the treatment of special diseases. Data content: Detailed information on special diseases. Data users: a very small range of personnel, audited, with confidentiality obligations\nMedical service on special diseases\nThe diseases are sensitive such as HIV etc.\nStrict identity identification, access control and other measures\n\n\n8.2\tBy-Scenario Key Safety MeasuresBased on the different scenarios of data circulation and use, the security aspects and responsibilities involved in the use of health data are different for each role, which determines the security control requirements to be met by each role. The security responsibilities and security measures that each role should take in different application scenarios shown in Table 4, and the security measures that need to be focused on for common scenarios are detailed in Chapter 11.\nTable 4. Highlights of data use security responsibilities and safety measures\n\n\n\n\nScenarios\nSecurity Point\nSecurity responsibilities and safety measures\nExamples\n\n\n\nSubjects -&gt; Controller data flow\nCollection SafeTransmission SafeStorage Safe\nControllers: Informed consent for data collectionControllers: Encryption, storage media controlControllers: Domestic storage, encryption, classification and grading, de-identification, backup recovery, storage media control\nExamples of scenarios: doctors access medical data, health sensing data, mobile applications. Subject: Individual. Controllers: Medical institutions, research institutions, health insurance institutions, commercial insurance companies, health service companies\n\n\nController-&gt;subjects data flow.\nTransmission SafeUse Safe\nControllers: Encryption, storage media controlControllers: identity identification, access control, sensitive data control\nScenario example: Patient query. Subject: Individual. Controller: Medical institution\n\n\nController&lt;-&gt;controller intra-data flow.\nCollection SafeProcess SafeUse SafeStorage Safe\nControllers: informed consent for data collection, approvalProcessors: de-identification, permission management, quality management, metadata managementControllers: approval of authorization, identity authentication, access control, auditingControllers: domestic storage, encryption, classification and grading, de-identification, backup recovery, storage media control\t\nExample of scenario: Internal data usage. Controllers: Medical institution\n\n\nController&lt;-&gt;processor data exchange.\nTransmission safeProcess SafeStorage Safe\nController: Before transmission review, evaluation, authorization; encryption, audit, traffic control, storage media control.Processors: data transmission encryption, transmission protocol controlProcessors: de-identification, permission management, quality management, metadata management, auditing.\tController: Domestic storage, encryption, classification and grading, de-identification, backup recovery, storage media control, manage the process of how processors storage data. Processors: Domestic storage, encryption, classification and grading, de-identification, backup recovery, storage media control, deletion mechanism.\nExample of scenario: Medical device maintenance. Controllers: Medical institutions, government agencies. Processors: research institutions, health information service providers, medical device manufacturers.\n\n\nInter-controller data exchange.\nTransmission SafeUse SafeStorage Safe\nController A: Integration security, encryption, auditing, traffic control, storage media control. Controller B: Integration security, encryption, auditing, traffic control, storage media controlController A: Approval of authorization, authentication, access control, auditing. Controller B: Approval of authorization, identification, access control, auditingController A: Domestic storage, encryption, classification and grading, de-identification, backup recovery, storage media control, deletion mechanism. Controller B: Domestic storage, encryption, classification and grading, de-identification, backup recovery, storage media control, deletion mechanism\t\nExamples of scenarios: Interoperation; telemedicine.Controllers: government agencies, medical institutions, health insurance agencies\n\n\nControllers -&gt; users data flow.\nTransmission SafeUse SafeStorage Safe\nController: Before transmission review, evaluation, authorization; encryption, audit, traffic control, storage media controlUsers: approval of authorization, identity authentication, access control, auditing\tController: Domestic storage, encryption, classification and grading, de-identification, backup recovery, storage media control, manage the process of how users storage data. User: Domestic storage, encryption, classification and grading, de-identification, backup recovery, storage media control, deletion mechanism\t\nExamples of scenarios: commercial insurance interface, clinical research, reuse. Controller: Medical institutions. Users: Commercial insurance companies, research institutions.\n\n\nNote: In the actual application scenario of data, there is a controller corresponding to multiple use scenarios, when it is necessary to implement security measures with reference to multiple data exchange scenarios.\n8.3  Key Safety Measures for Public OpenDifferent forms of data openness are applicable to:a) The “principle of least necessary” is followed.b) The purpose, content, and users of the open data are approved by the Data Security Committee to ensure compliance with the requirements of legality, legitimacy, and necessity.c) To the extent possible, de-identify the data according to the purpose of use.d) Clarify the purpose of data development and use, the security responsibilities to be borne by the user, the security measures, etc., and sign the corresponding agreement; it is appropriate to conduct security assessment in accordance with the regulations for those involving transfer from the countries, and to conduct assessment and approval in accordance with the regulations for those involving important data. In addition, the security measures that need to be satisfied in different forms of data opening are detailed in Table 5.\nTable 5. Key points of security measures for different ways of data opening\n\n\n\nWays of data opening\nKey security measures\n\n\n\nPublic website\nIt is advisable to have the data security committee approve the public data\n\n\nFile Share\n- 1) It is desirable to use cryptographic techniques to safeguard data integrity and traceability.  - (2) it is desirable to audit the volume, content and generated time of documents. - (3) Data transmitted via mobile media should be encrypted or access-controlled by mobile media solutions\n\n\nAPI interface\n(1) it is desirable to use password, cryptography, biotechnology and other identification technologies to identify the access user. (2) it is desirable to use verification technology or cryptographic technology to ensure the integrity of data in the communication process, and to ensure the confidentiality of data in the transmission process through encryption and other means, and the selection of encryption technology is desirable to consider the application scenario, data scale, efficiency requirements and other aspects; (3) It is desirable to conduct log auditing of API calls, including but not limited to caller, call time, call interface name, call result, etc. (4) it is desirable to take Web security measures to prevent attacks such as SQL injection, XSS, burst password, etc.\n\n\nOnline query\n(1) anonymously accessible data are approved by the Data Security Committee to ensure that no personal information, important data, etc. are involved. (2) it is desirable to use password, cryptography, biotechnology and other identification technologies to identify the querying user. (3) it is desirable to use verification technology or cryptographic technology to ensure the integrity of data in the communication process, and to ensure the confidentiality of data in the transmission process through encryption and other means, and the selection of encryption technology is appropriate to consider the application scenario, data scale, efficiency requirements, etc. (4) it is desirable to audit the data volume, number of queries and query time of queries and form exception reports. (5) it is desirable to monitor bulk query operations and alert in time when high-frequency queries are found. (6) it is desirable to take Web security measures to prevent attacks such as SQL injection, XSS, burst password, etc.\n\n\nData analysis Platform\n1) it is appropriate for the export of any analysis results to be approved by the data security committee. 2) it is appropriate to manage the access to the platform, including access permission and data use rights; 3) it is appropriate to have traceability and traceability functions for data operations. 4) The exported data or results should be kept for pending audit\n\n\n９ Guide for Security management9.1 SummaryIn order to achieve the security objectives described in Chapter 5, it is appropriate for controllers to conduct data classification and grading, scenario analysis in accordance with the requirements of GB&#x2F;T 22080-2016 with reference to Chapter 6, analyze the risks faced by health data security, adopt corresponding security measures, and check the effects of the implemented measures for continuous improvement.Controllers can establish data use management methods with reference to Appendix C, approve data applications with reference to Appendix D, sign data processing (use) agreements with processors (users) with reference to Appendix E, and conduct self-inspection with reference to Appendix F.\n9.2 OrganizationIt is advisable to establish a sound organizational guarantee system, with an organizational structure that includes at least a health data security committee and a health data security work group, to ensure good health data security management and to form corresponding documentation, including but not limited to:a) Establish a health data security committee (referred to as the committee) to take overall responsibility for health data security and discuss and decide on major health data security matters, the committee should    - 1) include the top management of the organization and the heads of each business port, etc.    - 2) cover information security, ethics, legal, statistical, audit, confidentiality and other related professionals    - 3) Have the top person in charge of the organization as the chief member.    - 4) Can rely on existing ethics committees, hospital councils, etc., without having to re-establish them.    - 5) coordinating the allocation of human, material, financial and other resources necessary for health data security work, such as security administrators, security auditors, system administrators, etc. based on the principle of separation of authority    - 6） Responsible for reviewing the health data security strategy, risk assessment plan, compliance assessment plan, risk disposal plan and emergency disposal plan.    - 7) Responsible for reviewing data security-related regulations (e.g. data use approval process).    - 8) Responsible for reviewing de-identification strategies and processes.    - 9) Hold regular working meetings, recommended to be held at least once a month.\nb) Establish a health data security work office and designate a person (e.g., data security officer) to be responsible for the day-to-day work of health data security to    - 1) Responsible for implementing the decisions of the Health Data Security Committee and reporting to the Committee.    - 2) Responsible for establishing, maintaining and updating the health data security policy, risk assessment program, compliance assessment program, risk disposal program and emergency disposal program.    - 3） Responsible for establishing, maintaining and updating data security related rules and regulations.    - 4) Responsible for developing, maintaining and updating data use approval processes, as well as de-identification strategies and processes.    - 5) Sorting out business processes and the health information systems and data involved, and conducting security risk analysis and compliance analysis, and making recommendations for health data security work.    - 6) Form and manage the metadata structure to form a data and system supply chain structure that is consistent with business processes.    - 7) Be responsible for data security education and training of personnel to ensure that relevant personnel have appropriate data security capabilities.    - 8) Conduct a comprehensive self-examination of health data security at least annually and make recommendations for rectification.    - 9) Audit the use of health data and adjust and improve security measures when appropriate.    - 10) Monitor and warn the security status of health data, and adjust and improve security measures as appropriate.\n9.3 Process9.3.1 PlanningThe main tasks of the planning phase are as follows, and each task should be documented accordingly.a) Define the scope of the health data security work, determine the work objectives, and establish a work plan.b) Establish a health data security strategy and communicate it to the entire organization.c) Establish rules and regulations related to health data security and inform the whole organization.d) Establish a health data security risk assessment program and a compliance assessment program.e) Sort out health data-related operations and the systems and data involved.f) Identify health data security risks and assess the impact.g) Identify health data security compliance risk points and assess the impact.h) Establish a risk disposal plan for the risks; for those involving data use disclosure, it is appropriate to dispose of them in accordance with Chapter 7 “Use and Disclosure Principles”; for those involving network and system security, it is appropriate to dispose of them in accordance with GB&#x2F;T 22081-1016 and GB&#x2F;T 22239-2019; for those involving basic security and data service security, it is appropriate to dispose of them in accordance with GB&#x2F;T 35274-2017; for cloud computing security, it is appropriate to dispose in accordance with GB&#x2F;T 31168.i) Review and adopt the risk disposal plan.j) Establish a data security emergency disposal plan.\n9.3.2 ImplementThe main tasks of the implementation phase are as follows, and each task should be documented accordingly.a) All aspects of the health data use and disclosure process should strictly implement the established data security-related regulations, security policies and processes.b) Implement a risk management program, including the implementation of selected security measures.c) Prepare appropriate resources, including human, material, and financial resources, to support the security work.d) Conduct necessary information security education and training.e) Implement effective control over the information security work carried out and the resources invested in information security work.f) Take effective response measures for information security incidents.\n9.3.3 CheckThe main tasks of the inspection phase are as follows, and it is appropriate to document each task accordingly.(a) monitor the process of work related to health data security, such as the implementation of security measures process.(b) Regularly review the effectiveness of the implementation of risk management programs, including assessing the acceptability of the residual risk after the implementation of the corresponding measures, etc.(c) Periodically check whether the health data use disclosure complies with Chapter 7 “Use and Disclosure Principles”.(d) Periodically check that technical security and de-identification work is performed in accordance with Chapter 10.(e) Integrate the inspection process into the organization’s internal management.(f) Self-inspection or third-party inspection, as appropriate.\n9.3.4 ImproveThe main tasks of the improvement phase are as follows, and each task should be documented accordingly(a) Improve security measures in response to monitoring or inspection results, including taking preventive measures or adjusting the content of business activities that may affect the security of health data.(b) Establish a corrective action plan and implement it according to the plan.\n9.4 Emergency DisposalThe main work of emergency disposal is as follows, and each work is suitable to form corresponding documentary records.a) Establish emergency plans, including the conditions for starting emergency plans, emergency handling process, system recovery process, incident reporting process, post-event education and training, etc. It is advisable to regularly evaluate and revise the network security emergency plan, and organize at least one emergency drill each year.b) It is advisable to designate special data security emergency support teams and expert teams to ensure that security incidents are dealt with in a timely and effective manner.c) It is desirable to develop a disaster recovery plan to ensure that the health information system can recover from a network security incident in a timely manner and establish a security incident traceability mechanism.d) After the occurrence of data security incidents, it is appropriate to dispose of them in accordance with the emergency plan; after the completion of the disposal of the incident, timely written reports on the incident to the security protection work department in accordance with the regulations, the content of which should include at least: description of the incident, analysis of the causes and effects, disposal methods and other information.e) It is desirable to carry out a comprehensive assessment based on the security issues identified in the detection and assessment, monitoring and early warning and disposal results, and if necessary, to carry out risk identification again and update the security policy.\n10 Security Technology Guide10.1 General Security TechnologyControllers are advisable to manage data security in accordance with GB&#x2F;T 22081-2016, GB&#x2F;T 22239-2019 and GB&#x2F;T 35274-2017, etc.a) It is appropriate to provide the necessary security protection for information systems and network facilities and cloud platforms that host health data.b) It is appropriate to implement data security measures for various activities in the data life cycle, including data collection, data transmission, data storage, data processing, data exchange, data destruction, etc., in order to reduce security risks and ensure data security.c) It is advisable to take necessary security measures for data platforms and applications around the characteristics of each phase of the system life cycle such as planning, development, deployment, operation and maintenance, to establish a secure data management infrastructure, reduce the security risks of data platform and application operation, and guarantee business continuity.d) It is advisable to classify and grade the management of health data, formulate and implement reasonable strategies and processes, and limit the use and disclosure to a minimum.e) It is advisable to implement security measures such as identity identification, access control, security audit, intrusion prevention, malicious code prevention, and media usage management.f) It is appropriate to ensure data quality to meet business needs and implement security measures such as backup recovery and residual information protection.g) It is advisable to use cryptographic technology to ensure the integrity, confidentiality and traceability of data in the process of collection, transmission and storage; if storage media is used for transmission, it is advisable to implement control over the media.h) When storing personal biometric information, it is appropriate to adopt technical measures for processing before storage, such as storing only the summary of personal biometric information.i) It is appropriate to use cryptographic technology in accordance with the relevant requirements of national cryptographic management.j) It is appropriate to comply with the relevant general requirements of important data management, critical information infrastructure security management and other policies.\n10.2 De-identificationIt is appropriate for controllers to carry out de-identification in accordance with GB&#x2F;T37964-2019, and de-identified data should be applied to controlled public sharing or enclave public sharing (an environment fully controlled by controllers), and it is appropriate to agree on the purpose, manner, duration, and security measures for data use through data use agreements. The de-identification strategy, process and results are desirable to be approved by the Data Security Committee. When data are applied to clinical research and pharmaceutical&#x2F;medical R&amp;D, the relevant requirements are as follows.\na) It is appropriate to remove information in personal attribute data that can be uniquely identified with an individual or whose disclosure would have a significant impact on the individual, such as: name; ID card&#x2F;driver’s license and other document numbers; telephone numbers, faxes, e-mails; health insurance numbers, medical record numbers, accounts; biometric information (information unrelated to the purpose of the application such as fingerprints, voice, etc.); photographs; hobbies, beliefs, etc.b) Information in personal attribute data that can be indirectly associated with individuals is appropriate for generalization, conversion, and other processing, such as    - 1) information such as employer, address, postal code, etc., if the employer information or the population covered by the combination with other information is more than 20,000 people, the employer information can be retained; if the address information includes province (municipality directly under the Central Government), city (county), street (township) or the population covered by the combination with other information is more than 20,000 people, it can be retained, otherwise it is appropriate to remove the street (township) to ensure that the population covered by the combination is above; if the population covered by zip code information or combined with other information is above 10,000, it can be retained, otherwise it is appropriate to set the low zip code to ‘0’ to ensure that the population that can be covered is above 10,000.    - 2) Generalization of a specific age, for example, giving an age range. For example, 38 years old can be converted into 30 to 40 years old to ensure that the number of people meeting the same age condition in the same region is more than 20,000.    - 3) Birthdays and all other date information, e.g., admission time, discharge time, can only be specific to the year, or time drift processing.c) It is appropriate to remove the names of health workers and other identifying information.d) It is desirable to have a minimum number of 5 or more people in the dataset with the same value for all attributes.e) For cases that need to be traced back to a patient, it is appropriate to create a patient code index within the controller.f) The configuration of various parameters used in the de-identification process, such as time drift ranges, patient code indexes, and various individual code generation rules, should be kept strictly confidential and limited to the controller’s internal dedicated management.g) When re-identification is required to identify the subject, it is desirable that the process be handled by the controller’s internal personnel and that the process be kept strictly confidential.h) It is desirable to prohibit data users from participating in the work related to de-identification.i) It is advisable to sign a data use agreement to govern the purpose and duration of data use and data protection measures, etc.j) In a controlled public sharing model, it is appropriate for users to record data usage and be audited by the controller.Related examples are shown in Table 6. The identifier categories of health information data elements and the proposed de-identification methods can be found in Appendix G.\nTable 6. Examples of De-identification\n\n\n\nAttribute\nSuggestions for deidentify\nApplicable data\n\n\n\nName\nSuggest to delete or set to empty\nSubject’s name, Doctor’s name, Researcher’s name, Family member’s name\n\n\nContact\nSuggest to delete or set to empty or generalize. For example, the address should only be specified to the city or county level, hiding the address below the county level\nPersonal telephone number, email, account, address\n\n\nDate\nIt is suggested to adopt the “time shifting method”, conversion method, or generalization. For example, different random offset values can be defined for different research projects, and data disturbance can be achieved by adding or subtracting the random offset value from the date and time, to achieve data de-identification. For example: Admission date 2018-01-01 + Random offset value 100 &#x3D; Admission date: 2018-04-11, Discharge date 2018-04-01 + Random offset value 100 &#x3D; Discharge date: 2018-07-10.  Discharge date - admission date &#x3D; 90 days.  Through this method, data de-identification can be ensured while ensuring the correctness of the calculation logic. The conversion method is to replace the result obtained by using it and other dates with operations, such as the length of stay. Generalization only retains the year and month, or even only the year.\nTime information in medical application that can be associated with individuals through data analysis: for example, admission date, discharge date, surgery date, etc\n\n\nBirth date\nIt is suggested to delete, set to empty, or replace with age.\nBirth date\n\n\nAge\nIt is suggested to use the method of “data generalization.” For example: — Age ≤ 89 or &gt; 89 — Age interval &lt;25, 25-29, 30-34, …, 85-89, &gt;89. Note: &gt; 89 cannot be further sub-divided.\nAge\n\n\nNumber\nIt is suggested to delete or set to empty. If it is necessary to use the uniqueness of numbers for logical analysis, such as judging whether multiple medical records belong to the same person through ID numbers, randomization based on the original data can be used to generate unique identification for replacement. If it is necessary to use implicit geographical information such as postal codes, disturbance and generalization methods can be used for processing, such as: original postal code record 100080, de-identified as 100* * * after generalization.\nID card number, social security card number, work permit number, and residency card number.\n\n\ninternal number assigned by medical institutions\nIt is recommended to replace or delete them. If these numbers are needed for logical analysis, a unique identifier can be generated based on the original data through randomization. If these numbers are not needed for logical analysis, they should be deleted.\nExam result report number, check report number, hospitalization number, outpatient (emergency) number, etc.\n\n\n11 Data Security on Typical Scenarios(Skip)","tags":["Data security"]},{"title":"Servitization in Subscription Mode","url":"/2023/02/23/servitization-in-subscription-mode/","content":"IntroductionServitization is shift from a product only focus to a strategy where productsand services are bundled in various combinations. The shift could also be the focus from goods, technology and manufacturing to services, intangible resources, co-creation of value and relationships. Some scholars call it Prodcut Service System aka PSS. Nowadays more and more tranditional companies increase the influence of services in their traditional business to secure their competitive position.It is driven by the growing complexity of products and the demand for individualization. Nowadays the extended target is to enhance the performance of products through the integration of services and digital components.\nServitization ProcessGeneral speaking, there are serveral steps to take so as a company transfers from product-centric to customer-centric \nKey Characterics of subscription modelConclusion","tags":["subscription model, Servitization"]},{"title":"Transformer Intepretion and Code","url":"/2023/04/02/Transformer-Architecture-code/","content":"Attention is all your needAttention is all your need这篇谷歌的transformer开山之作奠定了如今大热的GPT和机器视觉领域神经网络的基础架构。本文将在理解论文的基础上，结合其它材料，进一步深入了解具体代码实现（pytorch），并给出一个fine tune的实际应用例子。\nPaper IntroductionTransformer是第一个提出只使用attention+residual connection+MLP架构的神经网络, 起初论文用这种架构去做序列到序列的文本翻译工作，相比RNN和CNN, transformer在大规模训练集上的表现更好，同时这个架构也提高了计算并行性和计算效率。那么为什么它会表现地更好? 初步的研究发现，原因有如下几点：\n\ntransformer在网络中引入了更少的inductive bias归纳偏置，所以有更好的泛化性，对于没有训练过的样本就有更好的表现; \n同时attention机制可以全局地计算出序列之间的相关性，相对CNN可以更好地理解上下文，而不是局限卷积窗口特征;\n更少的网络深度减少了长距离传输梯度消失的问题，相对RNN就有更好的长输入表现;\n当然transformer引入残差连接也是性能提升的另一个因素，具体还有其他原因分析还待进一步研究。\n\nModel Architecturetransformer使用了encoder-decoder编码器-解码器架构, 整体架构论文完美地画了出来。实现的代码如下:\nimport torch.nn as nnfrom torch.nn.functional import log_softmaxclass EncoderDecoder(nn.Module):    &quot;&quot;&quot;    Standard encoder decoder class.    &quot;&quot;&quot;    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):        super(EncoderDecoder, self).__init__()        self.encoder = encoder        self.decoder = decoder        self.src_embed = src_embed        self.tgt_embed = tgt_embed        self.generator = generator    def encode(self, src, src_mask):        return self.encoder(self.src_embed(src), src_mask)    def decode(self, memory, src_mask, tgt, tgt_mask):        &quot;&quot;&quot;memory is feed by encoder output as interim input for decoder&quot;&quot;&quot;        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)    def forward(self, src, tgt, src_mask, tgt_mask):        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)class Generator(nn.Module):    &quot;&quot;&quot;standard linear + softmax generator&quot;&quot;&quot;    def __init__(self, d_model, vocab):        super(Generator, self).__init__()        self.projection = nn.Linear(d_model, vocab)    def forward(self, x):        return log_softmax(self.projection(x), dim=-1)\n上图中编码器和解码器实际上是由6个一样的网络串行堆叠而成，它的大致架构如下图:\n编码器输入对于翻译序列的例子，在数据输入阶段经过了如下流程：\n\n当输入句子X(x1,x2…xn), 模型先对其做embedding,生成每个词对应的向量Z(z1,z2…zn). \n然后在Z后加入位置编码信息，位置信息可以用余弦来表示，随后加入编码器输入。正弦和余弦的公式是：\n\n\n\nimport mathclass Embeddings(nn.Module):    def __init__(self, d_model, vocab):        super(Embeddings, self).__init__()        self.lut = nn.Embedding(vocab, d_model)  # look up table        self.d_model = d_model    def forward(self, x):        return self.lut(x) * math.sqrt(self.d_model)    class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout, max_len=5000):        &quot;&quot;&quot;dropout is random set to zero in some points to prevent overfitting&quot;&quot;&quot;        super(PositionalEncoding, self).__init__()        self.dropout = nn.Dropout(p=dropout)        pe = torch.zeros(max_len, d_model) # 5000*512 tensor        position = torch.arange(0, max_len).unsqueeze(1) # 5000*1        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model))        # 用指数转变把除法改成乘法, -1幂        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        pe = pe.unsqueeze(0)        # 保存状态        self.register_buffer(&quot;pe&quot;, pe)    def forward(self, x):        # todo why requires gradients?        return self.dropout(x + self.pe[:, : x.size(1)].requires_grad_(False))\n\n编码器层编码器由6个一样的网络串行组成，每个网络由一个多头注意力层和Feed forward前向反馈网络层组成。\ndef clones(module, N):    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])class Encoder(nn.Module):    def __init__(self, layer, N):        super(Encoder, self).__init__()        self.layers = clones(layer, N)        self.norm = LayerNorm(layer.size)    def forward(self, x, mask):        for layer in self.layers:            x = layer(x, mask)        return self.norm(x)\n\n多头注意层多头注意层是输入（K,V,Q）经过8个可学习的线性投影，再经过并行的自注意机制（点积计算），最后把结果相加并且投影缩放回来。这篇论文用的自注意力机制是scaled dot product即阶化的点积。整体自注意力层和多头层论文也很好地画了出来。自注意计算的公式是: \ndef attention(query, key, value, mask=None, dropout=None):    d_k = query.size(-1) # dim number    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)    if mask is not None:        scores = scores.masked_fill(mask==0, -1e9) # 掩码是0的位置赋值很小的数    p_attn = scores.softmax(dim=-1)    if dropout is not None:        p_attn = dropout(p_attn) # set p_attn% to 0    return torch.matmul(p_attn, value), p_attn\n\n多头注意力机制的公式是: \n\nWhere the projections are parameter matrices:\n\n\n多头注意力代码如下:\nimport copyclass MultiHeadAttention(nn.Module):    def __init__(self, d_model, h=8, dropout=0.1):        super(MultiHeadAttention, self).__init__()        self.d_k = d_model // h        self.h = h        self.linears = clones(nn.Linear(d_model, d_model), 4)        self.attn = None        self.dropout = nn.Dropout(p=dropout)    def forward(self, query, key, value, mask=None):        if mask is not None:            mask = mask.unsqueeze(1)        nbatches = query.size(0)        # projection to h * d_k, split by adding dimension        query, key, value = [            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)            for lin, x in zip(self.linears, (query, key, value))        ]        # self attention        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)        # concate and linear again        x = (x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k))        del query        del key        del value        return self.linears[-1](x)\n\n前馈网络前向反馈网络就是一个2层感知机MLP，经过一层ReLU然后再经过一层线性层, 公式如下: \n\nclass PositionwiseFeedForward(nn.Module):    def __init__(self, d_model, d_ff, dropout=0.1):        super(PositionwiseFeedForward, self).__init__()        self.w1 = nn.Linear(d_model, d_ff)        self.w2 = nn.Linear(d_ff, d_model)        self.dropout = nn.Dropout(dropout)    def forward(self, x):        return self.w2(self.dropout(self.w1(x).relu()))\n\nLayerNorm在每个子层输出还需要做layerNorm和残差连接. LayerNorm的大致公式如下图, LayerNorm主要是将样本值在特征维度上做归一化处理:\nclass LayerNorm(nn.Module):    def __init__(self, features, eps=1e-6):        super(LayerNorm, self).__init__()        self.gamma = nn.Parameter(torch.ones(features))        self.beta = nn.Parameter(torch.zeros(features))        self.eps = eps    def forward(self, x):        # 计算均值和方差        mean = x.mean(dim=-1, keepdim=True)        var = x.var(dim=-1, keepdim=True)        # 归一化        x = (x - mean) / torch.sqrt(var + self.eps)        # 线性化        x = self.gamma.unsqueeze(-1) * x + self.beta.unsqueeze(-1)        return x\n\n残差连接每个子层的输出将通过残差连接，然后再通过LayerNorm, 这里代码实现将先进行归一化然后再做dropout(cite)最后再进行残差连接, 目的是为了代码简单。子层输出经过公式:  \nclass SublayerConnection(nn.Module):    &quot;&quot;&quot;    A residual connection followed by a layer norm.    Note for code simplicity the norm is first as opposed to last.    &quot;&quot;&quot;    def __init__(self, size, dropout):        super(SublayerConnection, self).__init__()        self.norm = LayerNorm(size)        self.dropout = nn.Dropout(dropout)    def forward(self, x, sublayer):        &quot;Apply residual connection to any sublayer with the same size.&quot;        return x + self.dropout(sublayer(self.norm(x)))\n\n整个编码层代码如下：\nclass EncoderLayer(nn.Module):    def __init__(self, size, self_attn, feed_forward, dropout):        super().__init__()        self.self_attn = self_attn        self.feed_forward = feed_forward        self.sublayer = clones(SublayerConnection(size, dropout), 2)        self.size = size    def forward(self, x, mask):        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))        return self.sublayer[1](x, self.feed_forward)\n\n解码器层解码器层比编码器多了一个带mask的多头注意力层，同时编码器的输出也会是解码器的输入，另一个输入是编码器按位之前已经生成结果。\nclass Decoder(nn.Module):    def __init__(self, layer, N):        super().__init__()        self.layers = clones(layer, N)        self.norm = LayerNorm(layer.size)        def forward(self, x, memory, src_mask, tgt_mask):        for layer in self.layers:            x = layer(x, memory, src_mask, tgt_mask)        return self.norm(x)class DecoderLayer(nn.Module):    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):        &quot;&quot;&quot;(k,v) from sources called src_attn&quot;&quot;&quot;        super().__init__()        self.size = size        self.self_attn = self_attn        self.src_attn = src_attn        self.feed_forward = feed_forward        self.sublayer = clones(SublayerConnection(size, dropout), 3)    def forward(self, x, memory, src_mask, tgt_mask):        m = memory        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))        return self.sublayer[2](x, self.feed_forward)def subsequent_mask(size):    &quot;Mask out subsequent positions.&quot;    attn_shape = (1, size, size)    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(        torch.uint8    )    return subsequent_mask == 0\n\n整体模型def make_model(    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):    &quot;Helper: Construct a model from hyperparameters.&quot;    c = copy.deepcopy    attn = MultiHeadedAttention(h=h, d_model=model)    ff = PositionwiseFeedForward(d_model=model, d_ff=d_ff, dropout=dropout)    position = PositionalEncoding(d_model, dropout)    model = EncoderDecoder(        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),        Generator(d_model, tgt_vocab),    )    # This was important from their code.    # Initialize parameters with Glorot / fan_avg.    for p in model.parameters():        if p.dim() &gt; 1:            nn.init.xavier_uniform_(p)    return model# 没有训练的模型测试记录能力，结果是不能记忆输入。def inference_test():    test_model = make_model(11, 11, 2)    test_model.eval()    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])    src_mask = torch.ones(1, 1, 10)    memory = test_model.encode(src, src_mask)    ys = torch.zeros(1, 1).type_as(src)    for i in range(9):        out = test_model.decode(            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)        )        prob = test_model.generator(out[:, -1])        _, next_word = torch.max(prob, dim=1)        next_word = next_word.data[0]        ys = torch.cat(            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1        )    print(&quot;Example Untrained Model Prediction:&quot;, ys)def run_tests():    for _ in range(10):        inference_test()def show_example(fn, args=[]):    if __name__ == &quot;__main__&quot;:        return fn(*args)show_example(run_tests)\n\n","tags":["transformer, AI"]},{"title":"Use CDS Authoring Tool","url":"/2023/06/12/Use-CDS-Authoring/","content":"使用CDS作者工具\nCDS Connect 作者工具是一种用于创建临床决策支持规则的工具。此工具提供了一个用户友好的界面，供医疗保健专业人员和技术人员使用。它允许用户创建、编辑和管理临床决策支持规则，这些规则可以帮助医生在临床实践中做出更明智的决策。\n\n导言CDS Connect项目开源了一个工具Authoring tool用于快速创建临床辅助的规则、内容。有了这个工具可以方便地编写CQL（Clinical Quality Language）表达，而不用非常了解语法细节。该工具使用基于标准化的临床知识表示形式，如CQL和FHIR（Fast Healthcare Interoperability Resources）。用户可以根据特定的临床指南、研究结果和实践经验，使用这些规则来定义临床决策的逻辑和条件。CDS Connect 作者工具还提供了验证和测试功能，以确保创建的规则能够正确地应用于真实的临床环境。它还支持与其他系统的集成，可以将创建的规则导出为可用于不同临床决策支持系统的格式。下面就配合官方文档，简单地用一个例子作一下演示。\n基本概念\nElement元素。CDS每条规则由元素组成，每个元素描述了一条规则用于决定某个患者是否满足临床辅助推荐的标准。\nInclusion包含。即患者满足某个元素的一条标准。\nExclusion排除。即患者不满足某个推荐要求。\nSubpopulation亚群体。即亚群体需要更多的具体推荐才能作出决定。\nBase Element基础元素。可以自定义的基础元素。\nRecommendation推荐。当满足条件是cds给出的建议内容。\nParameter参数。可以在运行时动态的输出参数。\n\n创建和编辑元素流程通常步骤包括：\n\n首先选择一个元素，元素通常是一个FHIR资源类型，如Condition或Observation。元素的类型也会取决哪些数据需要从患者数据（病例记录）中来。具体的元素类型有：\n\nAllergy Intolerance过敏不耐受: Instances of the FHIR AllergyIntolerance resource type.\nBase Elements基本元素: Re-usable elements defined in the “Base Elements” tab. 可重复使用的基本元素。\nCondition病情: Instances of the FHIR Condition resource type.\nDemographics人口统计学: Age or Gender as specified in an instance of the FHIR Patient resource type\nDevice设备: Instances of the FHIR Device resource type.\nEncounter就诊: Instances of the FHIR Encounter resource type.\nExternal CQL外部CQL: Named CQL definitions, parameters, and functions from CQL files uploaded in the “External CQL” tab. 来自“外部CQL”标签中上传的命名CQL定义、参数和函数.\nImmunization免疫: Instances of the FHIR Immunization resource type\nMedication Statement用药记录: Instances of the FHIR MedicationStatement resource type\nMedication Request用药请求: Instances of the FHIR MedicationRequest(STU3&#x2F;R4) or MedicationOrder(DSTU2) resource type\nObservation观测: Instances of the Observation resource type\nParameters参数: Parameter values for parameters defined in the “Parameters” tab. 在“参数”标签中定义的参数值.\nProcedure手术: Instances of the FHIR Procedure resource type\nServiceRequest服务请求: Instances of the FHIR ServiceRequest resource type, available only in FHIR R4\n\n\n然后根据所选元素类型，赋予更多的具体含义，如赋予最少一个键值对（这里的键值对是指一组code，这些code会用来匹配患者病历的记录。举个例子，糖尿病会包含多个code来代表不同糖尿病诊断，如一型、二型糖尿病。同时不同的编码系统如ICD-9,ICD-10,SNOMED-CT又有不同值。所以使用键值对可以方便地把所有对应的编码code一次性地和患者电子病历记录匹配起来）或者code编码（单独赋予code需要指定编码系统并验证，如果是第三方编码系统需要指定FHIR兼容的URL），如Condition-&gt;Diabetes症状对应糖尿病，Observation-&gt;LDL Cholesterol Test观测对应LDL胆固醇检测；又如通过表单提供额外的信息(目前仅用于人口统计学)。\n\n命名元素。所有的元素可以被自定义命名，以便更好地体现用意。\n\n备注元素。所有元素也支持被标注，添加评论, 但是评论是不会参与决策逻辑的判别。评论主要的应用场景有：\n\n提供创建该元素的理由，或指示在创建该元素时做出的决策以及为何做出该决策。\n提供对该元素所包含的复杂逻辑的简要总结，或进一步解释构建该元素的目的。\n指示元素逻辑的来源并提供必要的参考文献。\n指示实施者可能基于特定场地需求而希望修改的逻辑部分。\n\n\n修改结果。通过自定义修改或者选择内建的表达式进一步过滤结果，得到元素类型需要得出的结果。工具自带的修改器能覆盖一般的场景，同时修改器还可以串联逻辑，应用多个过滤条件得到最终结果。\n\n\n5.1 （可选）使用自建修改器定义逻辑。如果作者想进行更加精准的控制，可以自建逻辑，有的时候自建修改器可能不如外部导入CQL方便。当前版本（June, 2023）自定义修改器只能对 FHIR 资源实例的列表进行过滤。未来的版本可能提供额外的功能，如排序和返回特定属性。\n6.测试artifact。在将 CDS 逻辑部署到测试环境之前进行测试，可以让作者在流程早期发现和修复错误，从而节省时间和金钱。CDS创作工具的测试允许作者上传其自己合成的测试患者，这些患者以FHIR资源形式存在。现存的患者测试数据构造工具主要 CQL Testing Framework , Synthea , ClinFHIR , and FHIR® Shorthand .然后，作者可以针对其中一个或多个患者运行他们的CDS逻辑，并检查结果以确定其是否符合预期。CDS创作工具目前不提供测试患者编辑器，也没有提供结果自动验证的机制（例如，“测试断言”）。对于更高级的测试功能，请考虑使用CDS Connect的CQL测试框架。\n虽然元素本身可以很有用，但通常它们会与其他元素结合使用，以表示更复杂的思想或要求。CDS 作者工具支持以下元素组合方式：\n\n与（And）：要求一组布尔元素中的每个元素都为真。\n或（Or）：要求一组布尔元素中至少有一个元素为真。\n缩进组（Indented Group）：将一组元素组合在一起，表示一个单一的逻辑单元。\n交集（Intersect）：找出一组元素中每个元素中都出现的项目集合。使用“交集”组合方式表示应检查每个元素，并仅将与每个元素匹配的项目包含在交集结果中。例如，如果将表示已确认的心肌梗塞的元素与表示过去六个月内的心肌梗塞的元素使用“交集”进行组合，结果将仅包含同时在两个元素集合中的心肌梗塞（即已确认且在过去六个月内）。“交集”组合方式只能在“基本元素（Base Elements）”选项卡中使用“列表操作（List Operations）”元素类型进行应用。\n并集（Union）：将多个元素中的项目合并为一个项目集合。使用“并集（Union）”组合方式表示应将所有元素中的所有项目合并为一个项目集合。例如，使用“并集”将LDL-c元素与HDL-c元素组合，将得到所有的LDL-c和HDL-c观测结果。并集只能在“基本元素（Base Elements）”选项卡中使用“列表操作（List Operations）”元素类型应用“并集”组合方式。\n\nSubpopulation亚群体页作者可以使用“亚人群（Subpopulations）”选项卡来指定将患者分组，组内的亚群体适用标准为更具体的相关建议。虽然亚人群不是必需的，但对于需要提供更细致建议或与特定人群相关的建议而言，它们非常有用。例如，他汀类药物artifact可能会针对10年风险评分为8%的患者和10年风险评分为12%的患者提供不同强度的建议。“亚人群”选项卡允许作者创建所需的亚人群。对于每个亚人群，作者必须指定一个唯一的名称，然后根据上述部分的描述创建和组合元素。\n基本元素作者可以使用“基本元素（Base Elements）”创建artifact，基础元素可以是在上下文中多次使用。这样一来，常用元素只需定义一次，就可以在需要的任何地方使用，并可进行进一步修改。作者可以在任何特定上下文之外定义独立的元素，并将这些元素按原样导出为CQL。基于基本元素创建的元素会以浅蓝色进行阴影处理，以便更容易区分。此外，在元素定义的内容中使用“基本元素”标签列出了原始基本元素的名称。如果您点击基本元素名称最右侧的链接图标，将直接进入基本元素的定义页面。请注意，在使用基本元素时会有一些限制。其中之一是您不能删除它。要删除正在使用的基本元素，必须首先删除（或编辑）其所有用到的地方。另一个限制是，如果删除应用表达式会改变基本元素的返回类型，则无法删除该表达式。这确保修改基本元素不会使其使用变得无效。为了删除表达式，请首先删除工件中所有对基本元素的使用。此外，在正在使用的基本元素上无法添加修改器，除非修改器不会改变元素的整体返回类型。如果修改器会改变基本元素的返回类型，则必须先删除工件中所有对基本元素的使用，然后再添加修改器。\n推荐页面Recommendation推荐页面是CDS artifact在执行后会给临床医生一个推荐，推荐的具体内容可以被编辑并伴随合理的推荐理由。推荐也可以分亚群定制不同的内容。\n参数在CDS运行时输入参数可以动态的输出结果。参数可以有默认值，主要类型有:Boolean, Code, Concept, Integer, DateTime, Decimal, Quantity, String, Time, Interval&lt;Integer&gt;, Interval&lt;DateTime&gt;, Interval&lt;Decimal&gt;, and Interval&lt;Quantity&gt;\n错误处理作者使用“处理错误”选项卡来定义在特定错误条件发生时向最终用户提供哪些信息。例如，作者可能希望在患者健康记录中缺少必要数据时返回错误消息。处理错误是可选的，对于某些用例可能不需要或不实用。\n外部CQL外部CQL可以通过页面上传并校验格式，外部CQL特别适合某些复杂数学计算和时间关系此工具处理不了的情况。同时外部CQL是read-only的类型。\n"},{"title":"FHIR Terminology Service","url":"/2023/08/10/FHIR-Terminology-Service/","content":"FHIR Terminology Service 术语服务\n本文将正对HL7 FHIR R4的术语模块做一下介绍，包括官方文档和实现指引。\n\nFHIR 术语模块的设计思路HL7 的术语模块提供了一个整体的设计用于指引大家如何使用术语资源，以及相应的操作，数据类型，及外部和自定义术语库如何展示和交换信息。最终的目的是为了提供标准的术语服务支持 FHIR 资源间使用标准的编码系统。术语模块主要有 5 个资源:\n\nCodeSystem: 编码系统，描述术语的关键元素和定义。\nConceptMap: 定义一个编码系统的概念与另一个的映射关系。\nValueSet: 将一个或多个编码系统中用于特定目的的一组代码进行组合, 形成有特定含义的值集。\nNamingSystem: 命名系统。\nTerminologyCapabilities: 术语能力声明。\n\n他们之间的关系可以用下图表示，\n实际使用中，我们在对接不同系统时可能只需做一下转换，把一个编码系统的值集映射成另一个编码系统的值集, 可能不一定需要命名系统，如下图所示。\n术语服务HL7 也给出标准的术语服务实施规范，用户在使用这个服务的时候可以做到不用十分了解编码系统、值集和映射概念。如果一个服务能满足如下列表要求，就可以在 FHIR 能力声明中宣称符合 FHIR 术语服务标准Terminology Service Capability Statement：\n\n安全\n值集拓展, $expand 从值集中扩展返回一组编码列表，实际使用时类似高级 search。\n概念查找&#x2F;分解, $lookup 查找资源的一个概念，值集或关系。\n值集验证， $validate-code 验证一个编码在值集中是否有效。\n包容性测试，$subsumes 查询两个代码之间是否存在（如果有的话）包含关系。\n批量验证\n翻译,$translate 使用 ConceptMap 资源，查找特定源概念被映射到的目标编码系统的概念。\n批量翻译\n可以维护一个闭环表\n\n服务的安全要求SSL 加密传输是强制要求。如果值集系统允许被实时维护，认证和审计机制也是必要的。\n熟悉基本概念由于术语服务是基于几个基本资源的，所以熟悉这些资源和使用方法对实现十分重要。基本资源包括：CodeSystem,ValueSet, ConceptMap. 以及熟悉如何在 FHIR 中使用编码(codesystem)。\n如何在 FHIR 中使用编码FHIR 中的资源很多会用到编码，这些编码往往是一串固定的值，代表一个特定的含义或概念。FHIR 这里的编码定义都是通过一对组合:system和code. system的值是一个 url 指出哪里定义了这个编码，这个值需要区分大小写。\n\n\n\nKey\nValue\n\n\n\nsystem\nURI 指定编码的位置\n\n\nversion\n版本\n\n\ncode\n字符串表示一个概念\n\n\ndisplay\n描述文字\n\n\n编码在资源中定义时可以有多种数据类型，但都代表编码。\n\n 当编码在资源中定义时，可以有如下数据类型\n code这数据类型只展示编码`code`, `system`隐含在元素中。\n Coding这个数据类型定义一个标准的`code`和`system`对。\n CodeableConcept`coding`另加上一个直白的文本字段.\n 另外，非资源元素定义时，如下字段也可以带着编码.或者内容当成编码，并绑定一个值集。\n Quantity数量这个字段可以用`system`andcode定义单位的类型.\n string有的时候string字符串也可以用来控制某个元素固定的几个值.\n uri类似string, uri也可以当成编码元素\n\n\n选择编码系统system对应的 url 需要指向一个编码系统。编码系统可以有如下地址可以引用:\n\n本规范指定的编码系统仓库code system registry\n编码系统发布者定义的 URI 或者 OID\nFHIR 社区的编码系统仓库，并且状态是active\n在HL7 OID registry注册的 OID\n\n下表是所有外部编码系统：\n\n\n\nURI\nSource\nComment\nOID (for non-FHIR systems)\n\n\n\nhttp://snomed.info/sct\nSNOMED CT (IHTSDO)\nSee Using SNOMED CT with FHIR\n2.16.840.1.113883.6.96\n\n\nhttp://www.nlm.nih.gov/research/umls/rxnorm\nRxNorm (US NLM)\nSee Using RxNorm with FHIR\n2.16.840.1.113883.6.88\n\n\nhttp://loinc.org\nLOINC (LOINC.org)\nSee Using LOINC with FHIR\n2.16.840.1.113883.6.1\n\n\nhttp://unitsofmeasure.org\nUCUM: (UnitsOfMeasure.org) Case Sensitive Codes\nSee Using UCUM with FHIR\n2.16.840.1.113883.6.8\n\n\nhttp://ncimeta.nci.nih.gov\nNCI Metathesaurus\nSee Using NCI Metathesaurus with FHIR\n2.16.840.1.113883.3.26.1.2\n\n\nhttp://www.ama-assn.org/go/cpt\nAMA CPT codes\nSee Using CPT with FHIR\n2.16.840.1.113883.6.12\n\n\nhttp://hl7.org/fhir/ndfrt\nNDF-RT (National Drug File – Reference Terminology)\nSee Using NDF-RT with FHIR\n2.16.840.1.113883.6.209\n\n\nhttp://fdasis.nlm.nih.gov\nUnique Ingredient Identifier (UNII)\nSee Using UNII with FHIR\n2.16.840.1.113883.4.9\n\n\nhttp://hl7.org/fhir/sid/ndc\nNDC&#x2F;NHRIC Codes\nSee Using NDC with FHIR\n2.16.840.1.113883.6.69\n\n\nhttp://hl7.org/fhir/sid/cvx\nCVX (Vaccine Administered)\nSee Using CVX with FHIR\n2.16.840.1.113883.12.292\n\n\nurn:iso:std:iso:3166\nISO Country &amp; Regional Codes\nSee Using ISO 3166 Codes with FHIR\n1.0.3166.1.2.2\n\n\nhttp://hl7.org/fhir/sid/dsm5\nDSM-5\nDiagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5)\n2.16.840.1.113883.6.344\n\n\nhttp://www.nubc.org/patient-discharge\nNUBC code system for Patient Discharge Status\nNational Uniform Billing Committee, manual UB-04, UB form locator 17\n2.16.840.1.113883.6.301.5\n\n\nhttp://www.radlex.org\nRadLex\n(Includes play book codes)\n2.16.840.1.113883.6.256\n\n\nICD-9, ICD-10\nWHO &amp; National Variants\nSee Using ICD-[x] with FHIR\nSee ICD page for details\n\n\nhttp://hl7.org/fhir/sid/icpc-1\nICPC (International Classification of Primary Care)\nNHG Table 24 ICPC-1 (NL)\n2.16.840.1.113883.2.4.4.31.1\n\n\nhttp://hl7.org/fhir/sid/icf-nl\nICF (International Classification of Functioning, Disability and Health)\n\n2.16.840.1.113883.6.254\n\n\nhttp://terminology.hl7.org/CodeSystem/v2-[X](/v)\nVersion 2 tables\n[X] is the 4 digit identifier for a table; e.g. http://terminology.hl7.org/CodeSystem/v2-0203\n2.16.840.1.113883.12.[X]\n\n\nhttp://terminology.hl7.org/CodeSystem/v3-[X]\nA HL7 v3 code system\n[X] is the code system name; e.g. http://terminology.hl7.org/CodeSystem/v3-GenderStatus\nsee v3 list\n\n\nhttps://www.gs1.org/gtin\nGTIN (GS1)\nNote: GTINs may be used in both Codes and Identifiers\n1.3.160\n\n\nhttp://www.whocc.no/atc\nAnatomical Therapeutic Chemical Classification System (WHO)\n\n2.16.840.1.113883.6.73\n\n\nurn:ietf:bcp:47\nIETF language (see Tags for Identifying Languages - BCP 47)\nThis is used for identifying language throughout FHIR. Note that usually these codes are in a code and the system is assumed\n\n\n\nurn:ietf:bcp:13\nMime Types (see Multipurpose Internet Mail Extensions (MIME) Part Four - BCP 13)\nThis is used for identifying the mime type system throughout FHIR. Note that these codes are in a code (e.g. Attachment.contentType) and in these elements the system is assumed. This system is defined for when constructing value sets of mime type codes\n\n\n\nurn:iso:std:iso:11073:10101\nMedical Device Codes (ISO 11073-10101)\nSee Using MDC Codes with FHIR\n2.16.840.1.113883.6.24\n\n\nhttp://dicom.nema.org/resources/ontology/DCM\nDICOM Code Definitions\nThe meanings of codes defined in DICOM, either explicitly or by reference to another part of DICOM or an external reference document or standard\n1.2.840.10008.2.16.4\n\n\nhttp://hl7.org/fhir/NamingSystem/ca-hc-din\nHealth Canada Drug Identification Number\nA computer-generated eight-digit number assigned by Health Canada to a drug product prior to being marketed in Canada. Canada Health Drug Product Database contains product specific information on drugs approved for use in Canada.\n2.16.840.1.113883.5.1105\n\n\nhttp://hl7.org/fhir/sid/ca-hc-npn\nHealth Canada Natural Product Number\nA computer-generated number assigned by Health Canada to a natural health product prior to being marketed in Canada.\n2.16.840.1.113883.5.1105\n\n\nhttp://nucc.org/provider-taxonomy\nNUCC Provider Taxonomy\nThe Health Care Provider Taxonomy code is a unique alphanumeric code, ten characters in length. The code set is structured into three distinct “Levels” including Provider Type, Classification, and Area of Specialization.\n2.16.840.1.113883.6.101\n\n\n\nCode Systems for Genetics\n\n\n\n\nhttp://www.genenames.org\nHGNC: Human Gene Nomenclature Committee\n\n2.16.840.1.113883.6.281\n\n\nhttp://www.ensembl.org\nENSEMBL reference sequence identifiers\nMaintained jointly by the European Bioinformatics Institute and Welcome Trust Sanger Institute\nnot assigned yet\n\n\nhttp://www.ncbi.nlm.nih.gov/refseq\nRefSeq: National Center for Biotechnology Information (NCBI) Reference Sequences\n\n2.16.840.1.113883.6.280\n\n\nhttp://www.ncbi.nlm.nih.gov/clinvar\nClinVar Variant ID\nNCBI central repository for curating pathogenicity of potentially clinically relevant variants\nnot assigned yet\n\n\nhttp://sequenceontology.org\nSequence Ontology\n\nnot assigned yet\n\n\nhttp://varnomen.hgvs.org\nHGVS: Human Genome Variation Society\n\n2.16.840.1.113883.6.282\n\n\nhttp://www.ncbi.nlm.nih.gov/projects/SNP\nDBSNP: Single Nucleotide Polymorphism database\n\n2.16.840.1.113883.6.284\n\n\nhttp://cancer.sanger.ac.uk/cancergenome/projects/cosmic\nCOSMIC: Catalogue Of Somatic Mutations In Cancer\n\n2.16.840.1.113883.3.912\n\n\nhttp://www.lrg-sequence.org\nLRG: Locus Reference Genomic Sequences\n\n2.16.840.1.113883.6.283\n\n\nhttp://www.omim.org\nOMIM: Online Mendelian Inheritance in Man\n\n2.16.840.1.113883.6.174\n\n\nhttp://www.ncbi.nlm.nih.gov/pubmed\nPubMed\n\n2.16.840.1.113883.13.191\n\n\nhttp://www.pharmgkb.org\nPHARMGKB: Pharmacogenomic Knowledge Base\nPharmGKB Accession ID\n2.16.840.1.113883.3.913\n\n\nhttp://clinicaltrials.gov\nClinicalTrials.gov\n\n2.16.840.1.113883.3.1077\n\n\nhttp://www.ebi.ac.uk/ipd/imgt/hla\nEuropean Bioinformatics Institute\n\n2.16.840.1.113883.6.341\n\n\n编码系统的复杂表达由于真实世界经常涉及通过组合多个概念或属性来创建复合术语，同时很多编码系统内部的概念之间会有各种关系，比如相等，包含，属于等等多种关系。编码系统如SNOMED-CT就设计了表达式来支持更细节的临床意义表达。这些特征可以在CodeSystem资源来展示，并通过上述的几种数据类型进行数据交换。\n&#123;  &quot;system&quot; : &quot;http://snomed.info/sct&quot;,  &quot;code&quot; : &quot;80146002 |appendectomy| : 260870009 |priority| = 25876001 |emergency|&quot;&#125;## 阑尾炎切除手术紧急 concept: refinement(key=value)\n\n元素编码值绑定当一个元素和值集绑定时，绑定有如下属性：\n\n\n\nName\nDescription\n\n\n\nStrength\n绑定的强度，灵活度\n\n\nReference\nURL 指定值集路径\n\n\nDescription\n编码使用的描述信息\n\n\n几乎所有的元素都有被编码的数据类型，这个数据类型和一个值集绑定。绑定的强度可以有多种程度：\n\n\n\nstrength\nDescription\n\n\n\nrequired\n必须遵循，元素中的这个概念必须被指定值集\n\n\nextensible\n必须遵循，值集的绑定可以通过扩展方式提供可替代的概念。但不是扩展值集本身\n\n\npreferred\n推荐使用特定编码来提高互操作性，但不是强制要求\n\n\nexample\n没有例子可以使用来绑定值集\n\n\n$expand 操作术语服务的“值集扩展”是指将值集从另一个编码系统引入编码的过程, 为了使其概念更容易在特定上下文中使用和处理。该过程确保了值集所代表的信息在真实复杂的医疗环境中被明确理解，并且可以轻松地被各种医疗保健应用程序和系统使用。\n### 扩展查询第23值集，过滤“abdo”GET [base]/ValueSet/23/$expand?filter=abdoExpanding a value set that is specified by the client (using JSON):POST [base]/ValueSet/$expand[other headers]&#123;  &quot;resourceType&quot; : &quot;Parameters&quot;,  &quot;parameter&quot; : [     &#123;     &quot;name&quot; : &quot;valueSet&quot;,     &quot;resource&quot; : &#123;       &quot;resourceType&quot; : &quot;ValueSet&quot;,     [value set details]     &#125;   &#125;  ]&#125;\n\n## 返回HTTP/1.1 200 OK[other headers]&lt;ValueSet xmlns=&quot;http://hl7.org/fhir&quot;&gt;  &lt;!-- the server SHOULD populate the id with a newly created UUID    so clients can easily track a particular expansion  --&gt;  &lt;id value=&quot;43770626-f685-4ba8-8d66-fb63e674c467&quot;/&gt;  &lt;!-- no need for meta, though it is allowed for security labels, profiles --&gt;  &lt;!-- other value set details --&gt;  &lt;expansion&gt;    &lt;!-- when expanded --&gt;    &lt;timestamp value=&quot;20141203T08:50:00+11:00&quot;/&gt;  &lt;contains&gt;    &lt;!-- expansion contents --&gt;  &lt;/contains&gt;  &lt;/expansion&gt;&lt;/ValueSet&gt;\n\n# 搜索acme.org中Systolic展示的编码POST /ValueSet/$expandContent-Type: application/fhir+json&#123;  &quot;resourceType&quot;: &quot;ValueSet&quot;,  &quot;compose&quot;: &#123;    &quot;include&quot;: [ &#123;      &quot;system&quot;: &quot;http://acme.org&quot;,      &quot;filter&quot;: [ &#123;        &quot;property&quot;: &quot;display&quot;,        &quot;op&quot;: &quot;=&quot;,        &quot;value&quot;: &quot;Systolic&quot;      &#125; ]    &#125; ]  &#125;&#125;\n$lookup 概念查找一个外部系统可以向术语服务器发出查询操作来获取关于特定的系统&#x2F;代码组合的一组信息。服务器会返回用于显示和处理的信息。例如，通过已知 valueset 23 其验证一个 codesystem 概念:\nGET [base]/ValueSet/23/$validate-code?system=http://loinc.org&amp;code=1963-8&amp;display=test\n\n通过指定一个 value set 来验证 CodeableConcept:\nPOST [base]/ValueSet/$validate-code[other headers]&#123;  &quot;ResourceType&quot; : &quot;Parameters&quot;,  &quot;parameter&quot; : [    &#123;    &quot;name&quot; : &quot;coding&quot;,    &quot;valueCodeableConcept&quot; : &#123;      &quot;coding&quot; : &#123;        &quot;system&quot; : &quot;http://loinc.org&quot;,          &quot;code&quot; : &quot;1963-8&quot;,      &quot;display&quot; : &quot;test&quot;      &#125;    &#125;  &#125;,  &#123;    &quot;name&quot; : &quot;valueSet&quot;,    &quot;resource&quot;: &#123;      &quot;resourceType&quot; : &quot;ValueSet&quot;,    [etc.]    &#125;  &#125;  ]&#125;## ResponseHTTP/1.1 200 OK[other headers]&#123;  &quot;resourceType&quot; : &quot;Parameters&quot;,  &quot;parameter&quot; : [    &#123;    &quot;name&quot; : &quot;result&quot;,    &quot;valueBoolean&quot; : false  &#125;,  &#123;    &quot;name&quot; : &quot;message&quot;,    &quot;valueString&quot; : &quot;The display \\&quot;test\\&quot; is incorrect&quot;  &#125;,  &#123;    &quot;name&quot; : &quot;display&quot;,    &quot;valueString&quot; : &quot;Bicarbonate [Moles/volume] in Serum&quot;  &#125;  ]&#125;\n\n$translate 概念转化客户端可以向服务器请求将一个概念从一个值集转换为另一个值集。通常，这用于在编码系统之间进行转换（例如从 LOINC 转换为 SNOMED CT，或从 HL7 V3 代码转换为 HL7 V2 代码）。例子：把 FHIR 复合状态映射成 v3\nGET [base]/ConceptMap/$translate?system=http://hl7.org/fhir/composition-status  &amp;code=preliminary&amp;source=http://hl7.org/fhir/ValueSet/composition-status  &amp;target=http://terminology.hl7.org/ValueSet/v3-ActStatusHTTP/1.1 200 OK[other headers]&#123;  &quot;resourceType&quot; : &quot;Parameters&quot;,  &quot;parameter&quot; : [    &#123;    &quot;name&quot; : &quot;result&quot;,    &quot;valueBoolean&quot; : true    &#125;,    &#123;      &quot;name&quot; : &quot;outcome&quot;,      &quot;valueCoding&quot; : &#123;        &quot;system&quot; : &quot;http://terminology.hl7.org/CodeSystem/v3-ActStatus&quot;,        &quot;code&quot; : &quot;active&quot;,      &#125;    &#125;  ]&#125;\n\n几个具体实现市面上有大部分术语库有自己的fhir terminology service实现，如：\n\nSNOMED实现的snowstorm, 支持SNOMED CT, LOINC, ICD-10, ICD-10-CM等等。当然对SNOMED-CT的支持更好。\nSnow-owl开源版本支持SNOMED-CT, 付费版本支持LOINC，ICD-10等等。\n\n推荐阅读\nFHIR Terminology Server\nUse codes in Resources\nSNOMED Expression\n\n","tags":["FHIR"]},{"title":"Use SDC to Create Qustionnaires","url":"/2023/09/30/Use-SDC-to-Create-Qustionnaires/","content":"使用SDC(Structure Data Capture)创建问卷\n本文将正对HL7Structure Data Capture的实施指引IG做一个介绍，通过本文可以基本了解如何设计一个流程化、符合FHIR标准的文件表单。\n\n问卷在医疗环境现状随着互联网医院的普及，越来越多的大型医院已经逐步将一些问卷调查如: 满意度调查，用药依从性调查等问卷都提上了手机端，患者可以方便地使用微信等生态录入反馈信息。但是还有一些场景的问卷，医院还是会使用纸质问卷，原因是主要为了：\n\n归档\n临床流程强制性环节，如术前主观知情同意书，或主观意愿征询等。目前的真实世界实践主要存在如下几个问题:\n没有一个标准化，可配置的问卷表单实现方式，即便一些系统声称可以实现灵活可配置，但是这些表单系统涉及的问题和回答\n\n"},{"title":"非财务经理的财务课","url":"/2023/12/13/%E9%9D%9E%E8%B4%A2%E5%8A%A1%E7%BB%8F%E7%90%86%E7%9A%84%E8%B4%A2%E5%8A%A1%E8%AF%BE/","content":"回顾财务知识对于很多非财务背景的同学看似很陌生，其实必要的财务知识对于我们日常的生活和工作都有着息息相关的帮助。本人曾参加多次参加线上和线下财务和金融知识方面的培训，可惜疏于总结和应用，直到最近开始规划产品的商业化落地才又捡起来。本文将总结自己曾经学习到的知识做个总结，适用于非财务背景的同学食用。\n基础财务术语\nB&#x2F;S &#x3D; Balance Sheet 资产负债表\nP&amp;L &#x3D; Profit and Loss Statement 损益表\nCash Flow (operating cash flow, free cash flow &#x3D; FCF) 现金流\nSales Revenue (Net sales) 销售收入 （净销售）\nCoS&#x2F;CoGS &#x3D; Cost of Sales&#x2F;Cost of Goods Sales 销售成本\nGP&#x2F;GM &#x3D; Gross Profit&#x2F;Gross Margin 毛利\nGP% &#x3D; Gross Profit &#x2F; Sales Revenue 毛利率\nOpEx &#x3D; Operating Expenses 运营费用\nOpEx% &#x3D; OpEx&#x2F;Sales Revenue 运营费用率\nEbit &#x3D; Earnings before Interest and Taxes 息税前利润\nMC &#x3D; Market Contribution 市场贡献(实体收入&#x2F;整个市场收入​) \nW&#x2F;C &#x3D; Working Capital 营运资本\nW&#x2F;C % of LTM Revenue &#x3D; Working Capital &#x2F; Last 12 month Revenue 营运资本占过去12个月收入百分比\nAR &#x3D; Accounts Receivables 应收账款\nInv &#x3D; Inventory 存货\n\n掌握三大表，韭菜当地好资产负债表 Balance Sheet**含义：**总括地反映会计主体在特定日期（年末、季末、月末）财务状况的报表。**基本结构：**资产 &#x3D; 负债 + 所有者权益。 无论公司处于什么状态这个会计平衡公式永远是恒等的。**提供的信息：**资本来源和资本状况，包括公司掌握的经济资源，公司所负担的债务，公司的偿债能力，公司所有者所享有的权益，公司经营模式特点。\n资产的组成包括：\n\n非流动资产\n无形资产\n有形资产\n资产投资\n权益法下投资的关系公司股权\n其它金融资产\n\n\n流动资产\n库存\n应收账款\n其它流动资产\n证券\n现金或现金等价物\n\n\n待摊费用\n待售非流动资产\n\n负债及所有者权益包括：\n\n所有者权益\n股本\n资本公积\n盈余公积\n未分配利润&#x2F;亏损\n累积其他综合收益\n少数股东收益\n\n\n非流动负债\n预提养老金\n其他非流动性预提\n非流动性银行借款\n其他非流动性负债\n递延所得税负债\n\n\n流动负债\n流动性预提费用\n流动性银行借款\n应付账款\n当期所得税负债\n其他流动性负债\n\n\n递延收益\n带出售非流动资产相关负债\n\n损益表&#x2F;利润表 Profit and Loss statement &#x2F; Income statement**含义：**反映公司在过去一段时间销售收入、成本和盈利关系的报表。P&amp;L体现在权责发生制度下，收入和支出被记录在它们发生的时候，而不是在现金实际收到或支付的时候。这意味着，即使现金尚未交换，但在业务活动发生的时候，相应的收入或支出就会被记录。**基本结构：**利润 &#x3D; 收入 - 支出**提供的信息：**反映公司获利情况，反映公司利润的构成，反映税金缴纳情况，预测公司未来发展的情况。**企业的净利润来自三方面：**1. 营业利润 2. 投资收益 3. 营业外收支净额销售收入的确认时间：\n\n销售商品采用托收承付方式，在办妥托收手续时确认收入。\n销售商品采用预收款方式的，在发出商品时确认收入，预收款应确认为负债。\n销售商品需要安装和检验的，在购买方接受商品以及安装和检验完毕前，不确认收入，待安装和检验完毕后确认收入。如果安装比较简单，可以在发出商品时确认收入。\n销售商品采用以旧换新的，销售的商品应当按照销售商品收入确认条件确认收入。回收的商品作为购进商品处理。\n\n净利润公式展开\nSales revenues 销售收入- Cost of goods sold(COGS) 销售成本= Gross profit  毛利+ Other operating income  其他运营收入- Sales and marketing (S&amp;M) expense  销售和市场费用- General administration (G&amp;A) expense 管理费用- Research and Development (R&amp;D) expense  研发费用- Other operating expense   其他运营费用= Earning Before Interest and Taxes(Ebit) 息税前利润+/- Financial result  财务费用= Earning Before Taxes (Ebt) 税前利润- income taxes  所得税= Net Income  净利润\n\n现金流量表 Cash Flow Statement**含义：**提供企业在某一特定期间内有关现金及现金等价物的流入和流出信息。FCF体现在现金发生制下，现金发生制度只在现金实际发生交易时记录收入和支出，而不考虑交易发生的时间。**基本结构：**现金流量 &#x3D; 现金流入 - 现金流出**提供的信息：**反映净收益与现金余额的关系，预测未来现金流量，评价企业取得和运用现金的能力，确定企业支付利息、股利和到期债务的能力。影响因素：\n\n应收账款 –\n存货 –\n固定资产投资 –\n应付账款 ++\n预收账款 ++\n\n现金流中的流入和流出与损益表的收入和支出不完全一致，某些项并不会对现金流产生影响：\n\n赊销产生的收入\n固定资产折旧\n存货或应收账款冲销\n费用的预提\n\n自由现金流 Free Cash Flow定义：自由现金流指扣除除新增资本投资后公司核心经营活动产生的现金流，是一项理论上更准确的业绩和公司财务状况衡量指标，反映公司在会计年度或季度内支付所有现金成本和费用后剩余的现金。计算自由现金流时会将运营资本的变化考虑进去。因此，自由现金流能够体现企业产生或消耗现金的能力程度。\nEbiT+/- change in trade receivables+/- change in inventories+/- change in prepayments received+/- change in payments in advance+/- change in other provisions- investments+ Deprecation= Free Cash Flow\n\n利润中心和成本中心利润中心 Profit Center指一个责任中心，如果能同时控制生产和销售，既要对成本负责又要对收入负责，但没有责任或权力决定该中心资产投资的水平，因而可以根据其利润的多少来评价该中心的业绩， 因此称为利润中心。\n\n处于内控目的而设定的反应管理架构的会计组织单位，是用于计算利润的一个组织单元\n可判断公司内各职责单元的损益情况\n事业部Business Unit内的最小组织单元\n\n利润中心生成报告的目的是：\n\n衡量组织内各单元的业务发展情况指标。\n报告采用全球统一的标准，保证全球各事业部的业务结果可统一衡量和合并。\n可比较衡量集团内或各事业部内各组织单元的利润情况\n\n成本中心 Cost Center成本中心对费用进行独立预算，纪律和归集组织单位。成本中心一般包括生产，行政，管理层，财务，服务，销售，IT和HR等。也可以按区域划分，比如销售区域。成本中心报告的目的：\n\n增加公司成本结构的透明度，以此明确各自的责任\n增强对于成本费用的管理，提供利润\n为组织内的各部分细分团队提供成本信息\n反映公司区域组织架构\n确保将成本正确地分担至各个利润中心\n\n上市公司报表例子资产&#x3D;负债+权益：\n负债及所有者权益表：\n损益表\n业财融合，发现问题和机会财务数据可以帮助高管做决策，而在决策的过程中，我们不能以看财报的眼光看待财务数据，而要从内部决策的角度看待数据，财务数据是可以支持内部决策的。财务并不是在经营结束后进行总结的滞后性数据，财务往往可以提前涉及，是基于未来的。例如，在编制预算中，我们就可以有针对性的对报表架构进行设计，如用多少倍的杠杆，是否轻资产架构等等。在做决策的过程中，我们往往不用账本上的数据做决策，因为记在账簿的数据是已经发生的沉没成本，沉没成本不能用于做决策，但可以参考这些数据，机会成本，机会业绩，这些可能在未来发生的数据才是用来支持决策。\n"},{"title":"RAG","url":"/2023/11/25/RAG/","content":"什么是检索增强生成RAG?\n本文将为您简要介绍RAG（Retrieval-Augmented Generation）的发展历程。通过仔细阅读以下内容，您将初步了解RAG，并通过实际应用案例找到适合您的应用场景。\n\n定义与用处检索增强生成（RAG）是一种人工智能框架，通过基于外部知识源对模型进行信息补充，以增强大型语言模型内部对信息的表示，从而提高模型生成的响应质量。\nRAG是用于从外部知识库中检索事实，在大型语言模型（LLMs）上建立基础的人工智能框架，旨在提供最准确、最新的信息，并使用户深入了解LLMs的生成过程。大型语言模型可能存在不一致性，有时它们能够准确回答问题，而其他时候则可能随机地重复训练数据中的事实。如果它们偶尔听起来好像不知道自己在说什么，那是因为它们确实不知道。大型语言模型知道词语在统计上的关联，但不知道它们的含义。\n好处在基于大型语言模型的问答系统中实施RAG有两个主要好处：它确保模型能够访问最新、可靠的事实，用户可以访问模型的信息源，以确保其响应可以进行准确性检查，最终保证可信度。\nRAG还具有额外的好处，那就是敏感信息不用计入预训练。通过基于一组外部可验证的事实来基础LLM，模型减少了将信息嵌入其参数的机会。这降低了LLM泄漏敏感数据或产生不正确或误导性信息的可能性。\nRAG还减少了用户不断对模型进行新数据训练和参数更新的需求。通过这种方式，RAG可以降低在企业环境中运行以LLM为动力的聊天机器人的计算和财务成本。\n大模型微调的一种方式支撑所有基础大模型的底层架构是一种被称为transformer的人工智能架构。它将大量原始数据转化为其基本结构的压缩表示。从这个基础架构表示开始，基础模型可以通过对标记的领域特定知识进行一些额外的微调来适应各种任务。\n但仅仅通过微调很少能够使模型获得在不断变化的背景下回答高度特定问题所需的全部知识广度。在一篇2020年的论文中，Meta（当时称为Facebook）提出了一个名为检索增强生成的框架，以使LLMs能够访问其训练数据之外的信息。RAG允许LLMs基于专业知识体系以更准确的方式回答问题。\n正如其名所示，RAG有两个阶段：检索和内容生成。在检索阶段，算法搜索并检索与用户提示或问题相关的信息片段。在开放领域的消费者设置中，这些事实可以来自互联网上索引的文档；在封闭领域的企业设置中，通常使用更窄范围的来源以增加安全性和可靠性。\n这些外部知识的集合被追加到用户的提示中，并传递给语言模型。在生成阶段，LLM从增强提示和其对训练数据的内部表示中获取信息，以合成一个针对用户的引人入胜的答案。然后，可以将答案传递给带有其来源链接的聊天机器人。\nRAG应用场景个性化和可验证的响应。如今，由LLM驱动的聊天机器人可以在无需人类编写新脚本的情况下为客户提供更个性化的答案。而RAG允许LLMs更进一步，极大地减少了对模型进行新样本喂养和重新训练的需求。只需上传最新的文档或数据，模型就以开卷考试的方式检索信息以回答问题。并且回答的问题将附带了其信息来源的链接以确认信息的正确性。\n教导模型识别自己不知道的情况。人类的查询并非总是如此直截了当。它们可能措辞模糊、复杂，或需要模型没有或不能轻松解析的知识。在这些条件下，LLMs容易编造事实。通过足够的微调，LLM可以被训练停下来并表示自己不知道如何解答而不是随意回答。但训练它可能需要看到数千个可以回答和无法回答的问题的示例。只有在那之后，模型才能学会识别无法回答的问题，并探索更多细节，直到找到它具备回答的信息的问题。\n小结RAG目前是将LLMs基础在最新的、可验证的信息上的最为知名的工具，同时降低了不断重新训练和更新它们的成本。RAG依赖于丰富提示信息的能力，其中包含在向量中的相关信息，而这些向量是数据的数学表示。向量数据库可以高效地索引、存储和检索诸如推荐引擎和聊天机器人等信息。但RAG并不完美，让RAG正确完成仍然存在许多有趣的挑战。\nAzure RAG演示推荐阅读\nRAG论文\n微软gpt-rag\n\n"},{"title":"What is vector database?","url":"/2023/12/17/vector-database/","content":"什么是向量数据库 Vector Database?\n本文将为您简要介绍向量数据库的定义，基础原理，应用和市面上已有的实现选型比较。通过阅读本文，读者将对向量数据库有基本的认识，对日后开发大模型 AI 相关应用有所帮助。\n\n定义向量数据库也叫矢量数据库，是一种以数学向量的形式存储数据集合的数据库。更通俗的说法，向量就是一个数字列表，例如：[12, 13, 19, 8, 9]。这些数字表示维度空间中的一个位置，代表在这个维度上的特征。就像行和列号表示电子表格中特定单元格一样（例如，“A10”表示 A 列 10 行）。向量数据库的应用是使机器学习模型更容易记住先前的输入，从而使机器学习能够用于支持搜索、推荐和内容生成等应用场景。向量数据可以基于相似性搜索进行识别，而不是精确匹配，使计算模型能够在上下文中理解数据。\n什么是向量？向量是一组有序的数值，表示在多维空间中的位置或方向。向量通常用一个列或行的数字集合来表示，这些数字按顺序排列。在机器学习中，向量可以表示诸如单词、图像、视频和音频之类的复杂对象，由机器学习（ML）模型生成。高维度的向量数据对于机器学习、自然语言处理（NLP）和其他人工智能任务至关重要。一些向量数据的例子包括：\n\n文本：想象一下你上次与聊天机器人互动的情景。它们是如何理解自然语言的呢？它们依赖于可以表示单词、段落和整个文档的向量，这些向量是通过机器学习算法转换而来的。\n图像：图像的像素可以用数字数据描述，并组合成构成该图像的高维向量。\n语音&#x2F;音频：与图像类似，声波也可以分解为数字数据，并表示为向量，从而实现声音识别等人工智能应用。\n\n向量数据库的兴起向量数据库的兴起主要源于大模型 embedding 的应用。Transformer作为当今大模型的基础架构，在数据输入时需要对输入做 embedding, 由于当时主要是处理文本，所以这个 embedding 要做的就是词嵌入（word embedding），把文本转化为向量。由于大模型使用海量数据，数据的维度一般大于 1000 以上，所以临时或永久存储和计算（检索）这些高维向量数据就成了一个难题，这也是向量数据库崛起的一个主要原因。另一个方面，将人的输入以向量的方式临时存储起来，也可以提高类 chatgpt 应用的性能，让人觉得 ai 是有记忆的，可以检索历史问过的相关问题。想要自己试一试 embedding 的应用吗？可以试试 openai 的embedding api, 通过这个 api, 你的一个文本输入将会转化为多维向量。\n向量数据库的具体实现关于向量数据库引擎盖下的核心实现和算法，比如如何做两个向量间的相关性搜索，各个算法实现的优劣，请移步这几个链接, 作者以视频的方式做了介绍，更加直观：\n\n向量数据库技术鉴赏\n向量数据库的崛起\n\n使用向量数据库的好处机器学习模型无法记住超出其训练范围的任何信息，它们必须在每次查询时重新获取上下文（这就是许多简单聊天机器人的工作方式）。每次将查询的上下文传递给模型都非常缓慢，因为可能涉及大量数据；而且成本高昂，因为数据必须移动，计算能力必须反复用于让模型解析相同的数据。而且在实践中，大多数机器学习 API 可能受到它们一次能够接受多少数据的限制(所谓的 token 限制）。这就是向量数据库发挥作用的地方：数据集只需通过模型一次（或在数据变化时定期进行），模型对该数据的嵌入被存储在向量数据库中。总结来说，使用向量数据库主要有如下好处:\n\n速度与性能：向量数据库使用各种索引技术实现更快的搜索。向量索引与诸如最近邻搜索等距离计算算法特别有助于在数百万甚至数十亿数据点中搜索相关结果，实现了优化的性能。\n可扩展性：向量数据库通过水平扩展存储和管理大量非结构化数据，可以在查询需求和数据量增加的情况下保持性能。\n拥有成本 TOC：向量数据库是训练基础模型的有价值的替代选择，无论是从头开始还是进行微调。这降低了基础模型推理的成本和速度。\n灵活性：无论是处理图像、视频还是其他多维数据，向量数据库都设计用于处理复杂性。由于具有从语义搜索到对话式人工智能应用等多种用例，向量数据库的使用可以定制以满足您的业务和人工智能要求。\nLLM 的长期记忆：组织可以从通用型模型（如 IBM watsonx.ai 的 Granite 系列模型、Meta 的 Llama-2 或 Google 的 Flan 模型）入手，然后将自己的数据提供给向量数据库，以增强与检索增强生成相关的模型和人工智能应用的输出。\n数据管理组件：向量数据库通常还提供内置功能，以便轻松更新和插入新的非结构化数据。\n\n向量数据库的选型我们对市面上有的向量数据库从扩展性，查询速度，搜索准确性，灵活性和可及性做一个表格，做相应的比较。\n\n\n\nTool\nScalability\nQuery Speed\nSearch Accuracy\nFlexibility\nPersistence\nStorage Location\n\n\n\nChroma\nHigh\nHigh\nHigh\nHigh\nYes\nLocal&#x2F;Cloud\n\n\nDeepsetAI\nHigh\nHigh\nHigh\nHigh\nYes\nLocal&#x2F;Cloud\n\n\nFaiss\nHigh\nHigh\nHigh\nMedium\nNo\nLocal&#x2F;Cloud\n\n\nMilvus\nHigh\nHigh\nHigh\nHigh\nYes\nLocal&#x2F;Cloud\n\n\npgvector\nMedium\nMedium\nHigh\nHigh\nYes\nLocal\n\n\nPinecone\nHigh\nHigh\nHigh\nHigh\nYes\nCloud\n\n\nSupabase\nHigh\nHigh\nHigh\nHigh\nYes\nCloud\n\n\nQdrant\nHigh\nHigh\nHigh\nHigh\nYes\nLocal&#x2F;Cloud\n\n\nVespa\nHigh\nHigh\nHigh\nHigh\nYes\nLocal&#x2F;Cloud\n\n\nWeaviate\nHigh\nHigh\nHigh\nHigh\nYes\nLocal&#x2F;Cloud\n\n\nDeepLake\nHigh\nHigh\nHigh\nHigh\nYes\nLocal&#x2F;Cloud\n\n\nLangChain VectorStore\nHigh\nHigh\nHigh\nHigh\nYes\nLocal&#x2F;Cloud\n\n\nAnnoy\nMedium\nMedium\nMedium\nMedium\nNo\nLocal&#x2F;Cloud\n\n\nElasticsearch\nHigh\nHigh\nHigh\nHigh\nYes\nLocal&#x2F;Cloud\n\n\nHnswlib\nHigh\nHigh\nHigh\nHigh\nNo\nLocal&#x2F;Cloud\n\n\nNMSLIB\nHigh\nHigh\nHigh\nHigh\nNo\nLocal&#x2F;Cloud\n\n\n主要推荐ChromaChroma 是由Chroma.ai开发的开源向量数据库。它专注于可伸缩性，能够高效地存储和查询大规模向量数据集。Chroma 采用分布式架构，具有横向可伸缩性，可以处理海量的向量数据。它利用 Apache Cassandra 实现高可用性和容错性，确保数据的持久性和耐久性。Chroma 的一个独特之处在于其灵活的索引系统。它支持多种索引策略，如近似最近邻（ANN）算法，例如 HNSW 和 IVFPQ，从而实现快速准确的相似性搜索。Chroma 还提供全面的 Python 和 RESTful API，便于与自然语言处理流水线轻松集成。凭借其对可伸缩性和速度的强调，Chroma 是对于需要高性能向量存储和检索的应用而言的绝佳选择。虽然它可能没有一些其他工具那么高的可伸缩性或先进的搜索算法，但对于小到中型项目或想要快速入门向量数据库的初学者来说，它是理想的选择。\nFaissFaiss 标志，由 Facebook AI Research 开发，是一种广泛使用的向量数据库，以其高性能的相似性搜索能力而闻名。它提供了一系列针对高效检索最近邻居的优化索引方法，包括 IVF 和 HNSW。Faiss 还支持 GPU 加速，可以在大规模嵌入式数据上进行快速计算。Faiss 的一个显著特点是其支持多索引搜索，这将不同的索引方法结合在一起，以提高搜索准确性和速度。此外，Faiss 提供了 Python 接口，便于与现有的自然语言处理流水线和框架集成。凭借其对搜索性能和多功能性的关注，Faiss 是在需要对大规模嵌入集合进行快速准确相似性搜索的项目中的首选选择。\nPostgreSQL + pgvectorPgvector 是用于 Postgres 的开源向量相似性搜索。Pgvector 有助于在流行的开源关系数据库 PostgreSQL 上构建向量数据库。它利用 PostgreSQL 扩展系统强大的索引功能，提供高效的向量嵌入存储和检索。Pgvector 支持 CPU 和 GPU 推理，实现高性能的向量操作。Pgvector 的一个关键优势是它与更广泛的 PostgreSQL 生态系统的无缝集成。用户可以利用 PostgreSQL 的丰富功能，如 ACID 兼容性和对复杂查询的支持，同时受益于向量特定的操作。Pgvector 扩展了 SQL 语法以处理向量操作，并提供了一个用于简化集成的 Python 库。凭借其与 PostgreSQL 的兼容性和高效的向量存储，pgvector 是需要无缝 SQL 集成的自然语言处理应用的可靠选择。\n小结向量数据库从算法和技术层面来说并不是一个完全全新的领域，但由于LLM兴起，我们对其有了更多的重视，当前已有的产品可以说是百家齐鸣，本文也没有完全罗列其它传统数据库厂商如SQL, Redis, Mongo等对向量数据库的支持。希望通过这次GAI的浪潮，乘风破浪者们能够在不同领域找到更多的机会。\n推荐阅读\nchroma\nFaiss\nIBM vector database\n\n","tags":["vector database"]},{"title":"Finetune vs RAG","url":"/2024/01/14/Finetune-vs-RAG/","content":"\n在生成式人工智能领域目前的争论之一是围绕微调、检索增强生成（RAG）和提示语（prompt）哪个是最佳方案，或者他们两两组合是不是合适的选择。在这篇文章中，我们将探讨RAG和Finetune两种技术，突显它们的优势、劣势以及可能的两者结合方案。通过阅读本文，您将清晰了解如何充分利用这些方法的潜力，推动您的人工智能取得成功。\n\n两者不是对立关系在深入比较之前，了解微调和检索增强生成并不是对立的技术是至关重要的。相反，它们可以结合使用，以充分发挥每种方法的优势。本文后面将举例说明如何让两者结合，提供企业级解决方案。如果不想看两者的简介，可以直接看下图比较\n\n微调大模型Finetune简介大多数当前的语言模型基于变种的Transformer，需要在大量文档的语料库上进行“预训练”，以学习单词之间的统计关系。语言模型学到的一切都存储在其基础人工神经网络的权重中。\n微调就是在以后的某个日期对已经训练过的语言模型进行额外数据的训练，以便重新调整其神经网络的权重以适应新的输入。微调和预训练的关键区别在于，您对模型进行微调的数据通常比LM在初始训练期间摄取的一般数据更为具体，符合实际场景使用。\n微调的优势微调允许语言模型专注于特定的领域或任务。例如，彭博（Bloomberg）在金融数据上训练了一个模型（BloombergGPT）；OpenAI对ChatGPT进行了问题-答案对的微调；ChatDoctor是一款使用LLaMA模型并结合医学知识进行微调训练的医疗助手。\n在这里，可以突破您的想象力是限制，您可以在诗歌、表情符号、您自己的日记、替代文本、小语种或任何您能够想象到的基于语言的数据上进行微调。视觉、音频和多模态模型都适用于微调。\n除了让语言模型专注于特定领域外，微调还享有附加优势，即允许使用更小的模型以及更便宜的训练和推理成本。例如，您通常可以使用大型预训练的语言模型，在其最初训练的数据的一小部分上进行微调，仍然能够在特定领域的应用中取得出色的性能。\n微调的劣势微调语言模型虽然听起来像是一种灵丹妙药，但并非没有缺点。首先，尽管微调比从头开始训练语言模型更快，但仍然需要时间。其中之一是收集和准备用于微调的新数据。也许像谷歌这样的组织可以几乎每天都对LLM进行微调，但人类处理的许多信息甚至比日常数据更具临时性（例如，天气和地缘政治可能在一瞬间发生变化，当它们发生变化时，有些人需要更早地了解情况，而不是等待进行大模型微调所需的时间）。\n除了不能充分解决大模型的时效性问题外，反复的微调可能会引发灾难性的遗忘。由于人工神经网络的权重存储了它们在训练期间学到的知识，而这些权重在微调期间稍后会更新，如果你对模型进行足够多次的微调，它可能会直接“忘记”一些先前的知识。当然，这会留下类似幻觉的问题。微调的大模型也可能“记忆”数据，这可能看起来不像是问题（肯定比忘记好），但是记忆可能引发隐私问题（如果训练数据清理得不够充分），而当LMs记忆数据时，它们对与其训练语料库差异显著的文本的泛化能力不强。\nRAG（Retrieval Augmented Generation）介绍现在我们知道微调可以使大模型更加专注于更专业的领域，但我们不能仅仅为了使它们的知识及时而不断进行微调。那么，我们还能以什么其他方式解决大模型的时效性和幻觉问题呢？\n如果大模型能够访问一系列外部文档，而不仅仅依赖于它们的内部知识，会怎么样呢？这基本上就是检索增强生成（RAG）所实现的概念，这个概念是在2020年Facebook的一篇论文中正式介绍的。随着LLM能力和受欢迎程度的不断提高，人们对它们的缺陷有了越来越多的认识，这促使对最初的RAG概念进行了实验。虽然不同的RAG变体提供了许多细节，但我们将专注于RAG的基本过程。\nRAG的本质RAG的本质是让模型获取正确的上下文，利用ICL (In Context Learning)的能力，生产足够多上下文的prompt输入以确保输出正确的响应。它综合利用了固化在模型权重中的参数化知识和存在外部存储中的非参数化知识(知识库、数据库等)。\n首先，RAG类似于从图书馆（文档集合或数据库）借阅几本书，浏览它们的目录和索引，以判断它们是否可能包含你感兴趣的信息（检索），如果是的话，阅读相关的段落以帮助了解你想学习的内容（生成）。参数记忆是存储在人工神经网络内部的内在知识（即，大模型在不查阅外部来源的情况下已经知道的内容），而外部的非参数化知识存储在大模型权重之外（例如，在数据库中），但仍然可以被大模型访问。如果将原始的大模型检索类比为闭卷考试，而RAG则类似于开卷考试。\nRAG如同名称所示，包含两个关键步骤：\n\n检索\n使用检索到的文档生成首先，为了使RAG良好运作，大模型需要知道它们不知道的事。这对于RAG的检索步骤至关重要。例如，如果一个大模型过于依赖其内部的参数化知识，它很少会查阅任何内容，使得该大模型容易产生幻觉或像谄媚者一样回答。虽然我们希望LM通过RAG利用外部知识，但如果大模型经常查阅外部的非参数化知识，它们就会浪费内部的参数化知识，从而在不断的信息检索中陷入困境。想象一下，为了塑造你的每一个想法，你都要查阅一本书、文章或网站——没有人会这样做。相反，我们在使用我们内部深深根植的任何知识和我们能够找到的任何相关外部知识之间，取得了一种不完美但有用的平衡。理想情况下，RAG也应该做到这一点。检索通常始于将文档、网站、数据库或其他数据嵌入到向量表示中，与大模型词嵌入（embedding）的方式有些相似，以便机器能够使用它们。对于安全性不太关心的应用，扩充数据可以是互联网（例如，Bing的生成搜索）。然而，注重安全性和准确性至关重要的应用（例如，企业应用）会将扩充数据限制在一个较小、安全且经过审查的池中。接下来，用户的问题也被嵌入。然后，将用户嵌入的问题与嵌入的文档（或段落、句子等）进行比较。一些足够相似的外部知识然后通过一些提示词工程追加到用户的查询中。有了这个额外的上下文，大模型通常能够生成比没有外部信息更准确的响应。RAG方法通常将外部数据存储在矢量数据库中，因为矢量数据库有助于快速找到与查询相关的信息。我们甚至可以设置流程来持续改进RAG。例如，可以将RAG生成的答案存储在与其他外部文档相同的矢量数据库中。然后，这些答案就像缓存的答案一样，因为当大模型在后来接收到足够相似的查询时，它可以检索其先前的答案（现在是一个嵌入的文档）并将其用作生成新答案的上下文。\n\nRAG 工作流\nRAG的优势RAG具有许多语言模型的优势。首先，它在一定程度上解决了大模型的时效性问题，因为数据可以轻松而快速地添加到大模型可以访问的任何数据库中；这显然比定期对LM进行微调更快速、更经济。\n其次，RAG显著减少了幻觉。即使使用RAG的大模型出现错误，RAG也允许大模型引用其信息来源，从而让我们能够识别、纠正或删除大模型检索到的不可靠信息。此外，由于RAG允许LM将其“知识”的大部分存储在外部来源中，您可以减少一些伴随着将敏感数据存储在大模型权重内的安全泄漏问题。此外，能够访问大型数据集的能力在一定程度上解决了大模型的上下文窗口限制问题，而且总体而言，RAG比持续微调更经济，当然这还要取决于需要为RAG嵌入多少文档以及某些微调语料库有多大。\nRAG的劣势RAG的劣势主要来自具体检索的实现上。这在实际工程实践上很有挑战。正如我们之前提到的，研究人员仍在努力找出如何让大模型可靠地知道何时查阅外部的非参数化知识以及何时依赖于其内部的参数化知识。查找字面所有信息的速度太慢且降低了大模型的效用，因此我们不希望RAG在检索部分过于宽泛。我们也不希望大模型总是凭直觉行事。找到一个平衡点可能是RAG最大的挑战。还有更多的考虑。如果一个大模型决定查阅外部来源，它应该如何做呢？大模型应该考虑每个文档吗？还是应该将每个文档分成页面、段落或句子，以便大模型可以查询更精细的信息？同样，我们需要一个平衡点。这个决定影响如何对大模型的外部知识库进行矢量化。同样，模型应该查阅哪些来源？整个互联网，还是其中的一些子集？手工制作的一组本地存储的文档？而RAG应该检索多少文档呢？所有相关文档吗？通常，RAG检索前n个文档（其中n是某个自然数），这似乎是合理的，但如果回答查询所需的知识要点位于第n+1个文档中呢？当前的RAG方法通常使用近似最近邻方法（因为纯最近邻可能导致查阅太多文档），但近似最近邻不能保证为某个查询返回一个有用的文档，因此即使使用了ANN减轻了工作量，幻觉问题仍然存在。最后，与仅查询大模型的内部知识相比，RAG可能更昂贵，因为RAG需要嵌入和存储大模型可以查阅的外部数据，通常会产生矢量数据库存储成本（但RAG仍然比例常规微调更便宜），并且RAG可能增加推理成本，因为它会将检索到的文档附加到查询中。当然，所有这些决策和考虑都依赖于具体的应用场景，但你可以看到实施RAG可能会变得多么复杂。\nFT和RAG结合RAG和微调为使大模型具有实时性并减少幻觉等问题行为提供了互补的优势和劣势。微调有助于使大模型专注于特定领域、词汇或新颖数据，但需要时间和计算资源，存在灾难性遗忘的风险，并且无法解决互联网快速演变的特性。RAG利用外部知识以增加事实和时效性，但仍然面临有关最佳检索的挑战。两种方法都不能完全解决LM的所有缺陷，但都有助于解决一些即时的问题。随着LLMs不断渗透到我们的数字体验中，我们可以预期进一步尝试将微调、RAG和其他信息检索技术结合起来，为LLMs提供更及时、真实的知识。目前meta的技术人员就开发了一个模型RA-DIT, 知识密集型的零和少量次学习基准范围内实现了最先进的性能。\nRAG和其它优化方式比较\n可能的企业级设计如下图所示, 一个企业级的应用可以结合微调模型和RAG技术，整合内部其它业务系统和数据将内部知识安全地与外部知识融为一体，从而减轻企业的成本，提高运营的效率。比如使用下面的架构重新整合企业内部HR相关服务，提高HR Share Serivce Center的能力和效率。\n又如，想象一个客户支持聊天助手。当代理需要在实时互动中获得指导时，他们会提示助手，触发以下流程：\n\n流水线检索关键客户信息，包括客户ID。\n流水线查询客户的历史记录、问题、政策、特殊情况（例如故障）、当前支持团队的可用性以及其他动态外部变量。\n流水线将收集到的上下文和原始查询成为最终提示。\n模型生成一个基于提供的上下文的响应。尽管此流水线可以使用“现成的”模型，但生成的响应的风格可能会有所不同，并偏离内部政策或最终用户的要求。为了提高一致性和帮助性响应的可能性，数据团队可以训练一个“专业化”的助手模型。数据团队将从适当的基础模型开始，例如Llama 2或GPT-4。然后，他们将构建或策划一个例子语料库，其中包含模型将经常遇到的每种特定任务类型的可能输入（上下文+提示）和理想输出（包括期望的格式）。这可能包括针对客户服务代理自己处理的任务（例如重新激活帐户）以及涉及引导用户执行他们自己操作的任务（例如故障排除）的不同方法。在这种情况下，微调和检索增强共同合作，以更快地提供正确的解决方案。\n\n推荐阅读：\nRetrieval-Augmented Generation for Large Language Models: A Survey\nRetrieval-Augmented Dual Instruction Tuning\nRAG vs. Finetuning: Enhancing LLMs with new knowledge | Deepgram\n\n","tags":["AI, RAG, Finetune"]},{"title":"机器学习一撇","url":"/2018/10/13/AI-2018-10-04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80%E6%92%87/","content":"机器学习是建立在大量数据的基础上， 在有意义的大数据上面提取有用的信息， 然后做出预测。\n典型应用场景\n推荐算法\n欺诈检测\n市场细分研究\n社交网络分析\n医学研究\n\n机器学习的三个分支\n监督学习监督学习主要用于基于一组训练集做出预测结果， 如训练集和结果用数组表示[x1, x2,...y], 算法将从训练集中学习如何预测将来的某个结果。如果y是数字那么这个预测叫做回归, 如果是明确的一些值， 那么这个预测叫做分类.\n\n非监督学习非监督学习主要用于发现数据中未知的模式， 典型的例子是分类集群。\n\n加强学习也叫连续学习, 在这种模式下， 我们不会得到最终结果。而是基于当前状态得出一个最优解， 也就是所谓的agent选择一个最优action, 然后将得到一个award.\n\n\n数据仓库数据仓库是存放数据的各种源集合叫法。它有什么用呢？ 一个例子是把OLTP如mysql的数据做备份然后导入数据仓库做OLAP. 这时数据仓库中的数据多了一个时间维度， 用于区分版本。所以从不同的数据源导入数据仓可能需要做ETL, 也就是抽取-转化-导入。 这里的导入一般会把数据按不同维度导入到data cube.\n数据仓一般会把数据组织成多个维度的数据立方， 基于所谓的star schema(一张fact表， 用于索引多张不同维度的其它表)，表中的数据是不同维度数据的聚合。\n在数据仓做OLAP操作时， 一般涉及如下步骤：\n\nrollup: 对某个维度的数据做聚合。\ndrilldown: 在某个维度把数据打散， 使其颗粒度更细。如把按月展示, 打成按天展示.\nslice: 在一个维度切一层数据看， 比如查看所有二月的数据。\nDice: 从一个cube中选择一个子集。\n\n数据仓可以被进一步切分为data marts, 用于不同角度的drilldown分析。\n机器学习的哲学正常想法，想要把一组输入数据经过处理得到结果， 一般可以研究输入的属性，然后写一个函数计算出结果。 但是有时候它们之间的关系往往难以确定。 这个时候机器学习就派上用处了，它可以基于历史所有的数据做出预测。 机器学习大致可以分两种学习策略：\n懒惰学习lazy learning懒惰学习基于实例， 学习者记住所有之前看过的例子， 当一个新的数据来到， 它尝试从历史见过的数据中找到相似数据，然后使用历史数据的结果作为参考去预测当前输入数据的结果。这种学习的前提是上述两者在属性上近似，所以他们的输出也近似。 Nearest neighbor最近邻居法就是经典的懒惰学习例子。\n主动学习eager learning主动学习是基于模型的学习，学习者假设输入属性和输出相关， 并且基于某个模型（比如线性回归， 逻辑回归， 神经网络等等）。 所以学习者学习基于历史数据得出的模型（主要是模型参数）， 当一个新的数据来临， 它就会使用这个模型去预测结果。主动学习学习通用模型， 懒惰学习从看过的例子中学习。他们还有一点不同， 主动学习的模型必须满足所有历史数据， 但是懒惰学习可以只关注最近某些时间的点。懒惰模型也更难以构建一个综合的模型，需要大量的内存存储所有见过的数据。\n","categories":["machine learning"],"tags":["ML"]},{"title":"Deep Learning Book Notes--Chapter 5","url":"/2018/10/03/AI-2019-09-21-deeplearning-notes-chap5/","content":"SummaryStatistical learning theory tells us:如果测试集和训练集都是从一个叫data generating process的数据集中产生, 我们可以做一些假设, 测试集和训练集将相互独立并且以相同的概率分布. 有了这个假设我们才能对训练集和测试集误差做数学上研究.\n取决机器学习算法性能的因素是:\n尽可能的减少训练误差\n使测试误差尽可能接近训练误差\n\n欠拟合模型的训练误差不够小\n过拟合模型的测试误差和训练误差过大\n可以通过调节capacity(容量)来调节拟合\n调节容量的方法之一是: 选择一个hypothesis space(假设空间).这个假设空间是能够表达所有解决方案的函数集.换个角度讲, 就是改变输入集特征的数值, 同时可以增加这些特征的新参数. 比如把一个model从线性空间改成多项式空间, 那么容量将会变大.\n另一调节容量的方式是: 指定模型从某些类别的函数选择. 这叫Representational capacity\n\nVC维度可以来量化容量VC维度可以测量一个二元分类器的容量. 定义为: 存在一个最大值m, 分类器使m个样本可以被任意标记.\n统计学习理论中最重要的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量增长而增长，但随着训练样本增多而下降\n随着训练集增加, 容量也需要增加, 否则将出现欠拟合\nno free lauch theorem在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率.所以我们只需要对特定Task设计最好算法就行.\n正则化正则化是指我们修改学习算法，使其降低泛化误差而非训练误差.\n","categories":["AI"],"tags":["AI"]},{"title":"Classic Rough sets translation","url":"/2020/06/29/AI-2020-06-30-Rough-sets/","content":"Forword本文主要想翻译一下经典论文Pawlak Z.Roughset， 以期全面地了解这一技术，为后续应用打下基础。\nIntroduction本论文目的是描述粗糙集的一些属性，\nREFERENCES\nE. Konrad, E. Ortowska, and Z. Pawlak, An approximate concept learning (Berlin,Bericht, 1981), pp. 81-87.\nW. Marek and Z. Pawlak, “Rough sets and information systems,” ICS PAS Reports(441) (1981).\nR. Michalski, “S., Pattern Recognition as Role-Guided Inductive Interference,” 1EEETransaction on Pattern Analysis and Machine Intelligence 2:179-187 (1971 ).\nE. Ortowska, “Semantics of vague concepts, Application of rough sets,” ICS PASReports (469) (1982).\nE. Ortowska, “Logic of vague concepts, Application of rough sets,” 1CS PAS Reports(474) (1982).\nE. Orlowska and Z. Pawlak, “Measurement and observability, Application of rough sets,”(to appear).\nZ. Pawlak, “Rough sets,” ICS PAS Reports (431) (1981).\nZ. Pawlak, “Rough relations,” ICS PAS Reports (435) (1981).\nZ. Pawlak, “Rough functions,” ICS PAS Reports (167) (1981).\nZ. Pawlak, “Information systems, theoretical foundations,” Information systems 6(3):205-218 (1981).t 1. Z. Pawlak, “Rough sets, Algebraic and topological approach,” ICS PAS Reports (482)(1982).\nA. Robinson, Non-standard analysis (North-Holland Publishing Company, Amsterdam,1966).\nL. A. Zadah, “Fuzzy sets,” Information and Control 8:338-353 (1965).\nE. O. Zeeman, “The Topology of the Brain and Visual Perception,” in Topology of 3-Manifolds and related topics, M. K. Fort, ed. (Englewood Cliffs, N.Y., 1962).\n\n","categories":["AI"],"tags":["AI"]},{"title":"机器学习二撇","url":"/2022/10/30/AI-2022-10-31-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%8C%E6%92%87/","content":"机器学习适合解决的场景\n需要大量手工调整或需要长串规则才能解决的问题，机器学习往往可以简化代码、提高性能\n问题复杂，传统方法难以解决\n环境有波动，机器学习可以适应新数据\n洞察复杂问题和大量数据，找出潜在规律\n\n","categories":["machine learning"],"tags":["ML"]},{"title":"集成学习","url":"/2022/11/06/AI-2022-11-07-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/","content":"集成学习的本质集成学习的本质是集众之长得出一个更好的结果。专业术语来讲是集成多个单独的classifier(一般是弱学习者weak learner)来获得一个更优的classifier.集成学习最大的好处是可以解决高偏差和方差问题。采用多个分类器可以增加模型的复杂性来解决方差问题，而只要分类器足够不相干，统计规律告诉我们这样可以减少偏差。\n偏差和方差的识别一般模型的error = Bias**2 + Variance + inreducible error, 实际应用中我们一般使用学习曲线和验证曲线来评估他们。validation curve是不同超参数下，算法可以达到的性能。在每个超参数下，我们使用K-fold交叉验证，存储样本内的性能和样本外的性能，然后计算并画出样本内和样本外的性能的平均和标准差值。最后通过比较相对和觉得的性能，我们可以计算出偏差和方差的大小。\n集成学习的关键—足够多样的分类器基本上有两种方式可以得到足够不同的分类器，一种是使用不同的算法，另一种是使用相同的算法在随机子集。\nBagging","categories":["machine learning"],"tags":["ML, ensemble"]},{"title":"PowerBI Quickstart","url":"/2022/10/01/BI-2022-10-04-PowerBI-Quickstart/","content":"PowerBI Quickstart Guide截止2022年末市面上主流的BI产品有PowerBI, Tableau和国产帆软的FineBI. 由于Tableau逐渐退出中国，本文主要介绍一下PowerBI的基础功能。\n数据导入方式PowerBI支持从130+不同数据源直接导入数据，主要导入数据的方式分为:\n\nDirect import 直接导入\nDirectQuery 直接查询\nLive Connection 在线连接\n\n这三种数据连接方式主要区别于:\n\ndirect import 是将数据直接导进PowerBI desktop端的SSAS(SQL Server Analysis Services)的向量内存中，既然要存入内存那么对客户端的内存有一些要求，当然SSAS提供了一些内存存储压缩的算法来减少内存占用量。缺点是数据不是实时更新的。\nDirectQuery 顾名思义是直接在另一端的数据库上做查询, 这样性能的压力来到了数据库端。PowerBI目前支持主流的数据库，做查询前PowerBI desktop会将查询解释成对应的数据库语言到目标数据库执行，所以这里的语言转化或者叫做翻译是限制DirectQuery能力的瓶颈点。\nLive Connection只支持类SSAS的远程数据库，所有的能力和限制取决于远端SSAS的能力。\n\n如下表可以用来决策什么时候用什么方式的数据连接，有时候我们可能需要一个综合的解决方案。\n数据转化策略PowerBI使用Power Query Editor来处理数据清洗和转换，底层使用的是M语言。在真正运行处理逻辑时，PowerQuery有个Applied Steps功能可以用来调整处理逻辑的顺序或者反悔某些操作如删除。\nPower Query Editor在增加列的时候有一个自动根据所填例子形成新增列的功能，就是在新增一列时填入这列一行想转化成的数据，powerBI会自动完成剩余列的数据填充。\n条件列，向下填充，逆透视，合并查询和追加查询conditional column就是在某列上经过if条件过滤后的生成一个新的列，比如某些列为空则生成新的列。fill down向下填充就是把为空的值按最上面的非空值填充。split column把一个列按一定规则拆成两个列。unpivot逆透视是把原列头作为新的表的行值，而原来列的值将会被相应转成对应的行。merge queries类似SQL的join, 分为内联，左联，右联，外联等。\n数据建模PowerBI有强大的数据建模能力，可以自动识别表的关系，处理多对多管理和角色扮演维表role-play table.\nRole-play table角色扮演维表是一张维表（比如时间维度表）对事实表有多个作用，同时一张表就可以扮演多个角色从而减少数据冗余。举个例子，一张销售事实表可有多个时间维度，如订单时间，交付时间，维护时间等。PowerBI需要使用非激活的关系和DAX度量来处理Role-play, 具体需要用到USERELATIONSHIP函数。\n","categories":["Business Intelligence"],"tags":["BI,powerbi"]},{"title":"OCT Basic","url":"/2022/08/04/Devices-2022-08-05-OCT-basic/","content":"OCT技术原理OCT(光学相干断层扫描)的基本原理是把光束(近红外光800-1500nm)投射到被成像的组织或标本上，光束被不同距离上的显微组织反射，通过测量反射光的延迟时间以及反射光的强度，将不同的位置上测量相干光所获得的反射信息转化为数字信号，经过计算机处理得出二维或三维的图像。\n时域OCT第一台被发明的OCT就是时域OCT，通过调节移动参考镜，使参考光分别与从眼内不同结构反射回来的信号光产生干涉，通过分别记录相应的参考镜空间位置，便可测量出眼球内不同组织结构的距离\n频域OCT频域OCT则保持参考镜不动，所有从不同层面反射回来的光通过分光仪和光电探测器，使用傅里叶变换将频谱干涉图变成时域含有深度信息的图。\n相比时域OCT需要移动参考镜且一次只能获取一个信号，频域OCT成像的速度更快（40-100倍），分辨率更高，信噪比更高。下图是具体比较。\nAscan, Bscan和CubeAscan是Z轴或深度的光反射扫描，成像是一个时间与强度坐标的点。Bscan是沿着X轴或者Y轴进行Ascan，成像是一幅2D的平面横截图. Cscan或者叫Cube是将Bscan沿着另一个轴进行扫描，形成3D立体的Cube.\n","categories":["devices"],"tags":["devices, OCT"]},{"title":"眼球结构简介","url":"/2022/08/03/Devices-2022-08-05-eyeball-intro/","content":"眼球简介眼球筋膜鞘（Tenon囊） Fascial sheath (Tenon’s capsule)纤维层 Fibrous layer血管层 （葡萄膜）Vascular layer (Uvea)神经层 （视网膜）Nervous layer (Retina)眼球的屈光介质 Refractive media of the eyeball临床症状 Clinical conditions","categories":["science"],"tags":["science, eyeball"]},{"title":"EEA以太坊企业联盟安全规范第三版","url":"/2025/08/03/EEA%E4%BB%A5%E5%A4%AA%E5%9D%8A%E4%BC%81%E4%B8%9A%E8%81%94%E7%9B%9F%E5%AE%89%E5%85%A8%E8%A7%84%E8%8C%83%E7%AC%AC%E4%B8%89%E7%89%88/","content":"​​当前版本(2025&#x2F;3)​​:https://entethalliance.org/specs/ethtrust-sl/v3/\n​​本版本检查清单​​:https://entethalliance.org/specs/ethtrust-sl/v3/checklist.html\n​​最新编辑草案​​:https://entethalliance.github.io/eta-registry/security-levels-spec.html\n​​编辑​​:EEA 编辑\n​​最新发布版本URL​​:https://entethalliance.org/specs/ethtrust-sl/\n​​本版本贡献者​​:除所有为之前版本做出贡献的人员外，以下人员对本版本规范有具体贡献：Chaals Neville、Anish Agrawal (Olympix)、Kenan Bešić (ChainSecurity)、Daniel Burnett、Valerian Callens、Luke Ciattaglia (Hacken)、Christopher Cordi、Ignacio Freire (Olympix)、Aminadav Glickshtein (EY)、Opal Graham (CertiK)、Channi Greenwall (Olympix)、James Harsh、Sebastian Holler、George Kobakhidze (Diligence)、Michael Lewellen (OpenZeppelin)、Luis Lubeck (Hacken)、Dominik Muhs、Anton Permenev (ChainSecurity)、Juliano Rizzo (Coinspect)、Gernot Salzer、Clara Schneidewinde、Tobias Vogel (Diligence)、Morgan Weaver (OpenZeppelin)\n版权声明© 2020-2025 企业以太坊联盟 (Enterprise Ethereum Alliance)。\n摘要本文档定义了EEA EthTrust认证的要求，该认证是一组证明智能合约已通过审查且未包含特定安全漏洞的认证。\n文档状态本节描述了本文档发布时的状态。新文档可能会取代本文档。本文档是​​EEA EthTrust安全等级规范第3版​​，由EthTrust安全等级工作组开发，企业以太坊联盟公司发布。本草案内容已获EEA批准发布。工作组预计在发布时将继续工作，并在2026年下半年最终确定并发布新版本以取代本文档。本规范由企业以太坊联盟公司根据Apache许可证2.0版授权。除非EEA明确书面授权，否则您只能根据该许可证的条款使用本规范。除非适用法律要求或书面同意，本规范按”原样”分发，不提供任何明示或暗示的担保或条件。对本规范的反馈可直接发送至编辑EEA Editor，或作为问题提交至EthTrust-public GitHub仓库。\n​​GitHub Issues​​是讨论本规范的首选方式。\n1. 简介本节为非规范性内容。本文档定义了为用Solidity编写的智能合约授予EEA EthTrust认证的要求。EEA EthTrust认证是安全审查员的一项声明，表明根据审查员针对特定要求的评估，​​被测代码​​不易受到一系列已知攻击或预期操作失败的影响。\n没有任何安全审查可以保证智能合约免受所有可能的漏洞攻击，如第三章. 安全考虑中进一步解释。然而，根据本规范中的要求审查智能合约，可以提供保证，确保其不易受到一组已知的潜在攻击。\n这种保证不仅基于审查员的声誉，还基于来自许多竞争组织的众多安全专家在EEA内的协作，确保本规范定义了对真实且重要的一组已知漏洞的防护。\n1.1 如何阅读本规范本节介绍如何理解本规范，包括示例和要求的约定、核心概念、参考文献、说明性章节等。\n1.1.1 文档概述文档结构大致如下：​​前置内容​​：文档基本信息（作者、版权等）​​符合性章节​​：声明符合本规范的含义和形式​​安全考虑​​：与智能合约相关的关键安全概念介绍​​测试方法​​：与本规范相关的不同测试方法介绍​​EthTrust安全等级​​：文档核心部分。按等级和主题分组的安全审查要求​​附加信息​​：\n\n术语表\n要求和推荐实践的摘要\n致谢\n自上一版本以来的重大变更摘要\n已从早期版本中移除的需求列表\n\n​​参考文献​​：扩展阅读，包括必要的规范性引用和说明性引用\n本规范附有​​检查清单​​，以表格形式列出所有要求。熟悉本规范的开发者或审查员可使用该清单快速回顾每个单独要求并跟踪测试状态。如有任何差异，以本规范文档中的规范性文本为准。\n1.1.2 排版约定​​要求​​的结构和格式详见§1.1.3。部分位置提供​​示例​​。这些示例不是要求，也不具有规范性。示例通过带边框的背景和标题进行区分，如下所示：\n\n​​示例1：示例的示例​​\n示例中可以包含行内代码如code()，也可以包含代码块：\n// SPDX-License-Identifier: MITpragma solidity 0.8.0;contract HelloWorld &#123;    string public greeting = &quot;Hello World&quot;;&#125;\n\n\n部分示例展示​​易受攻击的代码​​或​​不应采取的做法​​。将此类示例复制到生产代码中是非常危险的行为。这些示例会标记为警告：\n\n警告\n​示例2：问题示例 - 请勿复制​​部署未经安全审查的代码, 这是危险行为。\n\n\n​​术语定义​​的格式为​​加粗​ ​，后续对已定义术语的引用会以链接形式呈现​。对其他文档的引用以方括号内链接形式给出，如[CWE]。对要求的引用以安全等级开头：[S]、[M]或[Q]，对推荐实践的引用以[GP]开头。它们会包含要求或实践名称，并以加粗链接形式呈现：\n\n​​示例3​​\n示例链接：[M][记录特殊代码用途](https://entethalliance.org/specs/ethtrust-sl/v3/#req-2-documented)。\n\n\n在语句或要求中引入并后续描述的​​变量​​格式为斜体 var 。偶尔出现的说明性注释（如下所示）不具有规范性，不构成正式要求：\n\n注意\n​​注释用于说明注释内容旨在提供有用信息，但不构成要求。\n\n\n1.1.3 如何阅读要求本文档的核心是​​共同定义了EEA EthTrust认证的要求。要求包含：\n\n安全等级（[S]、[M]或[Q]）\n对应名称\n链接（以”🔗”标识）\n满足要求必须实现的声明\n\n工作组确保本编辑草案中要求的URL始终指向工作组编辑草案中的最新版本要求（可能代表未完成的进展）。部分相同安全等级的要求会分组在小节中，因为它们与特定主题或潜在攻击领域相关。要求后跟解释，包括：\n\n该要求的重要性\n测试方法\n覆盖要求和相关要求的链接\n测试用例\n其他有用信息的链接除要求外，本文档还包含​​§5.4 推荐良好实践​​，格式类似但安全等级标记为”[GP]”。为符合规范，无需实施这些实践，但若谨慎实施可提高智能合约安全性。\n\n以下要求示例：\n\n​​示例4：简单要求​​\n[S] 编译器漏洞SOL-2022-5于.push() 🔗被测代码若：- 从calldata或内存复制字节数组，且其大小不是32字节的倍数- 包含对结果数组执行空.push()指令则​​必须不使用​​早于0.8.15的Solidity编译器版本。\n在Solidity编译器0.8.15之前，复制长度非32字节倍数的内存或calldata可能暴露超出复制范围的数据，这些数据可通过assembly{}观察到。\n另见2022年6月15日安全警报和相关要求[M] assembly{}中的编译器漏洞SOL-2022-5。\n\n\n这是一个安全等级[S]要求，[S]前缀表示其等级。其名称为”编译器漏洞SOL-2022-5与.push()”。编辑草案中的URL（通过”🔗”链接）为：https://entethalliance.github.io/eta-registry/security-levels-spec.html#req-1-compiler-SOL-2022-5-push\n要求声明为：“被测代码若…必须不使用早于0.8.15的Solidity编译器版本。”要求后是对相关漏洞的简要解释和其他讨论链接。\n\n注意\n良好实践的格式与要求相同，但等级标记为[GP]。如§5.4所述，满足这些实践并非必须，也不会单独改变对本规范的符合性。\n\n\n1.1.3.1 覆盖要求部分要求的声明包含替代条件，由关键词unless引入，标识一个或多个​​条件可以覆盖当前要求​​。这些是更高安全等级的要求，若被测代码不直接满足较低等级要求，可通过满足这些覆盖要求来实现符合性。\n某些情况下需满足多个覆盖要求才能覆盖原要求，此时这些要求被称为​​覆盖要求集合​​。必须满足集合中所有要求才能视为覆盖原要求。\n许多情况下，一个要求可能有多个覆盖要求或覆盖要求集合可供选择以满足该要求。例如，有时可通过以下方式满足安全等级[S]要求：\n\n直接满足\n满足安全等级[M]的覆盖要求集合\n满足安全等级[Q]的覆盖要求集合\n覆盖要求支持对常见简单情况进行更简单的测试。\n\n对于使用需要额外谨慎处理功能的更复杂被测代码，它们确保此类使用经过适当检查。\n典型的安全等级[S]要求中，覆盖要求适用于相对罕见的情况或自动化系统通常无法验证被测代码是否满足要求的情况。通过进一步验证适用的覆盖要求，可确定被测代码是否正确使用功能，从而通过安全等级[S]要求。\n若被测代码不满足某要求且无适用的覆盖要求，则该代码不符合EEA EthTrust认证条件。但即使对于此类情况，请注意推荐实践[GP]尽可能满足更多要求；满足本规范中的任何要求都将提高被测代码的安全性。\n在以下要求中：\n\n安全等级为”[S]”\n名称为”禁用tx.origin”\n覆盖要求为”[Q] 验证tx.origin用法”\n\n​​示例5：带覆盖要求的示例​​[S] 禁用tx.origin🔗被测代码​​不得​​包含tx.origin指令，除非满足覆盖要求[Q] 验证tx.origin用法。\n不包含tx.origin指令的要求可通过自动化验证。满足安全等级[Q]覆盖要求[Q] 验证tx.origin用法的被测代码视为符合此安全等级[S]要求。\n作为其他要求的覆盖要求或属于覆盖要求集合的要求会明确说明：\n​​示例6：覆盖要求示例​​[M] 禁用非必要Unicode控制符🔗被测代码​​不得​​使用Unicode方向控制字符，除非：- 它们对正确渲染文本是必要的- 渲染结果不会误导读者这是[S] 禁用Unicode方向控制字符的覆盖要求。\n\n1.1.3.2 相关要求许多要求有​​相关要求​​，即解决主题相关问题的其他要求。提供这些链接作为有用信息。与覆盖要求不同，满足相关要求不能替代满足特定要求来实现符合性。\n1.2 为什么需要认证合约？本节为非规范性内容。以太坊上支撑去中心化应用的许多智能合约已被发现存在安全问题，而目前在发起交易前往往难以实际评估某个地址或合约的安全性。特别是在DeFi领域，用户快速批准代币合约交易、兑换代币和为资金池添加流动性的操作中，有时会忽略安全检查。\n要让企业信任以太坊作为关键数据存储或大额资本转移的交易层，需要明确的信号表明合约已通过适当的安全审计。在区块链开发背景下，生产部署前的早期审查尤为重要，因为尝试在部署后更新或修补智能合约所需的时间、精力、资金和&#x2F;或信誉成本通常远高于其他软件开发场景。\n本智能合约安全标准旨在提升对智能合约安全审计质量的信心，从而增强整个以太坊生态系统的信任度。认证不仅为智能合约的实际或潜在用户提供价值，也为可能受智能合约使用或滥用影响的其他利益相关方提供价值。通过EEA EthTrust认证限制暴露于某些已知弱点，这些利益相关方可以受益于风险降低和对被测代码安全性的信心提升。\n需要注意的是，这种保证并不完整：例如它依赖于出具认证的审计师的能力和诚信。专业声誉可能基于被测代码的后续表现而变化，特别是当被测代码变得足够引人注目以至于人们有更大的动机去探索剩余的漏洞。\n最后，当其他方（包括直接竞争对手）完成认证流程时，智能合约开发者和生态系统利益相关方也会获得价值，因为这意味着其他合约不太可能产生与利用相关的负面新闻，这些新闻可能导致商业领袖、潜在客户&#x2F;用户、监管机构和投资者对以太坊技术产生不安全或高风险的负面看法。\n智能合约安全认证的价值在某些方面类似于适用于飞机部件的认证流程。最直接的是，它通过提供最低质量保证，帮助部件制造商和集成商降低风险。间接而言，这些流程显著减少了航空事故和坠机，拯救生命并赢得监管机构和考虑行业整体安全与风险的客户的信任。许多安全认证流程最初是作为制造商创建的自愿程序，或代表重要市场份额的客户联盟指定和要求开始的。在证明其价值后，其中一些认证流程现在已成为法律要求以保护公众。\n我们希望认证流程的价值能激励其频繁使用，并推动开发使评估过程更简单、更便宜的自动化工具。随着新的安全漏洞、本规范中的问题以及实施挑战的发现，我们希望它们能带来反馈并增加参与企业以太坊联盟EthTrust安全等级工作组或其继任者的积极性，这些工作组负责开发和维护本规范。\n1.3 开发安全的智能合约本节为非规范性内容。\n本规范要求检查的安全问题对智能合约开发者（尤其是这个快速发展的领域的新手）来说并不总是显而易见的。通过让自有代码完成认证流程（即使没有潜在客户要求），智能合约开发者可以在部署前发现代码中存在的已知弱点并进行修复。\n开发者应尽可能使代码安全。与其仅瞄准特定安全等级的符合性要求，不如确保代码尽可能多地实现本规范的要求（根据[GP]尽可能满足更多要求），这有助于确保开发者已考虑本规范解决的所有漏洞。\n除了明显的声誉收益外，开发者还将从这一过程中学习，提高对潜在弱点的理解，从而完全避免这些弱点。\n对开发和部署智能合约的组织而言，这一流程减少了安全审查所需的工作量，并降低了对其信誉、资产和其他资本的风险。\n1.4 漏洞反馈与新漏洞工作组欢迎对本规范的反馈：实施经验、改进清晰度的建议，或对特定章节或要求难以理解的疑问。\n我们特别希望获得关于使用标准机器可读格式进行有效符合性声明的反馈，包括这种格式是否适合存储在区块链上，以及其他用例。\nEEA成员可通过加入工作组提供反馈。任何人都可通过Ethtrust-public Github仓库或发送邮件至EEA Editor提供反馈，邮件将被转发给工作组。\n我们预计本规范发布后将发现新的安全漏洞。为确保将它们纳入修订版本，我们欢迎相关通知。EEA已创建特定电子邮件地址接收新安全漏洞通知：security-notices@entethalliance.org。\n发送至该地址的信息应足以识别和纠正描述的问题，并应包括对该问题的其他讨论的引用。这些信息将由EEA工作人员评估，然后转发给工作组解决问题。\n当这些漏洞影响Solidity编译器，或建议修改编译器以帮助缓解问题时，还应按照solidity-reports中的描述通知Solidity开发社区。\n2. 符合性2.1 符合性声明本规范中的关键词”可以(MAY)”、”必须(MUST)”、”不得(MUST NOT)”、”建议(RECOMMENDED)”和”应当(SHOULD)”应按照BCP14 RFC2119RFC8174中的说明进行解释，且仅当这些词汇以全大写形式出现时才具有此特定含义。\n本规范定义了若干要求。如§1.1.3所述，每个要求都有安全等级([S]、[M]或[Q])和被测代码必须满足的声明。为获得特定安全等级的EEA EthTrust认证，被测代码必须满足该安全等级的所有要求，包括更低安全等级的所有要求。某些要求可以直接满足，也可以通过满足一个或多个覆盖要求来视为满足。\n本文档不产生任何方的主动合规义务，但潜在客户或投资者通过合同谈判或其他流程可能会产生合规要求。\n§5.4推荐良好实践章节包含更多建议。虽然它们格式与要求类似，但以”[GP]”标记开头。不要求测试这些内容；但建议谨慎实施和测试。\n请注意，§5.4推荐良好实践可以增强安全性，但在某些情况下，不完整或低质量的实施可能会降低安全性。\n2.2 谁可以提供EEA EthTrust认证？本版本规范未对谁能执行审计和提供EEA EthTrust认证做出任何限制。没有为出具认证的审计师或工具定义认证流程。这意味着审查员关于执行准确测试的声明是由他们自己做出的。对于提供EEA EthTrust认证第2版认证的任何人，始终存在欺诈、虚假陈述或无能的可能。\n\n注意\n原则上任何人都可以提交智能合约进行验证。但提交者需要注意使用限制，如版权条件等。此外，在智能合约开发控制有限的情况下，证明满足某些要求可能更加困难。工作组期望其成员（他们编写了本规范）保持高标准的诚信，并熟悉本规范，同时注意到还有许多其他人也这样做。工作组或EEA可能会为EEA EthTrust安全等级规范后续版本开发审计师认证计划。\n\n\n2.3 识别认证内容EEA EthTrust评估针对被测代码，被测代码是指智能合约或几个相关智能合约的Solidity源代码，以及使用指定参数编译代码生成的字节码。如果被测代码被划分为可部署在不同地址的多个智能合约，则称为合约集合。\n3. 安全考虑3.1 智能合约的上下文 - 更广泛的考虑信息系统安全是本规范主要的工作领域。任何中等复杂度的系统都存在固有风险。本规范描述了以太坊智能合约中安全问题的测试。然而，不存在完美的安全性。EEA EthTrust认证意味着至少已对智能合约执行了定义的最小检查集。这并不意味着被测代码绝对没有安全漏洞。新的安全漏洞会不时被发现。手动审计程序需要技能和判断力。这意味着始终存在审查中未注意到漏洞的可能性。\n3.2 可升级合约以太坊中的智能合约默认是不可变的。然而，在某些场景下需要修改它们，例如添加新功能或修复错误。可升级合约是通过启用对固定地址执行的代码的更改来满足这些需求的任何类型的合约。可升级合约的一些常见模式使用代理合约：用户直接与之交互的简单包装器，负责将交易转发到另一个合约（本文档中称为执行合约，也称为逻辑合约），该合约包含实际实现智能合约行为的代码。执行合约可以被替换，而作为访问点的代理合约永远不会更改。这两个合约在代码无法更改的意义上仍然不可变，但一个执行合约可以与另一个交换。因此，代理合约可以指向不同的实现，从而”升级”软件。这意味着遵循此模式使合约集合可升级的合约通常不能被视为不可变的，因为代理合约本身可以将调用重定向到新的执行合约，这些执行合约可能不安全或恶意。通过满足本规范中关于访问控制的要求以限制部署新执行合约的升级能力，并按照 [Q]按文档实现 记录升级模式并遵循该文档，被测代码的部署者可以证明可靠性。\n通常，代理合约的EEA EthTrust认证不适用于可升级合约的内部逻辑，因此在通过代理合约升级到新执行合约之前需要对新执行合约进行认证。这种核心结构有多种可能的变体，例如包含多个执行合约的合约集合。在称为变形升级的攻击中，一系列智能合约用于说服人们（例如DAO中的投票者）批准部署某段代码，但链中的一个代理合约被更新为部署不同的恶意代码。其他模式依赖于使用CREATE2指令在已知地址部署智能合约。目前可以使用selfdestruct()方法删除该地址的代码，然后向该地址部署新代码。这种可能性有时用于节省Gas费用，但也用于变形升级攻击中。\n3.3 预言机以太坊网络的常见功能是使用预言机：可以提供来自链上或链下数据的信息的功能。预言机解决了一系列问题，从提供随机数生成到资产数据，管理流动性池的操作，以及实现对天气、体育或其他特殊兴趣信息的访问。预言机在DeFi和游戏中被大量使用，其中资产数据和随机化是协议设计的核心。\n该规范包含检查智能合约是否足够强大以适当处理任何信息返回的要求，包括可能被故意制作用于预言机特定攻击的畸形数据的可能性。虽然预言机的某些方面在本规范考虑范围内，但预言机仍可能提供错误信息，甚至主动产生有害的虚假信息。两个关键的考虑因素是数据损坏或被操纵的风险，以及预言机故障的风险。与这些考虑因素相关的漏洞 - 过度依赖TWAP（Time-Weighted Average Price），以及对预言机故障的不安全管理 - 已经反复发生，导致各种DeFi协议损失数百万美元的价值。虽然有许多高质量和可信的预言机可用，但即使使用合法数据也可能遭受攻击。当调用预言机时，需要检查接收到的数据是否过时，以避免抢先交易攻击。即使在非DeFi场景中，例如随机性来源，通常也需要为每笔交易重置数据源，以避免下一笔交易的套利。\n定价预言机的常见策略是提供时间加权平均价格（称为TWAP）。这在一定程度上防止了闪电贷攻击等突然价格飙升，但代价是提供过时信息。仔细选择时间窗口很重要：当时间窗口太宽时，它不会反映波动的资产价格，给套利者留下机会。然而，资产的”瞬时”价格通常不是一个好的数据点：它是预言机数据中最容易被操纵的部分，而且几乎总是在交易执行时就已经过时。使用整理各种来源数据、从数据中清除异常值并受到社区好评的预言机更有可能可靠。如果预言机是链下的，它反映的是链上数据是过时的还是可靠和准确地反映链下数据是一个重要的考虑因素。即使使用选择良好的TWAP的预言机也可以操纵流动性池或其他DeFi结构，特别是通过利用闪电贷和闪电互换廉价筹集资金。如果目标操纵的资产流动性不足，则可能使其容易受到攻击者仅持有相对少量流动性的影响而导致大幅价格波动。\n在使用预言机时，第二个重要的考虑因素是如何优雅地处理故障情况。如果预言机停止返回数据，或者突然返回一个极不合理的数值，会发生什么？至少有一个协议因此遭受了损失：在极端价格崩盘的罕见情况下，价格并未真正跌至零，而是“挂”在了一个设定的最小值上，使得一些囤积了接近零价格资产的交易者可以将其高价卖回协议。将最小值或最大值硬编码在系统中，可能导致价格难以真实反映市场情况。\n3.4 外部交互与重入攻击依赖外部代码的代码可能引入多个攻击向量。这包括外部依赖包含恶意代码或通过安全漏洞受到恶意操纵的情况。然而，未能充分管理外部调用的可能结果也会引入安全漏洞。\n以太坊智能合约中最常被引用的漏洞之一是重入攻击。这些攻击允许恶意合约在原始合约的函数调用完成之前回调到调用它的合约中。这种效果导致调用合约以意外的方式完成其处理，例如，对状态变量进行意外更改。\n虽然Check-Effect-Interactions实现模式提供了关键保护，但新兴的跨合约交互模式可能需要额外的保障措施。定期审查交互模式有助于识别新的重入向量。\n只读重入攻击发生在视图函数读取随后将被更改的状态时。这些攻击特别危险，因为此类函数通常缺乏保障措施，因为它们不修改合约的状态。然而，如果状态不一致，可能会报告不正确的值。这种欺骗可能导致其他协议读取不准确的状态值，从而可能导致意外操作或结果。这个问题可能影响依赖这些视图函数准确报告状态的其它合约，以及被重入的合约本身。因此，调用智能合约的第三方以及由合约集合组成的协议可能容易受到只读重入的影响。\n\n示例7：Rari协议只读重入攻击​​\n在Rari协议攻击中，攻击者通过大额闪电贷存入资金，在借款调用期间触发Comptroller合约的`exitMarket()`函数。该函数读取cETH合约状态时发生只读重入。由于cETH合约尚未记录借款状态，攻击者得以赎回初始存款并保留借款资金。详见[certik-rari](https://entethalliance.org/specs/ethtrust-sl/v3/#bib-certik-rari)分析报告。\n\n\n3.5 签名机制本规范中的一些要求涉及可延展签名。这些是根据方案创建的签名，给定消息和签名，可以高效计算不同消息的签名 - 通常是以特定方式转换的消息。虽然这种签名方案允许有价值的用例，但如果不谨慎使用可能导致漏洞，这就是为什么本规范试图适当限制其使用。同样，对于使用可延展输入创建的可延展签名消息，哈希冲突可能发生。\n本规范中的其他要求与利用用于创建签名消息的输入中的模糊性进行攻击有关。当签名消息不包括关于其预期使用位置、时间、次数等的足够识别信息时，消息签名可能被用于（或重复用于）非预期功能、合约、链或时间。\n有关此主题的更多信息以及潜在利用，请参见chase。\n3.6 Gas与Gas价格Gas攻击是故意滥用以太坊用于调节计算能力消耗的Gas机制，以防范意外或不利结果（如拒绝服务攻击）。由于以太坊设计将Gas机制作为调节功能，仅检查交易是否有足够的Gas是不够的；检查Gas攻击需要考虑被测代码实现的目标和业务逻辑。\nGas虹吸是另一种滥用以太坊用于调节计算能力消耗的Gas机制的行为，攻击者从易受攻击的合约中窃取Gas，要么拒绝服务，要么为自己谋利（例如铸造Gas代币）。与Gas攻击类似，检查Gas虹吸需要仔细考虑被测代码实现的目标和业务逻辑。\nGas代币在铸造时使用Gas，在销毁时释放略少的Gas，前提是EVM退还足够数量的Gas以清除状态。当Gas价格低时铸造的Gas代币可以在Gas价格高时燃烧以补贴以太坊交易。在以太坊主链上，随着2021年8月部署EIP-3529的伦敦硬分叉，Gas退款被移除，实际上禁用了Gas代币。此外，以太坊网络升级的常见功能是更改特定操作的Gas价格。EEA EthTrust认证仅对指定的EVM版本有效；对其他EVM版本无效。因此，重新检查代码以确保其安全属性在网络升级中保持不变或采取补救措施非常重要。\n3.7 MEV（恶意提取价值）MEV(Maliciously Extracted Value)在本文档中表示”恶意提取价值”，指的是区块生产者或区块链的其他参与者通过恶意重新排序交易或抑制交易，或通过提出交易或采取其他行动获取非预期利益（即窃取）的可能性。\n\n示例8：MEV攻击实例​​\n当智能合约承诺奖励首个回答问题的交易时，区块生产者可以窃取其他交易中的答案，并丢弃包含该答案的所有其他交易。\n\n\n注意\nMEV一词通常扩展为“矿工提取价值”(Miner Extracted Value)，有时也扩展为“最大可提取价值”(Maximum Extractable Value)。通常，如上例所示，区块生产者可以充分利用漏洞。\n然而，即使没有自己生产区块，其他参与者也可能利用MEV。\n此外，部分价值提取本质上是区块生产者利用已知的套利机会，以提供一个更可预测的高效市场。\n\n一些MEV攻击可以通过仔细考虑交易中包含的信息（包括合约所需的参数）来防范。其他缓解策略包括针对排序攻击的保护措施。以太坊基金会维护有关MEV的信息资源集[EF-MEV](https://entethalliance.org/specs/ethtrust-sl/v3/#bib-ef-mev)。\n\n3.8 排序攻击各种攻击与恶意重新排列区块中的交易有关，例如通过重新排序、审查或插入特定交易。虽然这类攻击的主要动机是促进MEV攻击，但它们也可用于为其他类型的攻击创造条件。\n\n警告\n 示例9：排序攻击\n投票合约的设置阶段有时可能被恶意抑制或抢跑投票选项提案所利用，以限制投票阶段符合条件的候选人数量。区块提议者可以在知道他们将产生一系列区块时恶意安排此类投票初始化。\n\n\n\n排序攻击有多种类型：\n\n审查攻击, 区块处理者主动抑制提议的交易，为自己谋利。\n\n抢跑, 基于在添加到区块之前可见的交易，允许恶意参与者提交替代交易，挫败原始交易的目的。\n\n示例10：抢跑攻击策略\n在一个旨在认证原创著作权的系统中，恶意参与者利用著作权声明中的信息伪造竞争性声明，并率先将伪造声明加入区块，从而为其虚假主张作者身份提供依据。\n若重复实施此类攻击，将成为针对该服务本身的有效拒绝服务(DoS)攻击手段。\n\n\n尾随, 类似于抢跑，但攻击者将他们的交易放在被攻击的交易之后。\n\n三明治攻击, 攻击者将受害者的交易不理想地放在另外两个交易之间。\n\n示例11：三明治攻击策略​​\n攻击者构造代币买入交易插入受害者买单之前推高价格，再在抬高的价位插入对应卖单，通过这种\"夹心\"操作实现无风险套利。\n\n\n\n注意\n与利用MEV一样，区块生产者处于利用任何排序漏洞的最佳位置。如果他们提前知道他们将在未来产生特定区块，这一点尤其成立。例如参见[futureblock](https://entethalliance.org/specs/ethtrust-sl/v3/#bib-futureblock)或[postmerge-mev](https://entethalliance.org/specs/ethtrust-sl/v3/#bib-postmerge-mev)。\n\n\n实现等级[Q]要求 [Q]对敏感操作使用时间锁延迟 可以防止排序攻击影响敏感操作的执行。其他缓解策略包括使用哈希承诺方案hash-commit、批量执行或使用第2层EEA-L2链进行处理。\n3.9 源代码、编译指示和编译器本规范版本要求编译的字节码以及构成被测代码的是Solidity源代码。Solidity在很大程度上是以太坊智能合约最常用的编程语言，要求Solidity源代码的好处包括简化许多测试，并且有大量针对Solidity源代码的安全研究。\nSolidity允许源代码使用编译指示语句指定使用的Solidity编译器版本。本规范不要求任何特定的Solidity编译器版本，只要其不低于0.3.0，但在安全等级[Q]下，仅允许EEA EthTrust认证用于一组有限的Solidity编译器版本，已知这些版本的Solidity编译器在相同选项下从给定源代码生成相同的字节码。\n要求合约只是Solidity源代码有一些缺点。最明显的是某些代码不是用Solidity编写的。不同的语言有不同的特性，通常支持不同的编码风格。也许更重要的是，这意味着用Solidity编写的已部署合约不能直接测试，除非有人提供源代码。引入源代码读取的另一个重要限制是它容易受到同形文字攻击（Homoglyph Attacks），其中看起来相同但不同的字符（如拉丁字母”p”和西里尔字母”р”）可以欺骗人工阅读源代码的人，掩盖恶意行为。还有相关的攻击使用诸如Unicode方向控制字符之类的功能，或利用组合字符的不一致规范化来实现相同类型的欺骗。\n3.10 合约部署本规范主要解决智能合约代码中出现的漏洞。然而，需要注意的是，智能合约的部署通常是协议操作的关键要素。智能合约安全的某些方面主要取决于被测代码的部署方式。即使经过审计的合约如果部署不当也可能容易被利用。\n为特定区块链编写的代码可能依赖于该区块链可用的功能。当代码部署到兼容但不同的链时，功能差异可能暴露漏洞。对于部署到使用EVM补丁分叉的区块链或平行链的任何合约，在虚拟机级别可能不再适用常见的安全假设。首先将EEA EthTrust认证的合约部署到每个链的测试网，并进行彻底的渗透测试是有价值的。\n特别令人担忧的是可升级合约的问题，以及部署中具有初始化函数的任何合约。许多合约因意外保留其初始化函数不受保护，或在部署中未在同一交易中调用初始化函数的非原子部署而被黑客攻击。这种情况容易受到抢跑攻击，并可能导致合约被恶意方接管，以及资金被盗或丢失。在与合约部署相同的交易中初始化合约可降低恶意行为者控制合约的风险。\n此外，在构造函数和初始化函数中为msg.sender或其他变量分配访问角色对部署的影响需要仔细考虑。这在§5.3.2访问控制要求中进一步讨论。存在专门用于安全代理使用和安全合约部署的多个库和工具。从命令行工具到库再到复杂的基于UI的部署工具，存在许多解决方案来防止不安全的代理部署和升级。对给定合约的初始化函数使用访问控制，并限制在部署时或部署后可以调用初始化函数的次数，可以增强协议本身及其用户的安全性和透明度。此外，禁用重新初始化执行合约能力的函数可以防止以后的攻击或事故。\n虽然本规范不要求被测代码已部署，但某些要求在代码部署到区块链时更容易测试，或者在某些情况下可能只能”在现场”彻底测试。\n3.11 部署后监控虽然智能合约部署后的监控超出了本规范的正式范围，但它是智能合约安全的重要考虑因素。新的攻击技术会不时出现，某些攻击只能通过实时实施的主动措施来防范。对链上活动的监控可以帮助在攻击造成不可挽回的损失之前检测到它们。监控（基于自动化数据集）可以识别已在其他地方发生的攻击，甚至是在其他区块链上。自动化监控可以促进快速响应，生成警报或自动启动操作，提高合约的安全性，否则当安全响应延迟甚至几个区块时，合约可能会受到损害。然而，区分攻击和个人异常行为可能很困难。纯粹依赖自动化监控会使区块暴露于恶意行为者故意触发自动安全响应以致破坏区块链或项目的风险，类似于拒绝服务攻击。\n3.12 网络升级EVM或以太坊虚拟机作为以太坊网络的分布式状态机，计算由交易引起的状态变化。EVM维护简单以太转账以及更复杂的智能合约交互的网络状态。换句话说，它是运行智能合约代码的”计算机”（尽管实际上是软件）。以太坊社区不时实施网络升级，有时也称为硬分叉。这是对以太坊的向后不兼容更改。因为它们通常会更改EVM，所以以太坊主网网络升级通常对应于EVM版本。\n网络升级可能或多或少影响以太坊的各方面，包括更改EVM操作码或其Gas价格、更改添加块的方式或支付奖励的方式等许多可能性。由于不保证网络升级向后兼容，较新的EVM版本可能以意外的方式处理字节码。如果网络升级更改EVM以修复安全问题，考虑该更改很重要，遵循该升级是一个好习惯。因为对本规范的符合性声明仅对特定的EVM版本有效，所以网络升级可能意味着需要更新的审计以维护对当前以太坊网络有效的EEA EthTrust认证。网络升级通常只影响少数功能。这有助于限制升级后审计代码所需的努力：通常不会有影响被测代码的更改，或者审查受网络升级影响的唯一部分的一小部分就足以更新EEA EthTrust认证。\n3.13 组织与链下安全态势智能合约安全不仅限于代码，还涵盖组织流程和链下基础设施。全面的安全策略需要解决协议管理的技术和运营两个方面。\n运营安全措施包括：\n\n关键密钥存储的硬件安全模块(HSM)\n分布式密钥持有者的多重签名方案\n所有团队成员的定期安全培训\n安全开发环境协议\n\n基础设施安全包括：\n\n受保护的部署基础设施\n安全通信渠道\n访问控制系统\n网络安全监控\n\n监控和响应包括：\n\n实时交易监控\n自动警报系统\n事件响应程序\n紧急关闭能力\n\n旨在遵循最佳实践的组织将实施：\n\n定期对链上和链下系统进行安全评估\n记录明确角色和职责的事件响应计划\n基础设施的定期渗透测试\n安全意识培训计划\n访问控制审查和更新\n\n\n示例12：安全运营中心监控项​​\n健全的安全运营中心(SOC)应监控：非常规交易模式、可疑管理操作、基础设施安全警报、智能合约异常行为。\n\n\n3.14 预防链上对抗条件智能合约在高度对抗的环境中运行，其中网络条件、外部数据源和经济激励可能被恶意行为者操纵。在部署前模拟这些攻击场景对于识别在标准测试条件下可能不会显现的漏洞至关重要。需要模拟的关键对抗场景包括：网络操纵：\n\n极端Gas价格波动\n故意造成网络拥堵\n矿工策略性交易排序\n区块时间戳操纵\n\n预言机攻击：\n\n通过闪电贷操纵价格反馈\n延迟或过时数据场景\n多种预言机故障模式\n跨链预言机不一致\n\n经济战：\n\n极端资产价格波动\n流动性池操纵\n代币经济攻击\n套利利用\n\n治理利用：\n\n代币投票操纵\n提案泛滥攻击\n投票期的定时攻击\n恶意参数更新\n\n标准测试环境通常无法捕捉这些对抗条件之间的复杂交互。在受控测试下看起来安全的协议，当多个攻击向量组合或经济激励足够大时，可能隐藏关键漏洞。\n\n注意\n动态测试环境:\n测试这些场景需要能够模拟复杂市场条件和参与者行为的动态环境。简单的单元测试或静态分析工具无法充分建模这些对抗情况。\n\n\n通过在部署前建模这些场景，开发者可以：\n\n识别经济模型中的边缘情况\n验证断路器机制\n测试紧急关闭程序\n检查治理保障措施\n评估压力下的协议弹性\n\n4. 测试方法4.1 单元测试单元测试实践基于多个独立测试用例，每个测试用例验证特定需求。这种方法可融入测试驱动开发（TDD）流程，即在编写代码时同步构建测试用例集，确保变更不会引入已解决问题或新问题。\n测试覆盖率是评估单元测试价值的关键指标，意味着不仅需要覆盖每个需求，还需覆盖触发该需求的所有可能路径。例如，测试tx.origin指令时，需同时测试其在Solidity代码和assembly&#123;&#125;中的使用场景，否则无法满足特定需求的全覆盖测试。\n典型的单元测试自动化方案包括构建测试工具链，确保无论智能合约代码或运行环境发生变更（如集成新服务或系统），测试用例都能自动执行。\n4.2 静态分析静态分析指直接检查被测代码以识别潜在问题。本规范中，安全等级[S]的所有问题均可通过自动化静态分析发现。企业以太坊联盟（EEA）的EthTrust安全等级工作组已开始收集相关工具信息，并构建测试用例库验证工具准确性。\n人工静态分析同样重要，该方法依赖专家的经验和判断力识别代码编写方式可能导致的问题。工作组认为人工静态分析足以验证被测代码是否符合本规范安全等级[M]的要求。\n4.3 模糊测试模糊测试是通过向合约输入多样化数据来暴露缺陷的自动化测试方法。其有效性很大程度上取决于语料库质量——即测试输入数据集。维护语料库时需平衡代码覆盖率与效率，剔除冗余或重复输入。\n模糊测试主要分为三类：​​黑盒测试​​：将智能合约视为不透明目标，仅通过外部接口测试，适用于快速发现典型使用场景中的基础错误。​​白盒测试​​：基于完整源代码和执行路径可见性，使用符号执行等技术指导输入生成，能更精准定位复杂逻辑缺陷。​​灰盒测试​​：结合前两种方法要素，通过有限插装或启发式方法生成测试输入，在检测广度与深度间取得平衡。\n4.4 变异测试变异测试通过向源代码注入人工缺陷（变异）来评估测试用例的有效性。若测试用例能检测到变异（即”杀死”变异体），说明测试充分；否则表明存在覆盖缺口。\n智能合约适用的变异操作包括：状态变量突变： 修改状态变量声明和作。算术突变： 改变算术运算以测试数值计算。示例包括：\n\n替换运算符 （ ， -， *， &#x2F;）\n修改边界条件\n引入溢出&#x2F;下溢条件控制流突变： 更改执行路径，例如：\n反转条件语句\n修改循环条件\n更改函数修饰符访问控制突变： 不同的安全关键型权限检查，例如：\n删除所有权检查\n更改基于角色的权限\n修改身份验证逻辑关键指标是变异分数（被杀死的变异数&#x2F;非等价变异总数）。虽然100%的分数难以实现，但对核心合约组件应保持高分。该技术通过识别等价变异（不影响行为的变异）来优化测试效率，可作为模糊测试的补充。\n\n4.5 符号执行符号执行通过追踪符号值（类似代数中的未知变量）而非具体数值来分析程序。该方法能推导出可能输出结果的约束条件，用于检测潜在漏洞，同时可识别死代码、未使用变量等问题。详见符号执行权威指南WSE。\n4.6 形式化验证形式验证是一系列以数学方式证明代码某些属性的技术。它已用于嵌入式系统等应用。形式验证在智能合约中有很多用途，例如测试活跃性、高级别的协议不变量以确保安全性，或者证明程序执行的更窄、更具体的属性。\n在形式验证中，创建智能合约预期或期望结果的正式（符号或数学）规范，从而能够对协议的正确性进行正式的数学证明。为此，智能合约本身通常被翻译成另一种语言。\n存在多种用于创建形式验证证明的语言和程序，其中一些语言和程序的明确目的是使临时用户和非数学家更容易访问形式验证。请参阅 EF-SL了解一些示例。\n如果做得正确，形式化验证可以保证模糊测试和静态分析等方法无法保证。但是，其准确性取决于正确建模测试代码，并选择适当的属性进行测试。这项任务通常需要大量的专业知识，如果模型不能准确反映原始测试代码的属性，则得出的结果也可能不适用。\n许多智能合约的不可变性使得形式验证具有吸引力。\n4.7 属性与不变量基于属性的测试通过定义系统应满足的特性或不变量（在任何情况下都应保持的条件），自动生成测试场景。不变量测试是其子集，专门验证这些不变条件的保持性。\n4.8 测试网部署除了静态分析之外，许多测试方法都依赖于能够执行测试代码。一种常见的做法是将代码部署在测试网上，测试网是一个专门为测试而创建的区块链，已知该区块链包括可能包含安全漏洞的智能合约，并且底层加密货币和gas的成本为零或可以忽略不计。\n5. EEA EthTrust安全等级EEA EthTrust认证分为三个安全级别。安全级别描述了每个安全级别的认证的最低要求：[S]、[M] 和 [Q]。这些安全级别依次提供更强的保证，确保智能合约不存在特定的安全漏洞。\n安全级别 [S] 的设计使得在大多数情况下，如果按照众所周知的模式使用Solidity的常见功能，测试代码可以通过自动化的“静态分析”工具进行认证。安全级别 [M] 要求进行更严格的静态分析。它包括需要人工审核员确定是否有必要使用某个功能，或者关于代码安全属性的声明是否合理的要求。安全级别 [Q] 提供了对测试代码实现的业务逻辑的分析，并且代码不仅没有表现出已知的安全漏洞，而且还正确地实现了它声称要执行的逻辑。可选的§5.4推荐良好实践，如果正确实施，将进一步增强智能合约的安全性。但是，没有必要测试它们是否符合此规范。\n\n注意\n该方案已与“OWASP 应用程序安全验证标准”规范系列[ASVS]中使用的一致性方法进行了比较。存在一些明显的差异，这主要是由于ASVS旨在实现的普遍适用性与该规范非常精确地专注于测试用 Solidity 编写的以太坊智能合约的安全性之间的差异。\n\n\n该规范解决的漏洞来自多个来源，包括 Solidity 安全警报solidity-alerts、智能合约弱点分类swcregistry、TMIO 最佳实践tmio-bp、安全咨询通知的各种来源、以太坊社区的讨论和研究人员介绍新发现的漏洞，以及工作组参与者的丰富实践经验。\n5.1 安全等级[S]EEA EthTrust认证在安全等级[S]下，旨在允许未经引导的自动化工具分析大多数合约的字节码和源代码，并确定它们是否符合要求。安全等级[S]的要求设计为可使用自动化静态分析进行测试。截至本规范版本，工作组已开始维护（工具注册表），其中列出了声称覆盖特定要求的工具，以及测试用例和在这些工具中运行它们的结果。更多信息请参见ET-tools。\n对于某些难以自动验证的情况，存在更高级别的覆盖要求，可以通过满足这些要求来实现合规性。为了获得EEA EthTrust认证的安全等级[S]，被测代码必须满足所有安全等级[S]的要求，除非它满足适用的覆盖要求中的每一项要求。\n[S]哈希编码必需包含chainid被测代码必须按照[EIP-155]的建议，在交易哈希中纳入chainid值。\nEIP-155描述了一种增强的哈希规则，通过在哈希中纳入链标识符。虽然这仅在存在唯一链标识符时才能防止重放攻击，但使用该机制可以提供一定程度的鲁棒性，并大大增加执行重放攻击的难度。\n[S]禁用CREATE2被测代码不得包含CREATE2指令，除非满足以下覆盖要求集： - [M] 保护CREATE2调用 - [M] 记录特殊代码用途\nCREATE2操作码提供了与尚未在链上存在但可能最终包含代码的地址进行交互的能力。虽然这对于部署和与合约的反事实交互很有用，但它允许调用尚未知或可能被修改的代码，这些代码可能由于错误或保护不足而变得恶意或不安全。\n[S]禁用tx.origin被测代码不得包含tx.origin指令，除非满足覆盖要求[Q]验证tx.origin用法。\ntx.origin是Solidity中的一个全局变量，返回发送交易的账户地址。使用tx.origin的合约可能允许授权账户调用恶意合约，使恶意合约能够在非预期情况下通过授权检查。更好的做法是使用msg.sender进行授权。参见swcregistry中的SWC-115获取示例。\n[S]禁用精确余额检查被测代码不得测试账户余额是否完全等于（即==）指定金额或变量值，除非满足覆盖要求[M]验证精确余额检查。\n以账户余额作为某些操作的基础存在风险，包括意外接收以太币或其他代币，包括故意转移代币以使此类测试失败的MEV攻击。参见相关要求：\n\n[M] 随机性来源\n[M] 不要滥用区块数据\n[Q] 防范MEV攻击以及本规范安全考虑中§3.7 MEV（恶意提取价值）小节，swcregistry中的SWC-132示例，以及CWE-667中描述的不当锁定问题。\n\n[S]禁止对连续可变长度参数进行哈希被测代码不得对连续的可变长度参数使用abi.encodePacked()。\n在哈希之前，abi.encodePacked()的每个可变长度参数的元素按顺序打包。通过在连续的可变长度参数之间重新排列元素，同时保持它们连接顺序不变，可能导致哈希冲突。\n[S]禁用selfdestruct()被测代码不得包含selfdestruct()指令或其现已弃用的别名suicide()，除非满足以下覆盖要求集：- [M] 保护自毁操作- [M] 记录特殊代码用途\n如果selfdestruct()指令（或其已弃用的替代项suicide()）未得到妥善保护，恶意代码可以调用它并销毁合约，发送合约持有的任何以太币，从而可能窃取资金。还可以与CREATE2结合使用来更改特定地址的代码。此功能可能破坏不变性和去信任保证，引入众多安全问题。此外，一旦合约被销毁，发送的任何以太币都将丢失，这与禁用合约不同，后者会导致发送以太币的交易回滚。\n自Solidity编译器版本0.8.18solidity-release-818起，selfdestruct()已被正式弃用，不鼓励使用。\n参见[swcregistry]中的SWC-106和EIP-6049。\n[S]禁用assembly&#123;&#125;被测代码不得包含assembly&#123;&#125;指令，除非满足以下覆盖要求集：- [M] 避免常见的assembly&#123;&#125;攻击向量- [M] 记录特殊代码用途- [M] assembly&#123;&#125;中的编译器错误SOL-2022-5- [M] 编译器错误SOL-2022-7- [M] 编译器错误SOL-2022-4- [M] 编译器错误SOL-2021-3以及如果使用Solidity编译器版本0.5.5或0.5.6，还需满足[EthTrust-sl-v1]中的[M] assembly&#123;&#125;中的编译器错误SOL-2019-2。\nassembly&#123;&#125;指令允许包含更低级别的代码。这使作者能够更强烈地控制生成的字节码，例如可用于优化gas使用。然而，它也潜在地暴露了许多漏洞和错误，这些都是额外的攻击面，并且有多种方法可以使用assembly&#123;&#125;引入故意设计的难以检测的恶意代码。\n5.1.1 文本和同形字[S]无Unicode方向控制字符  被测代码不得包含任何Unicode方向控制字符  U+2066、U+2067、U+2068、U+2029、  U+202A、U+202B、U+202C、U+202D或U+202E  除非它满足覆盖要求[M]无不必要的Unicode控制。\n使用不可见的Unicode方向控制字符改变字符的显示顺序，可以在查看源代码时掩盖恶意代码，欺骗人工审计员。关于Unicode方向控制字符的更多信息，请参见W3C注释《如何为双向文本使用Unicode控制》unicode-bdo。\n5.1.2 外部调用[S] 检查外部调用返回  使用低级调用函数（即call()、delegatecall()、staticcall()和send()）进行外部调用的被测代码必须检查每次使用的返回值以确定调用是否失败，除非它满足覆盖要求  [M] 处理外部调用返回。\n另请参见相关要求：[M]保护外部调用，和[Q]验证外部调用。通常，调用中的异常会导致回滚。这将”冒泡”传播，除非在try&#x2F;catch中处理。然而，Solidity定义了一组低级调用函数：  \n\ncall()  \ndelegatecall()  \nstaticcall()\nsend()\n\n使用这些函数进行的调用行为不同。它们在失败时不会回滚，而是返回一个布尔值，指示调用是否成功完成。不显式检查返回值可能导致调用者合约中出现意外行为。依赖这些调用在失败时回滚会导致它们在未成功时出现意外行为。另请参见SWC-104 swcregistry，error-handling中描述的错误处理，未检查的返回值如CWE-252所述，以及相关要求：  \n\n[S] 使用检查-效果-交互，  \n[M] 处理外部调用返回，和  \n[Q] 验证外部调用。\n\n[S] 使用检查-效果-交互(Check-Effects-Interaction)  进行外部调用的被测代码必须使用`检查-效果-交互模式`来防止重入攻击  除非它满足以下覆盖要求集:- [M] 保护外部调用，和  - [M] 记录特殊代码使用，或它满足以下覆盖要求集:- [Q] 验证外部调用，  - [Q] 记录合约逻辑，  - [Q] 记录系统架构，和  - [Q] 按文档实现。\n检查-效果-交互模式是在进行任何状态更改之前验证所有前置条件，然后才在外部交互之前完成所有状态更新，然后才执行外部调用。\n以这种方式设计合约可显著减少重入攻击的范围。作为此模式的一部分，除了检查特定合约效果外，还可以测试协议不变量，以进一步确保请求不会产生不安全的结果。另请参见§3.4 外部交互和重入攻击，[c-e-i]中”Solidity安全注意事项”solidity-security对”检查-效果-交互”的解释，”Solidity模式”solidity-patterns中的”检查效果交互”，以及freipi。\n[S] 无delegatecall()  被测代码不得包含delegatecall()指令, 除非它满足以下覆盖要求集：- [M] 保护外部调用，和  - [M] 记录特殊代码使用，或它满足以下覆盖要求集- [Q] 验证外部调用，  - [Q] 记录合约逻辑，  - [Q] 记录系统架构，和  - [Q] 按文档实现。\ndelegatecall()指令使外部合约能够操纵调用它的合约的状态，因为代码以调用者的余额、存储和地址运行。\n5.1.3 编译器错误Solidity编译器的不同版本中存在许多已知的安全错误。本小节中的要求确保被测代码不会触发这些错误。要求的名称包括solidity-bugs-json中首次记录的错误的uid，作为可用于查找有关该错误的更多信息的关键字。[solidity-bugs]描述了用于JSON格式错误列表的约定。本小节中的要求按照受影响的最新Solidity编译器版本排序。\n\n注意\n实施推荐的良好实践[GP]使用最新编译器意味着被测代码通过本小节中的所有要求。  \n\n一些与编译器相关的错误包含在§5.2.5安全级别[M]编译器错误和覆盖要求作为安全级别[M]要求中，因为它们是本小节中的覆盖要求，或者因为它们是安全级别[S]一组覆盖要求的一部分，该要求已经确保无法触发错误。\n一些错误是在已知的`Solidity`编译器版本中引入的，而其他错误则已知或假定存在于所有`Solidity`编译器版本中，直到它们被修复。\n\n[S] 编译器错误SOL-2023-3  包含Yul代码并使用verbatim指令两次的被测代码，每次周围都有相同的代码，  在使用Solidity编译器版本0.8.5至0.8.22（含）时必须禁用块重复数据删除器。\n从Solidity编译器版本0.8.5到修复的0.8.22，块重复数据删除器错误地处理了verbatim项，意味着有时它会根据周围的代码混淆两个项，而不是正确比较它们。另请参见2023年11月8日的安全警报。\n[S] 编译器错误SOL-2022-6  ABI编码包含动态组件的元组（包括结构体、返回值或参数列表）的被测代码，  并且其最后一个元素是基本类型uint或bytes32的calldata静态数组，  不得使用Solidity编译器版本0.5.8至0.8.15（含）。\n从Solidity编译器版本0.5.8到修复的0.8.15，使用ABIEncoderV2对元组进行ABI编码，其最后一个组件是基本类型uint或bytes32的calldata静态数组，可能导致数据损坏。另请参见2022年8月8日的安全警报。\n[S] 编译器错误SOL-2022-5与.push()  被测代码  - 从calldata或内存复制字节数组  - 其大小不是32字节的倍数，且有一个空的.push()指令写入结果数组，  不得使用早于0.8.15的Solidity编译器版本。\n直到Solidity编译器版本0.8.15，复制长度不是32字节倍数的内存或calldata可能暴露超出复制的数据，这些数据可以通过assembly{}观察到。另请参见2022年6月15日的安全警报和相关要求[M] assembly{}中的编译器错误SOL-2022-5 。\n[S] 编译器错误SOL-2022-3  - 对同一函数使用内存和calldata指针，且  - 在继承期间更改函数的数据位置，且  - 在仅知道基合约中原始函数签名的位置进行内部调用  不得使用Solidity编译器版本0.6.9至0.8.12（含）。\nSolidity编译器版本从0.6.9到修复的0.8.13有一个错误，错误地允许内部或公共调用使用仅对外部调用有效的简化，将内存和calldata视为等效指针。另请参见2022年5月17日的安全警报。\n[S] 编译器错误SOL-2022-2  具有嵌套数组的被测代码  - 将其传递给外部函数，或  - 将其作为输入传递给abi.encode()，或  - 在事件中使用它  不得使用Solidity编译器版本0.6.9至0.8.12（含）。\nSolidity编译器版本从0.5.8到修复的0.8.13有一个错误，意味着嵌套数组的单次编码和解码可以读取超出calldatasize()的数据。另请参见2022年5月17日的安全警报。\n[S] 编译器错误SOL-2022-1  被测代码  对短于32字节的bytesNN类型使用数字字面量，或  对任何bytesNN类型使用字符串字面量，  并将此类字面量作为第一个参数传递给abi.encodeCall()，  不得使用Solidity编译器版本0.8.11或0.8.12。\nSolidity定义了一组变量类型，统称为bytesNN或固定长度变量类型，指定变量的长度为固定的字节数，遵循模式  \n\nbytes1  \nbytes2  \n…  \nbytes32\n\nSolidity编译器版本0.8.11和0.8.12有一个错误，意味着在某些情况下，abi.encodeCall()会错误编码字面量参数。另请参见2022年3月16日的安全警报。\n[S] 编译器错误SOL-2021-4  使用短于32字节的自定义值类型的被测代码不得使用Solidity编译器版本0.8.8。\nSolidity编译器版本0.8.8有一个错误，为不需要的自定义类型分配了完整的32字节存储空间。这可能被滥用来读取任意存储，如果被测代码包含使用不同Solidity编译器版本编译的代码，也可能导致错误。另请参见2021年9月29日的安全警报。\n[S] 编译器错误SOL-2021-2  使用abi.decode()对内存字节数组进行解码的被测代码  不得在Solidity编译器版本0.4.16至0.8.3（含）中使用ABIEncoderV2。\nSolidity编译器版本0.4.16引入了一个错误，在0.8.4中修复，意味着ABIEncoderV2在读取内存字节数组时错误验证了指针，这可能由于指针计算中的溢出错误而导致读取超出数组区域的数据。另请参见2021年4月21日的安全警报。\n[S] 编译器错误SOL-2021-1  具有两个或多个指令  keccak(mem,length)的被测代码，其中  - mem的值相等，且  - length的值不等，且  - length的值不是32的倍数，  不得在早于0.8.3的Solidity编译器版本中使用优化器。\nSolidity编译器版本在0.8.3之前有一个优化器错误，意味着keccak哈希，针对相同内容但不同长度（不是32字节的倍数）计算，错误地使用了缓存中的第一个值而不是重新计算。另请参见2021年3月23日的安全警报。\n[S] 使用现代编译器  被测代码不得使用早于0.8.0的Solidity编译器版本，  除非它满足EEA EthTrust安全等级规范版本2中的所有以下要求，  作为覆盖要求：[S] 无溢出/下溢  [S] 编译器错误SOL-2020-11-push  [S] 编译器错误SOL-2020-10  [S] 编译器错误SOL-2020-9  [S] 编译器错误SOL-2020-8  [S] 编译器错误SOL-2020-6  [S] 编译器错误SOL-2020-7  [S] 编译器错误SOL-2020-5  [S] 编译器错误SOL-2020-4并且  被测代码不得使用早于0.6.0的Solidity编译器版本，  除非它满足EEA EthTrust安全等级规范版本1中的所有以下要求，  作为覆盖要求：[S] 编译器错误SOL-2020-11-length  [S] 编译器错误SOL-2019-10  [S] 编译器错误SOL-2019-3,6,7,9  [S] 编译器错误SOL-2019-8  [S] 编译器错误SOL-2019-5[S] 编译器错误SOL-2019-4[S] 编译器错误SOL-2019-2[S] 编译器错误SOL-2019-1[S] 显式存储（包括通过其覆盖要求[M] 如果适用则显式声明存储）[S] 编译器错误SOL-2018-4[S] 编译器错误SOL-2018-3[S] 编译器错误SOL-2018-2[S] 编译器错误SOL-2018-1[S] 编译器错误SOL-2017-5[S] 编译器错误SOL-2017-4[S] 编译器错误SOL-2017-3[S] 编译器错误SOL-2017-2[S] 编译器错误SOL-2017-1[S] 编译器错误SOL-2016-11[S] 编译器错误SOL-2016-10[S] 编译器错误SOL-2016-9[S] 编译器错误SOL-2016-8[S] 编译器错误SOL-2016-7[S] 编译器错误SOL-2016-6[S] 编译器错误SOL-2016-5[S] 编译器错误SOL-2016-4[S] 编译器错误SOL-2016-3\n有许多已知的编译器错误影响早于0.6.0的Solidity编译器版本，但对编译器错误的研究往往集中在影响相对现代Solidity编译器版本的错误上，因此旧Solidity编译器版本中的任何进一步错误很可能只有在被利用后才会被发现并广为人知。使用现代Solidity编译器版本是一个良好的实践。在极少数情况下无法使用晚于0.6.0的Solidity编译器版本时，可以通过符合本规范版本1中定义的相关覆盖要求来实现EEA EthTrust认证。另请参见相关要求[M] 使用现代编译器，涵盖需要安全等级[M]审查的Solidity编译器错误。\n[S] 无古老编译器  被测代码不得使用早于0.3的Solidity编译器版本。\n未跟踪早于0.3的Solidity编译器版本的编译器错误。因此存在未知错误可能导致意外问题的风险。另请参见solidity-bugs-json中的”SOL-2016-1”。\n5.2 安全等级[M]获得安全等级[M]的EEA EthTrust认证意味着合约经过人类审计或团队的手动分析，并且重要的安全问题均已解决至审计者满意为止。这一级安全要求是对等级[S]的补充，通常用于：\n\n合约使用了不常见或高风险特性；\n出现静态分析难以确认的安全性；\n审计者需通过逻辑判断确定安全性。\n\n为满足安全等级[M]，测试代码必须首先满足§5.1的所有等级[S]要求，除非补充满足相应的&quot;Overriding Requirement&quot;来替代部分等级[S]要求\n[M]显式消除求值顺序歧义被测代码​​不得​​包含因变量求值顺序不同而导致结果差异的语句。\nSolidity 中函数的求值顺序并非完全确定，且不同编译器版本间无法保证一致性。若语句调用的多个函数对共享状态对象产生副作用，求值顺序的差异可能导致不同结果。此外，事件日志及addmod/modmul指令的求值顺序通常不符合常规模式，使用这些功能的被测代码可能产生预期外的结果。\n\n警告\n\n​​示例13：不确定的求值顺序​​\n当 g() 和 h() 修改了 f() 依赖的状态变量时，该调用无法保证可复现的结果。\nf(g(x), h(y));  // 无法保证结果一致性\n\n\n✅ 解决方案\n通过临时变量显式控制执行顺序：\n​示例 14：使用临时变量强制求值顺序​​// SPDX-License-Identifier: MITpragma solidity 0.8.18;uint256 public myNumber;uint256 public yourNumber;function firstTransform(uint256 someNumber) public returns (uint256) &#123;    myNumber += 1;                // 副作用：修改状态    return someNumber * myNumber; // 依赖更新后的 myNumber&#125;function secondTransform(uint256 someNumber) public returns (uint256) &#123;    yourNumber += 3;              // 副作用：修改状态    return someNumber / yourNumber; &#125;function deterministicResult(uint256 someNumber) public returns (uint256) &#123;    uint256 firstResult = firstTransform(someNumber); // 显式优先执行    return secondTransform(firstResult);              // 显式后续执行&#125;\n\n​​关键点​​： 使用firstResult临时变量分离执行步骤, 确保firstTransform在secondTransform前完成消除多函数调用的状态依赖歧义.关联参考: \n\nSolidity隐蔽漏洞模式[solidity-underhanded-richards2022](https://entethalliance.org/specs/ethtrust-sl/v3/#bib- solidity-underhanded-richards2022)\nSolidity速查指南solidity-cheatsheet\nSolidity编译器安全漏洞SOL-2023-2\n\n[M]验证精确余额检查被测代码若检查账户余额​​严格等于​​（==）指定值或变量时，​​必须​​防范转账操作对余额检查的影响。此为 [S] 级要求禁用精确余额检查的覆盖要求\n若智能合约在执行过程中检查账户余额是否等于​​特定精确值​​，则存在潜在风险：攻击者可通过向该账户转账来改变其余额，导致交易意外回滚等非预期结果。若必须使用此类检查，​​必须​​实施防护措施以抵御此类攻击可能性。\n5.2.1 文本与同形字攻击本节要求与安全公告CVE-2021-42574及CWE-94“代码生成控制不当”（亦称”代码注入”）相关。\n[M]禁用非必要Unicode控制符被测代码​​不得​​使用Unicode方向控制字符，除非对文本正确渲染确有必要, 并且渲染结果不会误导读者（此为 [S]级要求禁用Unicode方向控制字符的覆盖要求）\n​​安全等级[M]允许​​在文本字符串中使用Unicode方向控制字符，但需经必要性分析。\n[M] 禁止同形字式攻击被测代码​​不得​​使用同形字、Unicode控制符、组合字符或多Unicode区块字符，若其效果具有误导性。\n通过替换不同字母表中视觉相似的字符（如拉丁字母 “a” 与西里尔字母 “а”）、使用方向控制符或组合字符，可构造恶意代码欺骗审计人员。例如：使用 “í” 冒充 “i” 或 “ì”阿拉伯语 “ت” 冒充 “ث”数字 “1” 冒充字母 “l”数学符号 “𝚒” 冒充拉丁字母 “i”此类攻击称为​​同形字攻击​​，具体手法详见Ivanov。若变量名&#x2F;标签中​​确需混用多Unicode区块字符​​（如双语混合命名），只要不产生误导或混淆，仍可通过EEA EthTrust认证。审计人员判定存在​​非必要误导或混淆​​时，视为不符合要求。​​关联要求​​：[S]禁用Unicode方向控制字符\n5.2.2 外部调用防护[M] 必须保护所有外部调用[M] Protect External Calls如果测试代码中有外部调用（external call），则必须确保：- 被调用地址对应的是同一 &quot;Tested Code&quot; 集合中的确切合约代码；- 所有被调用合约都属于 Tested Code；- 被调用合约由同一实体控制；- 对于这些调用，必须提供与 Checks‑Effects‑Interactions 模式等效的重入攻击防御机制。- 否则必须满足以下 等级 [Q] 级别的 Overriding 条件：- [Q] Verify External Calls- [Q] Document Contract Logic- [Q] Document System Architecture- [Q] Implement as Documented这些额外条件允许部分外部调用通过场景理解和文档审计替代自动模式检测\n安全级别[M]的EEA EthTrust认证允许在构成测试代码一部分的一组合约内进行调用。这可确保在此安全级别一起审核所有调用的合约。\n如果合同调用了未作为测试代码的一部分进行审计的已知外部合约，则可以通过覆盖要求来证明符合此要求，这允许验证者根据自己的判断声称所调用的合同提供了适当的安全性。在这种情况下，通过实施覆盖性要求来声明符合性时适用的围绕测试代码文档的扩展要求反映了如果审查者简单地假设外部合同是安全的，因为它们已被广泛使用，则可能会带来非常高的风险。\n除非测试代码自己部署合约，并准确检索其地址以进行调用，否则有必要检查合约是否真的部署在测试代码中假设的地址。\n必须为测试代码提供与安全级别[S]使用[c-e-i]相同级别的针对重入攻击的保护级别。如果使用重入防护的合约不遵循 [c-e-i]，它们仍然容易受到攻击。跨函数重入攻击已经成功，其中共享相同状态的单独函数在外部调用后导致状态更改。\n[M]防御只读重入攻击被测试代码必须可以预防只读重入攻击\n如§3.4外部交互和重入攻击中所述，从函数读取信息的代码最终可能会读取不一致或不正确的信息。当测试代码调用出现这种可能性的函数时，调用代码需要适当的机制来避免这种情况发生。测试代码如果调用其他合约的view函数，必须确保调用不会读取到中间不一致状态。这通常需要使用锁机制或状态检查modifier，防止读取过程中被外部重入造成不一致。\n\n警告\n\n示例15：不安全方法：回放一个可以重入view函数的值\n这是一个简单的只读重入攻击案例。\n合约 Reentered 有一个视图函数，该函数根据特定 LPToken 中的 totalSupply 和 numberOfEther 确定价格，以及一个公共非重入函数，该函数出售 LP 代币并将收到的 ETH 发送给调用它的用户。\n当攻击者合约中调用攻击函数时，它会减少重新输入合约中 numberOfEther 的值，并在更新 totalSupply 之前触发攻击者合约的 receive() 函数，方法是阻止指令 LPToken.burnFrom（msg.sender，amount）执行。\n攻击者合约的 receive()函数调用了被攻击的 Reentered 合约的 buyToken()函数。由于 numberOfEther 的值已被修改，但 totalSupply 没有修改，因此被攻击合约中的 buyToken() 函数将请求错误数量的代币作为交换。比例是攻击会收到更多的代币。\n\n// SPDX-License-Identifier: MITpragma solidity 0.8.18;contract Reentered &#123;  function getPrice() public returns (uint256) &#123;    return totalSupply / numberOfEther; // 计算价格  &#125;  function sellLPToken(uint256 amount) public nonReentrant &#123;    // ...    numberOfEther -= amountToReceive;    (bool success, ) = msg.sender.call&#123;value: amountToReceive&#125;(&quot;&quot;);    require(success, &quot;Transfer failed&quot;);    LPToken.burnFrom(msg.sender, amount);  &#125;&#125;contract Attacked &#123;  function buyToken(uint256 amount) public &#123;    uint256 tokenToReceive = Reentered.getPrice() * amount;    // ...  &#125;&#125;contract Attacker &#123;  function attack() public &#123;    Reentered.sellLPToken(LPToken.balanceOf(address(this)));    // 触发 ETH 转账，从而触发 receive()  &#125;  receive() external payable &#123;    Attacked.buyToken(1000);  &#125;&#125;\n\n\n在上述场景中，`receive()`中的`getPrice()`调用读取了还未更新的状态，导致计算错误，是典型的`view`函数重入攻击示例。\n\n\n5.2.3 有防御性的代码使用说明[M]记录特殊代码使用并说明防护措施所有包含下面内容的实例都必须在合约文档中说明其使用理由，并阐明相应的安全防护或设计依据，且文档须对合约调用者可见：- CREATE2- assembly &#123; … &#125;- selfdestruct() 或 suicide()- 外部调用- delegatecall()- 有可能发生溢出/下溢的操作- block.number 或 block.timestamp- 使用预言机或伪随机性这是对以下多个 [S] 需求的 Overriding Requirement：- [S] No CREATE2- [S] No selfdestruct()- [S] No assembly &#123;&#125;- [S] Use Check-Effects-Interaction- [S] No delegatecall()\n\n所有这些编码模式都有合法用途，但它们也是安全漏洞的潜在原因。因此，安全级别 [M] 需要测试这些模式的使用是否得到解释和合理性，并且它们的使用方式不会引入已知漏洞。\n记录外部调用使用的要求适用于测试代码中的所有外部调用，无论它们是否满足相关要求[S]使用Check-Effects-Interaction。\n另请参阅相关要求：[Q] 文档契约逻辑、[Q] 文档系统架构、[Q] 按文档实现，[Q] 验证外部调用，[M] 避免常见汇编 {} 攻击向量，[M] 汇编 {} 中的编译器错误 SOL-2022-5，[M] 编译器错误 SOL-2022-4，[M] 编译器错误 SOL-2021-3，如果使用 Solidity 编译器版本 0.5.5 或 0.5.6，[M] [EthTrust-sl-v1] 中汇编 {} 中的编译器错误 SOL-2019-2。\n[M]确保数值计算的舍入不会被滥用若合约存在依赖舍入操作的数学逻辑，比如四舍五入、整除等：- 必须文档化可能产生的误差范围；- 不可因舍入造成价值意外增加或丢失；- 舍入逻辑设计应防止攻击者通过来回交换谓&quot;round‑trip&quot;不断获利（如累计额外份额）。\n\n智能合约通常使用整数算术实现实数上的数学公式。此类代码可能会引入舍入误差，因为大小有界的整数和有理数无法精确表示同一范围内的所有实数。如果使用舍入的过程导致可预测的错误量，从而增加往返产生的值，则可以通过重复该过程来累积虹吸大笔金额来利用该差异。\n\n警告\n\n示例Example 16: 舍入不安全写法\n四舍五入到最接近的可用数字的简单交换可能意味着往返实际上会产生一个 off-by-one 错误，因此来回交换正确数量的代币将在每笔交易中产生比开始时更多的价值\n// SPDX-License-Identifier: MITpragma solidity 0.8.18;function xChangeTo(uint256 numberOfEth) public returns (uint256) &#123;  return numberOfEth.mul(rateThatCausesRounding); // 四舍五入&#125;function xChangeFrom(uint256 numberOfOtherToken) public returns (uint256) &#123;  return numberOfOtherToken.div(rateThatCausesRounding); // 四舍五入&#125;\n\n\n为了防止此漏洞，“保留更改(Keep in Change)”方法确保产生的任何差异都不会为重复调用智能合约的攻击者提供优势。重要的是要注意，差异仍然会产生。合约可以使用“过度服务”，反复调用受“保留零钱”方法保护的交换，从用户那里窃取。\n// SPDX-License-Identifier: MITpragma solidity 0.8.18;function xChangeTo(uint256 numberOfEth) public returns (uint256) &#123;  return numberOfEth.mulDown(rateThatCausesRounding); // 向下舍入&#125;function xChangeFrom(uint256 numberOfOtherToken) public returns (uint256) &#123;  return numberOfOtherToken.divDown(rateThatCausesRounding); // 向下舍入&#125;\n该方法确保每次交易不会给攻击者带来优势，尽管仍可能累积舍入损失. 这个漏洞在实践中已经在DeFi协议智能合约中被发现，可能会使数亿美元面临风险。DevCon 2023 演讲[DevCon 四舍五入](https://archive.devcon.org/resources/6/tackling-rounding-errors-with-precision-analysis.pdf)的演示幻灯片中提供了进一步的解释。[舍入误差](https://entethalliance.org/specs/ethtrust-sl/v3/#bib-rounding-errors)中提供了自动做市商整数舍入的全面数学分析示例。\n\n此要求基于 CWE-1339精度不足或实数准确度不足。\n[M]保护合约自毁操作如合约中使用 selfdestruct() 或其旧别名 suicide()：- 必须保证只有授权方才能调用；- 必须采用与该操作与作者声明一致的保护逻辑。除非补充具备 [Q] Enforce Least Privilege 合规文档说明，否则不允许直接使用该操作。该项是对 &quot;[S] No selfdestruct()&quot; 的 Overriding Requirement\n如果selfdestruct()指令（或其已弃用的替代suicide()）没有得到仔细保护，恶意代码可以调用它并破坏合约，并可能窃取合约持有的任何以太币。此外，这可能会扰乱合约的其他用户，因为一旦合约被销毁，发送的任何以太币都会丢失，这与合约被禁用时不同，这会导致发送以太币的交易恢复。见SWC-106\n\n示例（Example 18）：\n这是 Parity 钱包多重签失败导致以太资金被冻结的案例，因合约中selfdestruct()被错误保护触发所引致。\n\n\n[M] 避免常见assembly&#123;&#125; 攻击向量测试的代码不得使用汇编assembly&#123;&#125; 指令来更改变量，除非代码不能：- 创建存储指针冲突，也不会- 允许将任意值分配给函数类型的变量。这是 [S] No assembly &#123;&#125; 的一组覆盖要求的一部分。\nassembly&#123;&#125;指令为开发人员提供了一种在智能合约中生成代码的低级方法。使用这种方法提供了极大的灵活性和控制力，例如降低gas成本。但是，它也暴露了一些可能的攻击面，恶意编码人员可能会在其中引入难以检测的攻击。此要求可确保不会公开两个众所周知的此类攻击面。\n另请参阅 SWC-124 和 SWC-127 [swcregistry]，以及相关要求 [M] 文档特殊代码使用、[M] 编译器错误 SOL-2022-7、[M] 编译器错误 SOL-2022-5 在程序集中 {}，[M] 编译器错误 SOL-2022-4、[M] 编译器错误 SOL-2021-3，如果使用 Solidity 编译器版本 0.5.5 或 0.5.6，则 [M] [EthTrust-sl-v1] 中的程序集 {} 中的编译器错误 SOL-2019-2。\n[M]保护使用 CREATE2 的调用任何使用 CREATE2 部署合约时，需满足：- 被部署合约必须包含在 Tested Code 内；- 不得使用 selfdestruct(), delegatecall() 或 callcode()；- 必须与合约作者的声明逻辑一致。否则必须满足- [Q] Verify External Calls, - [Q] Document Contract Logic, - [Q] Document System Architecture, - [Q] Implement as Documented 这四项 Overriding Requirement\n\nCREATE2能够与链上代码尚不存在的地址进行交互，因此防止外部调用尚未知的恶意或不安全的合约代码非常重要。\n测试代码需要包含可以使用CREATE2部署的任何代码，以验证保护是否到位以及代码的行为是否符合协定作者声明。这包括确保不存在可以改变不变性或转发调用的作码，例如使用CREATE2部署的合约，例如selfdestruct()、delegatecall()和callcode()。\n如果存在这些操作码中的任何一个，则需要覆盖要求所需的额外保护和文档。\n[M] 安全溢流/下溢测试的代码不得包含可以溢出或下溢的计算，除非- 有明显的需求（例如，用于模运算）和- 如有必要，任何计算都有保护措施，以确保行为与合同作者的主张一致。\n在少数情况下，算术溢出或下溢是预期行为。对此类案件进行适当保护非常重要。请注意，Solidity 编译器版本0.8.0引入了导致事务恢复的溢出保护。见SWC-101\n[M]伪随机性来源测试代码中使用的随机性来源必须具有足够的抵抗力，无法预测其目的得到满足。\n这一要求涉及对每个具体合同和案例的仔细评估。随机性的某些用途依赖于没有比任何其他预测更准确的预测。对于这种情况，可以准确猜测或由矿工或验证者控制的值，例如区块难度、时间戳和&#x2F;或区块编号，会引入漏洞。因此，需要像预言机服务这样的“强”随机性源。\n其他用途对“好的猜测”有抵抗力，因为使用接近但错误的东西不会比任何其他猜测提供更大的获得优势的可能性。\n\n警告\n\n示例 19：不要这样做：随机性容易受到近似猜测的影响\n一种在特定时间猜测链的区块数的竞赛，奖励最接近正确答案的答案，使用的是“随机性”来源，该源很容易受到近似猜测的影响。\n\n\n\n\n示例 20：抗近似的随机性\n只有当提交的号码与未来将要举行的链下彩票中的中奖条目完全匹配时才会支付彩票，在能够近似答案方面没有优势。\n\n另见[S] No Exact Balance Check, [M] Don't Misuse Block Data, and [Q] Protect against MEV Attacks.\n\n[M] 不要滥用块数据测试代码中使用的块号和时间戳不得给 MEV 或类似攻击带来漏洞。\n\n区块数很容易受到近似预测的影响，尽管它们通常不是可靠、精确的经过时间指标。block.timestamp会受到恶意行为者的操纵。因此，重要的是，测试代码不信任这些数据，因为它们是高度可靠或随机的信息。\nswcregistry 中对 SWC-116 的描述包括一些要避免的技术的代码示例，例如使用block.number / 14 作为经过的秒数的代理，或依靠 block.timestamp 来指示精确的时间已经过去。\n对于概率低精度的使用，例如“大约 1&#x2F;2 小时已经过去了”，像 （block.number / 14 &gt; 1800） 这样的表达式在主网上或具有类似规则区块周期约为 14 秒的区块链上可以足够稳健。但是使用这种方法来确定例如“恰好 36 秒”已经过去了，则无法满足要求。\n\n警告\n如果依赖特定区块周期的合约部署在区块频率截然不同的区块链上，可能会带来严重的风险。\n\n同样，由于 `block.timestamp` 依赖于可被恶意节点运营商操纵的设置，因此在以太坊主网等情况下，它适合用作粗粒度近似值（以分钟为单位），但不同区块链上的相同代码可能容易受到 MEV 攻击。\n\n请注意，这与预言机的使用有关，预言机也可能提供不准确的信息。\n另请参阅相关要求 [S] 无精确余额检查、[M] 随机性来源和 [Q] 防止 MEV 攻击。\n5.2.4 签名管理[M] 正确的签名验证测试代码必须正确验证签名，以确保链下签名的消息的真实性。\n\n一些智能合约处理链下签名的消息，以提高灵活性，同时保持真实性。执行自己的签名验证的智能合约需要验证此类消息的真实性。\n使用 ecrecover() 进行签名验证，根据预期结果验证返回的地址非常重要。特别是，返回值address(0) 表示未能提供有效签名。\n另见 SWC-122 swcregistry。\n对于使用 ecrecover()和早于 0.4.14 的Solidity编译器版本的代码，请参阅相关要求 [M] 使用现代编译器，特别是 [M] 验证 [EthTrust-sl-v1] 中的 ecrecover() 输入\n[M] 不当使用签名进行重放攻击保护使用签名来防止重放攻击的测试代码必须确保签名不能重复使用：- 在同一函数中验证相同的消息，也不会- 在多个函数中验证测试代码中的相同消息，也- 在多个合约地址中验证同一消息，其中同一帐户可能正在签署消息，也- 在跨多个链的同一个合约地址中，除非它满足优先要求 [Q] 预期重播。此外，测试代码必须验证不能为同一消息创建多个签名，就像可延展签名的情况一样。\n在重放攻击中，攻击者重放正确签名的消息以利用系统。签名消息需要包含足够的标识信息，以便明确定义其预期设置。\n可延展签名允许攻击者为同一邮件创建新签名。如果使用可延展的签名，检查签名哈希值以确保消息仅被处理过一次的智能合约可能容易受到重放攻击。\n5.2.5 安全级别 [M] 编译器错误和覆盖要求§5.1.3编译器错误中描述的一些solidity编译器错误在安全级别[M]上具有覆盖要求，并且有些具有在软件中不易检测到的触发条件。\n\n注意\n实施推荐的良好实践 [GP] 使用最新编译器意味着测试代码通过了本小节中的所有要求。\n\n[M] Solidity 编译器错误 2023-1包含使用 .selector 的具有副作用的复合表达式的测试代码必须将 viaIR 选项与 0.6.2 到 0.8.20 之间的 Solidity 编译器版本一起使用。\nSolidity 编译器版本 0.6.2 中引入并在 Solidity 编译器版本 0.8.21 中修复的一个bug, 这个bug是当复合表达式访问`.selector`成员时，除非使用 viaIR 管道，否则不会计算表达式。因此，不会发生由该表达引起的任何副作用。\n\n另请参阅 2023 年 7 月 19 日的安全警报。\n[M] 编译器错误 SOL-2022-7​​经测试的代码中，若存在存储写入操作后跟随条件性提前终止（该终止由包含return()或stop()指令的内联汇编函数触发），则绝对禁止使用 Solidity 编译器版本 0.8.13 至 0.8.16（含）进行编译。​这是 [S] No assembly &#123;&#125; 的覆盖要求集的一部分。\nSolidity 编译器版本 0.8.17 中修复的一个错误意味着在优化期间，存储写入后从内联汇编函数有条件提前终止有时会被错误地丢弃。\n另请参阅 2022 年 9 月 5 日的安全警报。\n[M] Compiler Bug SOL‑2022‑5 in assembly &#123;&#125;当从 calldata 或 memory 中拷贝字节数据长度不是 32 字节倍数，并用 assembly &#123;&#125; 读取时，Solidity 0.8.14 及以下（不含 0.8.15）版本存在超界可见漏洞，必须禁止使用老的版本。\n在 Solidity 编译器版本 0.8.15 之前，复制长度不是 32 字节倍数的内存或调用数据可能会暴露超出复制数据的数据，这可以使用assmebly &#123;&#125; 观察到。\n另请参阅 2022 年 6 月 15 日安全警报和相关要求 [S] 编译器错误 SOL-2022-5 与 .push（） 、[M] 避免常见assmebly {} 攻击向量、[M] 文档特殊代码使用、[M] 编译器错误 SOL-2022-4 和 [M] 编译器错误 SOL-2021-3。\n[M] Compiler Bug SOL‑2022‑4在不同的 assembly &#123;&#125; 片段中写入与读取时使用共享内存，但某个优化路径下丢失该内存，Solidity 0.8.13 或 0.8.14 不可使用yulOptimizer\nSolidity 编译器版本 0.8.13 引入了一个yulOptimizer错误，该错误在 Solidity 编译器版本 0.8.15 中修复，其中在assembly &#123;&#125; 指令中创建但仅在不同的assembly &#123;&#125; 指令中读取的内存被丢弃。\n另请参阅 2022 年 6 月 17 日安全警报和相关要求 [M] 避免常见assmebly {} 攻击向量、[M] 文档特殊代码使用、[M] 编译器 Bug SOL-2022-7、[M] 程序集 {} 中的编译器 Bug SOL-2022-5 和 [M] 编译器 Bug SOL-2021-3。\n[M] Compiler Bug SOL‑2021‑3在 assembly &#123;&#125; 中读取 immutable 但短于 256 位的带符号整数时，Solidity 0.6.5 到 0.8.8(含) 存在错误，不可使用\n\nSolidity 编译器版本 0.6.8 引入了一个错误，该错误在 Solidity 编译器版本 0.8.9 中修复，这意味着在内联assembly &#123;&#125; 指令中可能会错误地读取短于 256 位的不可变有符号整数类型。\n另请参阅 2021 年 9 月 29 日安全警报和相关要求 [M] 安全使用程序集 {}、[M] 文档特殊代码使用、[M] 程序集 {} 中的编译器错误 SOL-2022-5 和 [M] 编译器错误 SOL-2022-4。\n[M] Use a Modern Compiler测试的代码不得使用早于 0.8.0 的 Solidity 编译器版本，除非它满足 EEA EthTrust 安全级别规范第 2 版中的 [M] 编译器错误检查构造函数付款的要求，作为优先要求, 并且测试代码不得使用早于 0.6.0 的 Solidity 编译器版本，除非它满足 EEA EthTrust 安全级别规范第 1 版中的以下所有要求，作为覆盖要求：- [M] 编译器错误 SOL-2020-2，- [M] 汇编中的编译器错误 SOL-2019-2 &#123;&#125;，- [M] 编译器错误检查标识调用，- [M] 验证 ecrecover（） 输入，- [M] 编译器错误-无零以太发送，以及- [M] 显式声明存储。\n\n5.3 安全等级[Q]除了可自动的静态测试验证（安全级别 [S]）和手动审核（安全级别 [M]）之外，安全级别 [Q] 的 EEA EthTrust 认证还意味着检查测试代码的预期功能是否得到充分记录，可以验证其功能正确性，代码和文档是否经过人工审核员或审核团队的彻底审查，以确保它们内部连贯且相互一致， 足够仔细地识别复杂的安全漏洞。\n这种级别的审查对于使用 ERC20 ERC20、ERC721 ERC721 等的代币尤其相关;token-standards 标识了可以定义标记的许多其他标准。\n在此安全级别，还需要检查以确保代码不包含不直接影响安全性但会影响代码质量的错误。代码经常被复制，因此安全级别 [Q] 要求代码尽可能编写好。要解决的风险是，在复制现有代码作为起点后引入弱点很容易，而且并不少见。\n[Q] 通过安全级别 [M]要获得安全级别 [Q] 的 EEA EthTrust 认证，经过测试的代码必须满足 § 5.2 安全级别 [M] 的要求。\n[Q] 将 TimeLock 延迟用于敏感操作影响所有或大多数用户的敏感作必须使用 [TimeLock] 延迟。\n敏感操作，例如智能合约升级和 RBAC 更改，会影响协议中的所有或大多数用户。TimeLock 延迟允许用户在不同意提议的更改时退出系统，并允许开发人员在检测到可疑更改时做出反应。\n[Q] 代码 Linting测试的代码- 不得创建不必要的变量，并且- 不得对可能在同一范围内出现的函数、变量或其他标记使用相同的名称，并且- 不得包含在正常作中失败的 assert() 语句，并且- 不得包含执行中无法访问的代码- 明确用于管理意外错误的代码除外，例如 assert() 语句，以及- 不得包含与智能合约同名的函数，除非使用 constructor 关键字将其显式声明为构造函数，并且- 必须显式声明所有函数和变量的可见性，并且- 必须在其编译指示中指定一个或多个 Solidity 编译器版本。\n代码通常从“好示例”复制，作为开发的起点。达到安全级别 [Q] EEA EthTrust 认证的代码意味着高质量，因此确保复制它不会鼓励坏习惯非常重要。查看不包含无意义代码的测试代码也更容易。\n函数和变量具有相同名称的代码通常更难阅读。如果这些项目在范围上可以重叠，编译将消除它们的歧义，但它们通常需要审稿人进行大量工作才能在精神上分离。在多个不重叠的循环中使用 i，j 作为计数器是可以的，但是在“外部作用域”中有一个变量，其名称被其他一些变量或函数复制在“内部作用域”中会增加遵循执行模式所需的工作量。\n明确允许​​设计用于捕获意外错误的代码, 例如 assert()指令，因为若防御性编写的代码成功消除了触发特定错误的可能性，却无法获得 EEA EthTrust 认证，将非常遗憾。assert()语句​​仅适用于不变量的检查​​，而非作为通用错误处理机制。若在常规操作中因将assert()作为错误捕获机制而导致失败，​​应替换为require()语句​​或专为该用例设计的类似机制。若因编码缺陷导致失败，则​​必须修复该缺陷​​。\n对 assert()语句的要求基于 CWE-670 始终不正确的控制流实现。\n[Q] 管理Gas使用量增加必须有足够的 Gas 来处理测试代码中随时间增长的数据结构，根据 [Q] 文档契约逻辑提供的描述。\n一些结构（例如数组）可以增长，并且变量的值（根据设计）是可变的。迭代一个事先不清楚大小的结构，无论是增长的数组、变化的边界还是由外部值决定的东西，都可能导致 gas 使用量显着增加。需要在预期的业务逻辑的上下文中考虑什么是合理的增长，以及测试代码如何防止 Gas Griefing 攻击，在这些攻击中，恶意参与者或错误导致值超出预期的合理范围。\n另请参阅 SWC-126、SWC-128 [swcregistry] 和第 5.3.1 节文档要求中的相关要求。\n[Q] 保护Gas使用测试的代码必须防止恶意行为者窃取或浪费gas。\n允许“无 Gas”交易的智能合约使用户无需提供自己的 Gas 即可提交交易。需要仔细实施它们，以防止 Gas Griefing 和 Gas Siphoning 攻击的拒绝服务。\n另请参阅 The Gas Siphon Attack： How it Happened and How to Protect Yourself from the DevCon 2019 talk DevCon-siphoning。\n[Q] 防止 Oracle 预言机故障经过测试的代码必须保护自己免受其所依赖的 Oracle 中的故障。\n众所周知，一些预言机很容易受到操纵，例如，因为它们提供的信息来自易受只读重入攻击的信息，或者通过使用闪电贷来纵价格以启用 MEV 攻击，以及其他众所周知的攻击。此外，作为网络软件，预言机可能会遇到从延迟问题到彻底失败或停产等问题。重要的是要检查预言机用于生成其提供的信息的机制，以及依赖该预言机的测试代码是否可能受到其失败的影响，或者恶意行为者纵其输入或代码以启用攻击的影响。另请参阅相关要求 [Q] 防范排序攻击和 [Q] 防范 MEV 攻击。\n[Q] 防止排序攻击测试的代码必须以防止排序攻击的方式管理信息。\n在排序攻击中，攻击者将其交易置于与受害者交易相比的有利位置。这可以由恶意区块生产者或监控内存池的攻击者完成，并通过广播他们自己的交易并以更高的交易费用抢占易受攻击的交易。取消激励措施是通常缓解措施，通过应用哈希承诺方案 hash-commit 或批量执行等缓解措施。另请参阅相关要求 [Q] 防范 MEV 攻击。\n[Q] 防范 MEV 攻击易受 MEV 攻击的测试代码必须遵循适当的设计模式来降低这种风险。\nMEV是指区块生产者可以恶意重新排序或压制交易，或者区块链中的另一个参与者可以提议交易或采取其他行动来获得他们本不应该获得的利益的可能性。这一要求需要审计师仔细判断测试代码如何容易受到MEV攻击，以及哪些缓解策略是合适的。一些方法在§ 3.7 MEV（恶意提取值）中进一步讨论。需要考虑许多攻击类型，包括排序攻击。另请参阅相关要求 [S] 无精确余额检查、[M] 随机性来源、[M] 不滥用区块数据、[Q] 防止 Oracle 故障和 [Q] 防止排序攻击。\n[Q] 防止治理接管包含治理系统的测试代码必须防止对治理设计的恶意利用。\n恶意利用很难准确定义，因为它取决于系统的既定目标 - 例如 [Q] 文档契约逻辑中描述的目标。从广义上讲，此要求是检查测试代码是否能够抵御恶意实体为获得治理控制权而做出的努力，这些控制权预计将归属于指定的受信任实体，或者分散和广泛分布，以提供治理决策和所采取的行动得到广泛支持的安全感。治理攻击特定于被利用的系统。根据治理提案系统的不同，某些漏洞领域可能包括：\n\n发行的治理代币;\n治理代币的分配方法;\n治理提案的接受和执行的设计。例如，如果质押合约用于分配治理代币作为奖励，那么质押合约不易受到闪电贷攻击的攻击非常重要，在闪电贷攻击中，大量代币在非常短期的闪电贷中借入，并以原子方式质押以获得临时大多数治理代币，然后用于做出治理决策， 例如耗尽被攻击者钱包中持有的所有资金。\n\n另请参阅相关要求 [Q] 防止排序攻击。\n[Q] 处理所有输入测试的代码必须验证输入，并且无论输入符合设计还是格式错误，都能正常运行。\n未能验证输入的代码可能会被恶意制作的输入，从而触发错误或作者没有预料到的行为。另请参见 SWC-123 swcregistry，其中指出重要的是要考虑输入要求是否过于严格，以及是否过于宽松，CWE-573 调用者不正确遵循规范，并注意 §5.1.3 编译器错误中有一些特定于特定Solidity 编译器版本的相关要求。\n[Q] 状态更改触发事件测试的代码必须为导致状态更改的所有事务发出合约事件。\n事件是方便的接口，可在 EVM 的日志记录功能之上提供抽象。应用程序可以通过以太坊客户端的 RPC 接口订阅和监听这些事件。更多信息请访问 solidity-events。\n事件通常被期望用于记录所有状态变化，因为它们不仅对链下应用程序有用，而且对安全监控和调试也很有用。记录合约中的所有状态更改可确保与合约交互的任何开发人员都能了解作为ABI一部分的每个状态更改，并可以通过事件注释了解预期行为，如 Q 使用NatSpec注释代码。\n[Q] 没有私人数据测试代码不得将私有数据存储在区块链上。\n这是安全级别 [Q] 要求，主要是因为什么是私人数据的问题通常需要仔细和深思熟虑的评估以及对上下文的合理理解。一般来说，这可能包括对如何收集数据的评估，以及数据提供者被告知哪些信息的使用情况。\n本规范中的私有数据用于指代不打算向公众普遍提供的信息。例如，个人的家庭电话号码通常是私人数据，而企业的客户查询电话号码通常不是私人数据。同样，识别个人帐户的信息通常是私人数据，但在某些情况下它是公共数据。在这种情况下，可以按照此要求将公共数据记录在链上。\n\n警告\n请注意：在某些情况下，[GDPR] 等法规对某些私人数据施加了正式的法律要求。但是，针对此要求执行测试会导致专家技术意见，说明审计师认为私有的数据是否被泄露。关于 Tested Code 是否满足此要求的声明不代表任何形式的法律建议或意见、律师代理等。\n\n\n[Q] 预期重播如果测试代码中的签名可以重复使用，则重放实例必须是有意的、记录的并且可以安全地重复使用。\n这是 [M] 不得不当使用重放攻击保护签名的首要要求。\n在极少数情况下，测试代码的意图可能是允许重放签名。例如，签名可以用作在给定时间段内参与白名单的权限。在这些特殊情况下，重播必须作为已知限额包含在文档中。此外，必须验证重用不会被恶意利用。\n5.3.1 文档要求安全级别 [Q] 一致性需要详细描述测试代码的行为方式。除了详细的测试要求以检查它是否在特定已知漏洞方面确实按照描述的行为外，重要的是对其提出的声明是准确的。此要求有助于确保测试代码满足审计特定文档之外的声明。\n这些要求的结合有助于确保测试代码中没有隐藏恶意代码，例如恶意“后门”或“定时炸弹”。由于存在充当“定时炸弹”、“电话回家”等行为的代码的合法用例，因此这种组合有助于确保测试专注于实际问题。\n本节中的要求扩展了满足安全级别 [M] 要求 [M] 文档特殊代码使用所需的覆盖范围。与该要求一样，此级别有多个要求需要本小节中规定的文件。\n[Q] 合同逻辑文档测试代码功能旨在实现的业务逻辑规范必须可供任何可以调用测试代码的人使用。\n以人类可读的格式记录的契约逻辑，并具有足够的细节，审计员可以验证特殊代码使用的功能正确性和安全性假设，帮助他们更高效、更有信心地评估复杂的代码。\n重要的是要记录逻辑如何防范潜在的攻击，例如闪电贷攻击（尤其是治理或价格操纵）、MEV 和其他利用生态系统功能或代币经济学的复杂攻击。\n[Q] 系统架构文档必须提供测试代码的系统架构文档，以传达总体系统设计、特权角色、安全假设和预期用途。\n系统文档提供审计员信息，以了解安全假设并确保功能正确性。如果系统文档包含在代码存储库的自述文件中或引用，以及有关如何测试、构建和部署源代码的文档，这将很有帮助。\n随着时间的推移，变量和更复杂的数据结构的管理是本文档的重要组成部分。这一要求的这一方面可能会通过满足相关要求 [Q] 管理gas使用增加。另请参阅相关要求 [Q] 使用 NatSpec 注释代码。\n[Q] 威胁模型文档必须提供测试代码的记录威胁模型，描述每个威胁、安全假设、预期响应和预期结果。\n威胁模型是一种工具，可帮助为可能单独或协同引发的各种攻击做好准备，提供一组假设但可能的场景，以确保测试代码能够充分抵御这些攻击。\n一个好的威胁模型将涵盖一系列可能性，包括\n\n出现以前未知的攻击媒介\n网络操纵\n经济战\n治理开发\n预言机攻击以及其他场景以及此类场景的组合，这些场景可能会压倒测试代码对它们的防御。\n\n[Q] 使用 NatSpec 注释代码测试代码中包含的所有公共接口必须根据 [NatSpec] 格式使用内联注释进行批注，这些注释解释每个函数、参数、事件和返回变量背后的意图，以及安全使用的开发人员说明。\n内联注释对于确保开发人员和审计员了解每个函数和其他代码组件背后的意图非常重要。公共接口是指编译的测试代码的ABI中包含的任何内容。还建议对实现敏感和&#x2F;或复杂逻辑的私有或内部函数使用内联注释。\n遵循 [NatSpec] 格式允许 Solidity 编译器理解这些内联注释，以便将它们提取为机器可读格式，其他第三方工具可用于安全评估和自动文档，包括与 Sourcify 等源代码验证工具集成的钱包向用户显示的文档。这也可用于生成完全或部分满足 [Q] 合约逻辑文档。\n[Q] 按文档实施测试代码的行为必须按照 [Q] 合约逻辑文档和 [Q] 系统架构文档提供的文档中所述进行。\n\n安全级别 [Q] 提供文档的要求很重要。然而，测试代码的实际行为与记录的一样也很重要。如果没有，则可能反映出不够谨慎，并且由于实施中遗漏的错误，代码也容易受到攻击。也有可能，差异是试图在测试代码中隐藏恶意代码。\n5.3.2 访问控制[Q] 强制执行最低权限启用特权访问的经过测试的代码必须根据为 [Q] 合约逻辑文档，实现适当的访问控制机制，为这些交互提供所需的最低权限。这是 [M] 保护自毁的覆盖性要求。\n\n有几种常见的方法来实现访问控制，例如基于角色的访问控制 RBAC 和 Ownable，并且通常针对给定用例实施定制访问控制。使用行业标准方法可以帮助简化审计过程，但不足以确定不存在因实施错误或恶意制作的合同而产生的风险。\n在协议运维和部署级别考虑访问控制非常重要。如果一个协议是以确定性的方式部署的，例如允许多链部署在所有链上具有相同的地址，那么显式设置所有者而不是默认为msg.sender 很重要，因为这可能会留下一个简单的工厂部署契约作为协议的不足的新管理员。\n正如 SWC-105 中所述，适当的访问控制适用于支付尤为重要，但其他作（例如 SWC-124 中描述的数据覆盖或更改特定的访问控制）也需要得到适当的保护 [swcregistry]。此要求与 CWE-284 不正确的访问控制匹配。\n另请参阅 solidity-patterns 中的“访问限制”。\n[Q] 使用可撤销和可转让的访问控制权限如果测试代码将访问控制用于特权操作，则它必须实现撤销和转移这些权限的机制。\n特权帐户可以对合同集执行管理任务。如果这些帐户被泄露或执行这些任务的责任分配给不同的人，那么拥有撤销和转移这些权限的机制非常重要。\n[Q] 没有特权操作的单个管理员 EOA如果测试代码将访问控制用于特权操作，则它必须确保所有关键管理任务都需要执行多个签名，除非存在具有更高权限的管理员（multisg），并且可以在 EOA 受到损害或恶意操作的情况下撤销权限并撤销权限。\n特权帐户可以对合同集执行管理任务。如果单个 EOA 可以执行这些操作，并且该权限无法撤销，那么私钥泄露或丢失对智能合约构成的风险可能是存在的。\n[Q] 验证外部调用包含外部调用的测试代码必须- 记录对它们的需求，并且- 以与合约作者的主张完全兼容的方式保护它们。这是 [S] 使用 check-effects-interaction 和 [m] 保护外部调用的一组覆盖性要求的一部分。\n在安全级别 [Q] 审计师可以非常灵活地为外部调用的不同用途提供 EEA EthTrust 认证。此要求实际上允许审阅者声明外部调用的目的地不存在安全风险。值得注意的是，任何此类声明都非常密切地反映了审稿人的声誉。仅仅因为智能合约被广泛使用就假设它是安全的是不合适的，假设用户提供的智能合约未来会是安全的也是不可接受的——这是一个已知的载体，已被用于许多严重的安全漏洞。同样重要的是要考虑审阅者引用和声明安全的任何代码如何容易受到基于其使用外部调用的攻击。举一个常见的例子，如果其中一个合约是恶意的，或者只是易受攻击，则允许用户提供任何一对代币合约的交换合约可能会面临风险，而交换合约无法预料和防范。另请参阅相关要求 [Q] 文档契约逻辑、[Q] 文档系统架构和 [Q] 文档方式实施。\n[Q] 验证 tx.origin 使用情况对于使用 tx.origin 的测试代码，每个实例- 必须与测试代码中规定的安全性和功能目标一致，并且- 不得允许另一个合约违反针对 [Q] 文档合约逻辑或 [Q] 文档系统架构所做的关于合约功能的断言，即使该合约是由授权直接与测试代码交互的用户调用的。这是 [S] No tx.origin 的优先要求。\ntx.origin可用于启用网络钓鱼攻击，诱骗用户与合约进行交互，从而访问其账户中的所有资金。对于调用方的授权，msg.sender 是更安全的选择。\n另请参阅相关要求 [Q] 文档合约逻辑、[Q] 强制执行最低权限、Solidity 安全注意事项 [solidity-security] 中的“tx.origin”部分和 CWE 284：不正确的访问控制 CWE-284。\n[Q] 指定 Solidity 编译器版本以产生一致的输出测试代码必须在其编译指示中指定一系列 Solidity 版本，这些版本在给定相同编译选项的情况下生成相同的字节码。\n不同的编译器版本可能存在不同的安全漏洞并引入意外行为。虽然编译器升级几乎总是会改善安全特性，但无法保证将来所做的任何给定更改都是这种情况。\n因此，如果通过遵守测试代码生成的字节码发生变化，则该规范需要进行新的评估，以获得安全级别 [Q] 的 EEA EthTrust 认证。\n显式指定确切的 Solidity 版本可以防止与其他版本进行编译，从而确保开发、审计和部署之间的一致性。指定一系列 Solidity 编译器版本，以便使用相同的设置使用该范围内的任何 Soldity 编译器版本编译测试代码会产生相同的字节码，这意味着只要已知编译器没有更改需要进行新的分析，EEA Ethtrust 测试代码认证就有效。\n\n注意\n此要求的措辞允许在编译指示中指定开放式范围，例如\npragma solidity &gt;=0.8.14;\n但是，EEA EthTrust 认证需要指定其有效的特定范围的 Solidity 编译器版本，以防 Solidity 编译器的未来版本编译测试代码以生成不同的字节码。\n这是为了确保在编译器的升级不会导致生成的字节码发生任何变化的情况下，为较新的 Solidity 编译器版本重新认证测试代码几乎不需要工作。\n\n请注意，即使使用特定的编译指示，声称是相同版本、优化设置和其他构建配置参数的不同编译器实现也会影响生成的字节码。\n\n5.4 推荐的良好实践本节介绍了需要大量人工判断来评估的良好实践，其中没有明确的方法来确定是否已完成，或者糟糕的实施可能会降低而不是提高测试代码的安全性。测试和满足这些要求不会直接影响对本文档的一致性。但是请注意，满足推荐的良好实践 [GP] 满足尽可能多的要求在实践中意味着测试代码满足基于编译器错误的所有要求，包括大多数安全级别 [S] 要求。\n[GP]检查并解决新的安全漏洞检查 [solidity-bugs-json] 和其他来源，了解 2023 年 11 月 1 日之后宣布的错误并解决它们。\n该版本的规范于 2023 年底最终确定。新的漏洞会不时被发现，而且时间表是不可预测的。此版本中考虑的最新 solidity 编译器错误是 SOL-2023-3。\n检查发布太晚而无法合并到本文档当前版本中的安全警报是维护尽可能高安全性的重要技术。\n还有其他有关新安全漏洞的信息来源，从 CWE 到关注许多面向安全的组织的博客，例如那些为该规范做出贡献的组织。\n[GP]满足尽可能多的要求测试代码应在高于其认证的安全级别的安全级别上满足本规范的尽可能多的要求。\n虽然满足更高 EEA EthTrust 认证安全级别的一些要求不会改变测试代码的正式一致性级别，但每个要求都是指定的，因为满足这些要求可以防止特定的已知攻击。如果可以满足特定要求，即使不需要在被测试的安全级别上保持一致性，满足该要求将提高测试代码的安全性，因此值得这样做。\n[GP]使用最新编译器测试代码应该使用最新的可用稳定 Solidity 编译器版本。\nSolidity 编译器会定期更新以提高性能，但也专门用于修复发现的安全漏洞。§ 5.1.3 编译器错误中有许多要求，这些要求与编写本规范时已知的漏洞有关，以及为默认提供更好的安全性而进行的增强功能。一般来说，较新的 Solidity 编译器版本会提高安全性。除非有特定的已知原因不这样做，否则使用可用的最新 Solidity 编译器版本将带来更好的安全性。\n[GP]编写清晰易读的 Solidity 代码测试代码应该编写以便于理解。\n没有严格的规则来定义如何编写清晰的代码。使用足够的描述性名称、适当注释代码并使用易于理解的结构而不会导致代码变得过大非常重要，因为这也使其难以阅读和理解。\n过多的嵌套、非结构化注释、复杂的循环结构以及对变量和函数使用非常简洁的名称都是编码样式的示例，这些样式也会使代码更难理解。\n重要的是要注意，在某些情况下，开发人员可以牺牲易读性来换取其他好处，例如降低 gas 成本——这可以通过有据可查的代码在一定程度上缓解。\n同样，对于涉及多个单独智能合约的复杂代码，将源代码组织到多个文件中的方式可以帮助澄清正在发生的事情。特别是，命名源代码文件以匹配它们定义的智能合约的名称是一种常见的模式，可以简化理解。\n此良好实践在某种程度上扩展了相关要求 [Q] 代码 Linting，但关于如何满足它的判断必然比要求所确立的细节更加主观。那些寻求有关代码样式的额外指导的人可以参考 Solidity-Style-Guide。\n[GP]遵循公认的 ERC 标准当测试代码有合理能力为其用例遵守最终的 [ERC] 标准时，它应该符合最终的 [ERC] 标准。\nERC 是 EIP（以太坊改进提案）的一类，它定义了应用程序级标准和约定，包括代币标准 ERC20 和名称注册表 ERC137等智能合约标准。\n虽然遵循 ERC 标准本质上不会使 Solidity 代码安全，但它们确实使开发人员能够与通用接口集成并遵循预期行为的已知约定。如果测试代码确实声称遵循给定的 ERC，则审计师可以验证其在符合该标准方面的功能正确性。\n[GP]定义软件许可证测试代码应定义软件许可证\n软件许可证为贡献者和用户（包括审计员和白帽）如何与代码交互提供法律指导。由于部署到公共网络的字节码可以被任何人读取，因此通常的做法是对用于生成它的 Solidity 代码使用开源许可证。\n选择最能满足项目需求的 软件许可证 非常重要，并在整个测试代码和文档中清楚地链接到它，例如，在代码存储库中使用一个突出的 LICENSE 文件并从每个源文件中引用它。\n[GP]负责任地披露新漏洞本规范未解决的安全漏洞应通过 § 1.4 反馈和新漏洞中所述的负责任披露提请工作组和其他人注意。\n不时发现新的安全漏洞。它有助于修订此规范的工作，以确保工作组了解新的漏洞或有关现有已知漏洞的新知识。\nEEA已同意管理此类通知的特定电子邮件地址 - 如果发生变化，则相应地更新此规范。\n[GP]使用模糊测试模糊测试应用于探测测试代码是否存在错误。\n有效的模糊测试可能需要数天甚至数周的时间：耐心等待总比过早停止要好。\n由于模糊测试依赖于语料库，因此维护该语料库以最大限度地提高代码覆盖率非常重要，并且有助于修剪不必要或重复的输入以提高效率。\n\n示例 21：使用 `Scribble` 的模糊测试规范\n使用 `Scribble` 编写的属性的简单示例，`Scribble` 是一种规范语言，可将 `Solidity` 中的注释转换为具体断言。注释是“///”之后的注释。从正在检查的注解派生的属性确保如果对合约 `Foo` 和函数 `add` 的调用成功，则状态变量 x 必须等于调用 - `old（x）` - 添加到函数参数 y 之前的值。从本质上讲，使用此属性进行模糊测试将检查状态的存储是否正确更新。\ncontract Foo &#123;  int x;  /// #if_succeeds &#123;:msg &quot;test&quot;&#125; x == old(x) + y;  function add(int y) public &#123;      require(y &gt; 0);      x += y;  &#125;&#125;\n\n模糊测试规则和属性可能很复杂，并且取决于特定的合约、函数、变量、它们在执行前和/或之后的值，以及可能的许多其他事情。如果 `Fuzzing` 在 `Solidity` 编译器版本中发现任何漏洞，请负责任地披露。\n[GP]使用突变测试突变测试应该用于评估和改进智能合约测试套件的质量。\n突变测试是一种基于故障的测试技术，它将人为缺陷（突变）引入源代码，以发现测试覆盖率中的潜在差距。\n\n有几类突变算子与智能合约特别相关：状态变量突变： 修改状态变量声明和操作，包括：\n\n更改可见性修饰符（公共&#x2F;私有）\n更改常量值\n修改存储位置算术突变： 更改算术运算以测试数值计算：\n替换运算符 （ ， -， *， &#x2F;）\n修改边界条件\n引入溢出&#x2F;下溢条件控制流突变： 通过合约更改执行路径：\n反转条件语句\n修改循环条件\n更改函数修饰符访问控制突变： 安全关键型权限检查：\n删除所有权检查\n更改基于角色的权限\n修改身份验证逻辑将突变测试集成到 CI&#x2F;CD 管道中会很有帮助，并具有适当的性能基准（例如，最小突变分数阈值、测试超时限制）和退出标准（例如，关键路径突变覆盖率、所需的突改运算符）。\n\n[GP]使用形式验证测试代码应该经过正式验证。\n形式验证是一系列技术，可以从数学上证明智能合约的功能正确性。它已用于其他应用，例如嵌入式系统。形式验证在智能合约中有很多用途，例如测试活跃性、高级别的协议不变量以确保安全性，或者证明程序执行的更窄、更具体的属性。\n在形式验证中，创建智能合约预期或期望结果的正式（符号或数学）规范，从而能够对协议的正确性进行正式的数学证明。为此，智能合约本身通常被翻译成正式语言。\n存在多种语言和程序来创建原始验证证明，其中一些明确的目的是使临时用户和非数学家更容易访问形式验证。请参阅 EF-SL 了解一些示例。\n当由具有经验和技能的从业者正确实施时，形式化验证可以保证模糊测试和测试无法提供的保证。然而，这在实践中往往很难实现。形式验证需要大量的体力劳动和专业知识。\n与单元或集成测试、模糊测试或其他方法相比，全面的形式化验证很可能具有更高的成本和复杂性。许多智能合约的不可变性，以及在可能的情况下升级合约的复杂性，使得形式验证对协议的管理员和利益相关者具有吸引力。\n[GP]为多重签名钱包选择合适的阈值特权操作的多重签名要求应该有足够数量的签名者，并且不需要“1 of N”或所有签名。\n管理操作要求多个签名已成为许多团队的标准。如果管理不当，即使智能合约代码是安全的，它们也可能成为攻击源。\n“1 of N”设置的问题在于，它相对容易被利用。同时，“N of N”设置意味着，即使有一个签名者无法访问其帐户或不批准某个操作，也不可能获得批准。这可能会影响必要的操作，例如用另一个签名者替换一个签名者，例如确保作连续性，这可能会产生非常严重的影响。\n选择较少数量的签名来满足要求可以更快地做出响应，而较高的值则需要更强的多数支持。考虑使用“M of N”多重签名，其中 M &#x3D; （N&#x2F;2） 1，换句话说，批准所需的尽可能小的大多数签名作为起点。但是，重要的是要考虑有多少潜在签名者以及需要签名的具体情况，以确定给定情况下 M 的合理值。\nA. Additional Information 额外信息A.1 Defined TermsThe following is a list of terms defined in this Specification.\nAccess Control MutationsArithmetic MutationsBack-RunningBlack-Box FuzzingCensorship AttacksChecks-Effects-InteractionsControl Flow MutationsCorpusEconomic WarfareEEA EthTrust CertificationEVMEVM versionsExecution ContractFixed-length VariableFlash Loan AttackFormal VerificationFront-RunningFuzzingGas GriefingGas SiphoningGas TokensGovernance ExploitationGray-Box FuzzingHard ForkHash CollisionsHomoglyph AttacksInfrastructure SecurityInvariant TestingInvariantsLike ThisLogic ContractLow-level Call FunctionsMalleable SignaturesMetamorphic UpgradeMetamorphic UpgradesMEVMonitoring and ResponseMutation ScoreMutation TestingNetwork ManipulationNetwork UpgradeOperational Security MeasuresOracle AttacksOraclesOrdering AttacksOverriding RequirementsPrivate DataPrivileged AccountsProperty-Based TestingProxy ContractPublic InterfacesRe-entrancy AttacksRead-only Re-entrancy AttackRelated RequirementsSandwich AttacksSecurity Level [M]Security Level [Q]Security Level [S]Security LevelsSet Of ContractsSet of Overriding RequirementsState Variable MutationsStatic AnalysisSymbolic ExecutionTest CoverageTest-driven DevelopmentTested CodeTestnetThreat ModelTWAPUnicode Direction Control CharactersUnit TestingUpgradable ContractValid Conformance ClaimWhite-Box Fuzzing\nA.2 Summary of RequirementsThis section provides a summary of all requirements and Recommended Good Practices in this Specification.\n[S] Encode Hashes with chainidTested code MUST create hashes for transactions that incorporate chainid values following the recommendation described in [EIP-155]\n[S] No CREATE2Tested code MUST NOT contain a CREATE2 instruction.unless it meets the Set of Overriding Requirements\n[M] Protect CREATE2 Calls, and[M] Document Special Code Use,[S] No tx.originTested code MUST NOT contain a tx.origin instructionunless it meets the Overriding Requirement [Q] Verify tx.origin Usage\n[S] No Exact Balance CheckTested code MUST NOT test that the balance of an account is exactly equal to (i.e. &#x3D;&#x3D;) a specified amount or the value of a variableunless it meets the Overriding Requirement [M] Verify Exact Balance Checks.\n[S] No Hashing Consecutive Variable Length ArgumentsTested Code MUST NOT use abi.encodePacked() with consecutive variable length arguments.\n[S] No selfdestruct()Tested code MUST NOT contain the selfdestruct() instruction or its now-deprecated alias suicide()unless it meets the Set of Overriding Requirements\n[M] Protect Self-destruction, and[M] Document Special Code Use.[S] No assembly {}Tested Code MUST NOT contain the assembly {} instructionunless it meets the Set of Overriding Requirements\n[M] Avoid Common assembly {} Attack Vectors,[M] Document Special Code Use,[M] Compiler Bug SOL-2022-5 in assembly {},[M] Compiler Bug SOL-2022-7,[M] Compiler Bug SOL-2022-4,[M] Compiler Bug SOL-2021-3, andif using Solidity compiler version 0.5.5 or 0.5.6, [M] Compiler Bug SOL-2019-2 in assembly {} in [EthTrust-sl-v1].[S] No Unicode Direction Control CharactersTested code MUST NOT contain any of the Unicode Direction Control Characters U+2066, U+2067, U+2068, U+2029, U+202A, U+202B, U+202C, U+202D, or U+202Eunless it meets the Overriding Requirement [M] No Unnecessary Unicode Controls.\n[S] Check External Calls ReturnTested Code that makes external calls using the Low-level Call Functions (i.e. call(), delegatecall(), staticcall(), and send()) MUST check the returned value from each usage to determine whether the call failed,unless it meets the Overriding Requirement [M] Handle External Call Returns.\n[S] Use Check-Effects-InteractionTested code that makes external calls MUST use the Checks-Effects-Interactions pattern to protect against Re-entrancy Attacksunless it meets the Set of Overriding Requirements\n[M] Protect external calls, and[M] Document Special Code Use,or it meets the Set of Overriding Requirements\n[Q] Verify External Calls,[Q] Document Contract Logic,[Q] Document System Architecture, and[Q] Implement as Documented.[S] No delegatecall()Tested Code MUST NOT contain the delegatecall() instructionunless it meets the Set of Overriding Requirements:\n[M] Protect External Calls, and[M] Document Special Code Use,\nor it meets the Set of Overriding Requirements[Q] Verify External Calls,[Q] Document Contract Logic,[Q] Document System Architecture, and[Q] Implement as Documented.[S] Compiler Bug SOL-2023-3Tested code that includes Yul code and uses the verbatim instruction twice, in each case surrounded by identical code, MUST disable the Block Deduplicator when using a Solidity compiler version between 0.8.5 and 0.8.22 (inclusive).\n[S] Compiler Bug SOL-2022-6Tested code that ABI-encodes a tuple (including a struct, return value, or a parameter list) that includes a dynamic component with the ABIEncoderV2, and whose last element is a calldata static array of base type uint or bytes32, MUST NOT use a Solidity compiler version between 0.5.8 and 0.8.15 (inclusive).\n[S] Compiler Bug SOL-2022-5 with .push()Tested code that\ncopies bytes arrays from calldata or memory whose size is not a multiple of 32 bytes, andhas an empty .push() instruction that writes to the resulting array,MUST NOT use a Solidity compiler version older than 0.8.15.\n[S] Compiler Bug SOL-2022-3Tested code that\nuses memory and calldata pointers for the same function, andchanges the data location of a function during inheritance, andperforms an internal call at a location that is only aware of the original function signature from the base contractMUST NOT use a Solidity compiler version between 0.6.9 and 0.8.12 (inclusive).\n[S] Compiler Bug SOL-2022-2Tested code with a nested array that\npasses it to an external function, orpasses it as input to abi.encode(), oruses it in an eventMUST NOT use a Solidity compiler version between 0.6.9 and 0.8.12 (inclusive).\n[S] Compiler Bug SOL-2022-1Tested code that\nuses Number literals for a bytesNN type shorter than 32 bytes, oruses String literals for any bytesNN type,and passes such literals to abi.encodeCall() as the first parameter, MUST NOT use Solidity compiler version 0.8.11 nor 0.8.12.\n[S] Compiler Bug SOL-2021-4Tested Code that uses custom value types shorter than 32 bytes MUST NOT use Solidity compiler version 0.8.8.\n[S] Compiler Bug SOL-2021-2Tested code that uses abi.decode() on byte arrays as memory MUST NOT use the ABIEncoderV2 with a Solidity compiler version between 0.4.16 and 0.8.3 (inclusive).\n[S] Compiler Bug SOL-2021-1Tested code that has 2 or more occurrences of an instruction keccak(mem,length) where\nthe values of mem are equal, andthe values of length are unequal, andthe values of length are not multiples of 32,MUST NOT use the Optimizer with a Solidity compiler version older than 0.8.3.\n[S] Use a Modern CompilerTested code MUST NOT use a Solidity compiler version older than 0.8.0, unless it meets all the following requirements from the EEA EthTrust Security Levels Specification Version 2, as Overriding Requirements:\n[S] No Overflow&#x2F;Underflow[S] Compiler Bug SOL-2020-11-push[S] Compiler Bug SOL-2020-10[S] Compiler Bug SOL-2020-9[S] Compiler Bug SOL-2020-8[S] Compiler Bug SOL-2020-6[S] Compiler Bug SOL-2020-7[S] Compiler Bug SOL-2020-5[S] Compiler Bug SOL-2020-4AND\nTested code MUST NOT use a Solidity compiler version older than 0.6.0, unless it meets all the following requirements from the EEA EthTrust Security Levels Specification Version 1, as Overriding Requirements:\n[S] Compiler Bug SOL-2020-11-length[S] Compiler Bug SOL-2019-10[S] Compiler Bugs SOL-2019-3,6,7,9[S] Compiler Bug SOL-2019-8[S] Compiler Bug SOL-2019-5[S] Compiler Bug SOL-2019-4[S] Compiler Bug SOL-2019-2[S] Compiler Bug SOL-2019-1[S] Explicit Storage (including through its overriding requirement [M] Declare storage Explicitly if appropriate)[S] Compiler Bug SOL-2018-4[S] Compiler Bug SOL-2018-3[S] Compiler Bug SOL-2018-2[S] Compiler Bug SOL-2018-1[S] Compiler Bug SOL-2017-5[S] Compiler Bug SOL-2017-4[S] Compiler Bug SOL-2017-3[S] Compiler Bug SOL-2017-2[S] Compiler Bug SOL-2017-1[S] Compiler Bug SOL-2016-11[S] Compiler Bug SOL-2016-10[S] Compiler Bug SOL-2016-9[S] Compiler Bug SOL-2016-8[S] Compiler Bug SOL-2016-7[S] Compiler Bug SOL-2016-6[S] Compiler Bug SOL-2016-5[S] Compiler Bug SOL-2016-4[S] Compiler Bug SOL-2016-3[S] No Ancient CompilersTested code MUST NOT use a Solidity compiler version older than 0.3.\n[M] Pass Security Level [S]To be eligible for EEA EthTrust certification at Security Level [M], Tested code MUST meet the requirements for § 5.1 Security Level [S].\n[M] Explicitly Disambiguate Evaluation OrderTested code MUST NOT contain statements where variable evaluation order can result in different outcomes\n[M] Verify Exact Balance ChecksTested code that checks whether the balance of an account is exactly equal to (i.e. &#x3D;&#x3D;) a specified amount or the value of a variable. MUST protect itself against transfers affecting the balance tested.This is an Overriding Requirement for [S] No Exact Balance Check.\n[M] No Unnecessary Unicode ControlsTested code MUST NOT use Unicode direction control characters unless they are necessary to render text appropriately, and the resulting text does not mislead readers.This is an Overriding Requirement for [S] No Unicode Direction Control Characters.\n[M] No Homoglyph-style AttackTested code MUST not use homoglyphs, Unicode control characters, combining characters, or characters from multiple Unicode blocks, if the impact is misleading.\n[M] Protect External CallsFor Tested code that makes external calls:\nall addresses called by the Tested code MUST correspond to the exact code of the Set of Contracts tested, andall contracts called MUST be within the Tested code, andall contracts called MUST be controlled by the same entity, andthe protection against Re-entrancy Attacks for the Tested Code MUST be equivalent to what the Checks-Effects-Interactions pattern offers,unless it meets the Set of Overriding Requirements\n[Q] Verify External Calls,[Q] Document Contract Logic,[Q] Document System Architecture, and[Q] Implement as Documented.This is an Overriding Requirement for [S] Use Check-Effects-Interaction.\n[M] Avoid Read-only Re-entrancy AttacksTested Code that makes external calls MUST protect itself against Read-only Re-entrancy Attacks.\n[M] Handle External Call ReturnsTested Code that makes external calls MUST reasonably handle possible errors.This is an Overriding Requirement for [S] Check External Calls Return.\n[M] Document Special Code UseTested Code MUST document the need for each instance of:\nCREATE2,assembly {},selfdestruct() or its deprecated alias suicide(),external calls,delegatecall(),code that can cause an overflow or underflow,block.number or block.timestamp, orUse of oracles and pseudo-randomness,and MUST describe how the Tested Code protects against misuse or errors in these cases, and the documentation MUST be available to anyone who can call the Tested Code.\nThis is part of several Sets of Overriding Requirements, one for each of\n[S] No CREATE2,[S] No selfdestruct(),[S] No assembly {},[S] Use Check-Effects-Interaction, and[S] No delegatecall().[M] Ensure Proper Rounding of Computations Affecting ValueTested code MUST identify and protect against exploiting rounding errors:\nThe possible range of error introduced by such rounding MUST be documented.Tested code MUST NOT unintentionally create or lose value through rounding.Tested code MUST apply rounding in a way that does not allow round-trips “creating” value to repeat causing unexpectedly large transfers.[M] Protect Self-destructionTested code that contains the selfdestruct() or suicide() instructions MUST\nensure that only authorised parties can call the method, andMUST protect those calls in a way that is fully compatible with the claims of the contract author,unless it meets the Overriding Requirement [Q] Enforce Least Privilege.\nThis is an Overriding Requirement for [S] No selfdestruct().\n[M] Avoid Common assembly {} Attack VectorsTested Code MUST NOT use the assembly {} instruction to change a variable unless the code cannot:\ncreate storage pointer collisions, norallow arbitrary values to be assigned to variables of type function.This is part of a Set of Overriding Requirements for [S] No assembly {}.\n[M] Protect CREATE2 CallsFor Tested Code that uses the CREATE2 instruction, any contract to be deployed using CREATE2\nMUST be within the Tested Code, andMUST NOT use any selfdestruct(), delegatecall() nor callcode() instructions, andMUST be fully compatible with the claims of the contract author,unless it meets the Set of Overriding Requirements\n[Q] Verify External Calls,[Q] Document Contract Logic,[Q] Document System Architecture, and[Q] Implement as Documented.This is part of a Set of Overriding Requirements for [S] No CREATE2.\n[M] Safe Overflow&#x2F;UnderflowTested code MUST NOT contain calculations that can overflow or underflow unless\nthere is a demonstrated need (e.g. for use in a modulo operation) andthere are guards around any calculations, if necessary, to ensure behavior consistent with the claims of the contract author.[M] Sources of RandomnessSources of randomness used in Tested Code MUST be sufficiently resistant to prediction that their purpose is met.\n[M] Don’t Misuse Block DataBlock numbers and timestamps used in Tested Code MUST NOT introduce vulnerabilities to MEV or similar attacks.\n[M] Proper Signature VerificationTested Code MUST properly verify signatures to ensure authenticity of messages that were signed off-chain.\n[M] No Improper Usage of Signatures for Replay Attack ProtectionTested Code using signatures to prevent replay attacks MUST ensure that signatures cannot be reused:\nIn the same function to verify the same message, norIn more than one function to verify the same message within the Tested Code, norIn more than one contract address to verify the same message, in which the same account(s) may be signing messages, norIn the same contract address across multiple chains,unless it meets the Overriding Requirement [Q] Intended Replay. Additionally, Tested Code MUST verify that multiple signatures cannot be created for the same message, as is the case with Malleable Signatures.\n[M] Solidity Compiler Bug 2023-1Tested code that contains a compound expression with side effects that uses .selector MUST use the viaIR option with Solidity compiler versions between 0.6.2 and 0.8.20 inclusive.\n[M] Compiler Bug SOL-2022-7Tested code that has storage writes followed by conditional early terminations from inline assembly functions containing return() or stop() instructions MUST NOT use a Solidity compiler version between 0.8.13 and 0.8.16 inclusive.This is part of the Set of Overriding Requirements for [S] No assembly {}.\n[M] Compiler Bug SOL-2022-5 in assembly {}Tested code that\ncopies bytes arrays from calldata or memory whose size is not a multiple of 32 bytes, andhas an assembly {} instruction that reads that data without explicitly matching the length that was copied,MUST NOT use a Solidity compiler version older than 0.8.15.This is part of the Set of Overriding Requirements for [S] No assembly {}.\n[M] Compiler Bug SOL-2022-4Tested code that has at least two assembly {} instructions, such that\none writes to memory e.g. by storing a value in a variable, but does not access that memory again, andcode in a another assembly {} instruction refers to that memory,MUST NOT use the yulOptimizer with Solidity compiler versions 0.8.13 or 0.8.14.This is part of the Set of Overriding Requirements for [S] No assembly {}.\n[M] Compiler Bug SOL-2021-3Tested code that reads an immutable signed integer of a type shorter than 256 bits within an assembly {} instruction MUST NOT use a Solidity compiler version between 0.6.5 and 0.8.8 (inclusive).This is part of the Set of Overriding Requirements for [S] No assembly {}.\n[M] Use a Modern CompilerTested code MUST NOT use a Solidity compiler version older than 0.8.0, unless it meets the requirement [M] Compiler Bug Check Constructor Payment from the EEA EthTrust Security Levels Specification Version 2, as an Overriding Requirement,\nAND\nTested code MUST NOT use a Solidity compiler version older than 0.6.0, unless it meets all the following requirements from the EEA EthTrust Security Levels Specification Version 1, as Overriding Requirements:\n[M] Compiler Bug SOL-2020-2,[M] Compiler Bug SOL-2019-2 in assembly {},[M] Compiler Bug Check Identity Calls,[M] Validate ecrecover() input,[M] Compiler Bug No Zero Ether Send, and[M] Declare storage Explicitly.[Q] Pass Security Level [M]To be eligible for EEA EthTrust Certification at Security Level [Q], Tested code MUST meet the requirements for § 5.2 Security Level [M].\n[Q] Use TimeLock Delays for Sensitive OperationsSensitive operations that affect all or a majority of users MUST use [TimeLock] delays.\n[Q] Code LintingTested code\nMUST NOT create unnecessary variables, andMUST NOT use the same name for functions, variables or other tokens that can occur within the same scope, andMUST NOT include assert() statements that fail in normal operation, andMUST NOT include code that cannot be reached in executionexcept for code explicitly intended to manage unexpected errors, such as assert() statements, andMUST NOT contain a function that has the same name as the smart contract unless it is explicitly declared as a constructor using the constructor keyword, andMUST explicitly declare the visibility of all functions and variables, andMUST specify one or more Solidity compiler versions in its pragma directive.[Q] Manage Gas Use IncreasesSufficient Gas MUST be available to work with data structures in the Tested Code that grow over time, in accordance with descriptions provided for [Q] Document Contract Logic.\n[Q] Protect Gas UsageTested Code MUST protect against malicious actors stealing or wasting gas.\n[Q] Protect against Oracle FailureTested Code MUST protect itself against malfunctions in Oracles it relies on.\n[Q] Protect against Ordering AttacksTested Code MUST manage information in such a way that it protects against Ordering Attacks.\n[Q] Protect against MEV AttacksTested Code that is susceptible to MEV attacks MUST follow appropriate design patterns to mitigate this risk.\n[Q] Protect Against Governance TakeoversTested Code which includes a governance system MUST protect against malicious exploitation of the governance design.\n[Q] Process All InputsTested Code MUST validate inputs, and function correctly whether the input is as designed or malformed.\n[Q] State Changes Trigger EventsTested code MUST emit a contract event for all transactions that cause state changes.\n[Q] No Private DataTested code MUST NOT store Private Data on the blockchain.\n[Q] Intended ReplayIf a signature within the Tested Code can be reused, the replay instance MUST be intended, documented, and safe for re-use.\nThis is an Overriding Requirement for [M] No Improper Usage of Signatures for Replay Attack Protection.\n[Q] Document Contract LogicA specification of the business logic that the Tested code functionality is intended to implement MUST be available to anyone who can call the Tested Code.\n[Q] Document System ArchitectureDocumentation of the system architecture for the Tested code MUST be provided that conveys the overrall system design, privileged roles, security assumptions and intended usage.\n[Q] Document Threat ModelsDocumented Threat Models for the Tested code MUST be provided, describing each threat, security assumptions, expected responses, and expected outcomes.\n[Q] Annotate Code with NatSpecAll Public Interfaces contained in the Tested code MUST be annotated with inline comments according to the [NatSpec] format that explain the intent behind each function, parameter, event, and return variable, along with developer notes for safe usage.\n[Q] Implement as DocumentedThe Tested code MUST behave as described in the documentation provided for [Q] Document Contract Logic, and [Q] Document System Architecture.\n[Q] Enforce Least PrivilegeTested code that enables privileged access MUST implement appropriate access control mechanisms that provide the least privilege necessary for those interactions, based on the documentation provided for [Q] Document Contract Logic.This is an Overriding Requirement for [M] Protect Self-destruction.\n[Q] Use Revocable and Transferable Access Control PermissionsIf the Tested code makes uses of Access Control for privileged actions, it MUST implement a mechanism to revoke and transfer those permissions.\n[Q] No Single Admin EOA for Privileged ActionsIf the Tested code makes uses of Access Control for privileged actions, it MUST ensure that all critical administrative tasks require multiple signatures to be executed, unless there is a multisg admin that has greater privileges and can revoke permissions in case of a compromised or rogue EOA and reverse any adverse action the EOA has taken.\n[Q] Verify External CallsTested Code that contains external calls\nMUST document the need for them, andMUST protect them in a way that is fully compatible with the claims of the contract author.This is part of a Set of Overriding Requirements for [S] Use Check-Effects-Interaction, and for [M] Protect External Calls.\n[Q] Verify tx.origin UsageFor Tested Code that uses tx.origin, each instance\nMUST be consistent with the stated security and functionality objectives of the Tested Code, andMUST NOT allow assertions about contract functionality made for [Q] Document Contract Logic or [Q] Document System Architecture to be violated by another contract, even where that contract is called by a user authorized to interact directly with the Tested Code.This is an Overriding Requirement for [S] No tx.origin.\n[Q] Specify Solidity Compiler Versions to Produce Consistent OutputThe Tested Code MUST specify a range of Solidity versions in its pragma directive(s) that produce the same Bytecode given the same compilation options.\n[GP] Check For and Address New Security BugsCheck [solidity-bugs-json] and other sources for bugs announced after 1 November 2023 and address them.\n[GP] Meet as Many Requirements as PossibleThe Tested Code SHOULD meet as many requirements of this specification as possible at Security Levels above the Security Level for which it is certified.\n[GP] Use Latest CompilerThe Tested Code SHOULD use the latest available stable Solidity compiler version.\n[GP] Write Clear, Legible Solidity CodeThe Tested Code SHOULD be written for easy understanding.\n[GP] Follow Accepted ERC StandardsThe Tested Code SHOULD conform to finalized [ERC] standards when it is reasonably capable of doing so for its use-case.\n[GP] Define a Software LicenseThe Tested Code SHOULD define a software license\n[GP] Disclose New Vulnerabilities ResponsiblySecurity vulnerabilities that are not addressed by this specification SHOULD be brought to the attention of the Working Group and others through responsible disclosure as described in § 1.4 Feedback and new vulnerabilities.\n[GP] Use FuzzingFuzzing SHOULD be used to probe Tested Code for errors.\n[GP] Use Mutation TestingMutation Testing SHOULD be used to evaluate and improve the quality of test suites for smart contracts.\n[GP] Use Formal VerificationThe Tested Code SHOULD undergo formal verification.\n[GP] Select an Appropriate Threshold for Multisig WalletsMultisignature requirements for privileged actions SHOULD have a sufficient number of signers, and NOT require “1 of N” nor all signatures.\nB. References 引用B.1 Normative references[c-e-i]Security Considerations - Solidity Documentation. Section ‘Use the Checks-Effects-Interactions Pattern’. Ethereum Foundation. URL: https://docs.soliditylang.org/en/latest/security-considerations.html#use-the-checks-effects-interactions-pattern[CVE-2021-42574]National Vulnerability Database CVE-2021-42574. The National Institute of Standards (US Department of Commerce). URL: https://nvd.nist.gov/vuln/detail/CVE-2021-42574[CWE]Common Weakness Enumeration. MITRE. URL: https://cwe.mitre.org/index.html[CWE-1339]CWE-1339: Insufficient Precision or Accuracy of a Real Number. MITRE. URL: https://cwe.mitre.org/data/definitions/1339.html[CWE-252]CWE-252: Unchecked Return Value. MITRE. URL: https://cwe.mitre.org/data/definitions/252.html[CWE-284]CWE-284: Improper Access Control. MITRE. URL: https://cwe.mitre.org/data/definitions/284.html[CWE-573]CWE-573: Improper Following of Specification by Caller. MITRE. URL: https://cwe.mitre.org/data/definitions/573.html[CWE-667]CWE-667: Improper Locking. MITRE. URL: https://cwe.mitre.org/data/definitions/667.html[CWE-670]CWE-670: Always-Incorrect Control Flow Implementation. MITRE. URL: https://cwe.mitre.org/data/definitions/670.html[CWE-94]CWE-94: Improper Control of Generation of Code (‘Code Injection’). MITRE. URL: https://cwe.mitre.org/data/definitions/94.html[DevCon-rounding]Tackling Rounding Errors with Precision Analysis. Raoul Schaffranek. Ethereum Foundation. URL: https://archive.devcon.org/archive/watch/6/tackling-rounding-errors-with-precision-analysis/?tab=YouTube[DevCon-siphoning]The Gas Siphon Attack: How it Happened and How to Protect Yourself. Shane Fontaine. Ethereum Foundation. URL: https://archive.devcon.org/archive/watch/5/the-gas-siphon-attack-how-it-happened-and-how-to-protect-yourself/?tab=YouTube[EF-SL]Specification languages for creating formal specifications. Ethereum Foundation. URL: https://ethereum.org/en/developers/docs/smart-contracts/formal-verification/#specification-languages[EIP]EIP-1: EIP Purpose and Guidelines. Ethereum Foundation. URL: https://eips.ethereum.org/EIPS/eip-1[EIP-155]Simple Replay Attack Protection. Vitalik Buterin. Ethereum Foundation. URL: https://eips.ethereum.org/EIPS/eip-155[EIP-6049]Deprecate SELFDESTRUCT. William Entriken. Ethereum Foundation. URL: https://eips.ethereum.org/EIPS/eip-6049[ERC]ERC Final - Ethereum Improvement Proposals. Ethereum Foundation. URL: https://eips.ethereum.org/erc[ERC137]ERC-137: Ethereum Domain Name Service - Specification. Ethereum Foundation. URL: https://eips.ethereum.org/EIPS/eip-137[ERC20]EIP-20: Token Standard. Ethereum Foundation. URL: https://eips.ethereum.org/EIPS/eip-20[ERC721]ERC 721: Non-fungible Token Standard. Ethereum Foundation. URL: https://eips.ethereum.org/EIPS/eip-721/[error-handling]Control Structures - Solidity Documentation. Section ‘Error handling: Assert, Require, Revert and Exceptions’. Ethereum Foundation. URL: https://docs.soliditylang.org/en/v0.8.14/control-structures.html#error-handling-assert-require-revert-and-exceptions[ET-tools]EEA EthTrust Tool Implementation Registry. EEA. URL: https://github.com/EntEthAlliance/ethtrust-tool-registry/[EthTrust-sl-v1]EEA EthTrust Security Levels Specification. Version 1. Enterprise Ethereum Alliance. URL: https://entethalliance.org/specs/ethtrust-sl/v1/[EVM-version]Using the Compiler - Solidity Documentation. (§Targets). Ethereum Foundation. URL: https://docs.soliditylang.org/en/latest/using-the-compiler.html#target-options[freipi]You’re writing require statements wrong. Brock Elmore. Nascent. URL: https://www.nascent.xyz/idea/youre-writing-require-statements-wrong[GDPR]Regulation (EU) 2016&#x2F;679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95&#x2F;46&#x2F;EC (General Data Protection Regulation) (Text with EEA relevance). The European Union. URL: https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679[hash-commit]Commitment scheme - WikiPedia. WikiMedia Foundation. URL: https://en.wikipedia.org/wiki/Commitment_scheme[Ivanov]Targeting the Weakest Link: Social Engineering Attacks in Ethereum Smart Contracts. Nikolay Ivanov; Jianzhi Lou; Ting Chen; Jin Li; Qiben Yan. URL: https://arxiv.org/pdf/2105.00132.pdf#subsection.4.2[NatSpec]NatSpec Format - Solidity Documentation. Ethereum Foundation. URL: https://docs.soliditylang.org/en/latest/natspec-format.html[Ownable]ERC-173: Contract Ownership Standard. Ethereum Foundation. URL: https://eips.ethereum.org/EIPS/eip-173[RBAC]INCITS 359-2012: Information Technology - Role Based Access Control. InterNational Committee for Information Technology Standards. URL: http://www.techstreet.com/products/1837530[RFC2119]Key words for use in RFCs to Indicate Requirement Levels. S. Bradner. IETF. March 1997. Best Current Practice. URL: https://www.rfc-editor.org/rfc/rfc2119[RFC8174]Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words. B. Leiba. IETF. May 2017. Best Current Practice. URL: https://www.rfc-editor.org/rfc/rfc8174[rounding-errors]Formal Specification of Constant Product (x × y &#x3D; k) Market Maker Model and Implementation. Runtime Verification. URL: https://github.com/runtimeverification/verified-smart-contracts/blob/uniswap/uniswap/x-y-k.pdf[SHA3-256]FIPS 202 - SHA-3 Standard: Permutation-Based Hash and Extendable-Output Functions. The National Institute of Standards (US Department of Commerce). URL: http://dx.doi.org/10.6028/NIST.FIPS.202[software-license]Choosing an Open Source License. GitHub. URL: https://choosealicense.com/[solidity-alerts]Solidity Blog - Security Alerts. Ethereum Foundation. URL: https://blog.soliditylang.org/category/security-alerts/[solidity-bugs]List of Known Bugs. Ethereum Foundation. URL: https://github.com/ethereum/solidity/blob/develop/docs/bugs.rst[solidity-bugs-json]A JSON-formatted list of some known security-relevant Solidity bugs. Ethereum Foundation. URL: https://github.com/ethereum/solidity/blob/develop/docs/bugs.json[solidity-cheatsheet]Solidity Documentation: Cheatsheet - Order Of Precedence Of Operators. Ethereum Foundation. URL: https://docs.soliditylang.org/en/latest/cheatsheet.html#order-of-precedence-of-operators[solidity-events]Solidity Documentation: Contracts - Events. Ethereum Foundation. URL: https://docs.soliditylang.org/en/latest/contracts.html#events[solidity-patterns]Solidity Patterns. Franz Volland. URL: https://fravoll.github.io/solidity-patterns/[solidity-release-818]Solidity 0.8.18 Release Announcement. Ethereum Foundation. 2023-02-01. URL: https://soliditylang.org/blog/2023/02/01/solidity-0.8.18-release-announcement/[solidity-security]Security Considerations - Solidity Documentation.. Ethereum Foundation. URL: https://docs.soliditylang.org/en/latest/security-considerations.html[Solidity-Style-Guide]Solidity Style Guide - Solidity Documentation. URL: https://docs.soliditylang.org/en/latest/style-guide.html[solidity-underhanded-richards2022]Solidity Underhanded Contest 2022. Submission 9 - Tynan Richards. Ethereum Foundation. URL: https://github.com/ethereum/solidity-underhanded-contest/tree/master/2022/submissions_2022/submission9_TynanRichards[swcregistry]Smart Contract Weakness Classification Registry. ConsenSys Diligence. URL: https://swcregistry.io[TimeLock]Protect Your Users With Smart Contract Timelocks. OpenZeppelin. URL: https://blog.openzeppelin.com/protect-your-users-with-smart-contract-timelocks/[tmio-bp]Best Practices for Smart Contracts (privately made available to EEA members). TMIO. URL: https://github.com/EntEthAlliance/eta-registry/blob/master/working-docs/tmio-bp.md[token-standards]Ethereum Development Documentation - Token Standards. Ethereum Foundation. URL: https://ethereum.org/en/developers/docs/standards/tokens/[unicode-bdo]How to use Unicode controls for bidi text. W3C Internationalization. 10 March 2016. URL: https://www.w3.org/International/questions/qa-bidi-unicode-controls[unicode-blocks]Blocks-14.0.0.txt. Unicode®, Inc. 22 January 2021. URL: https://www.unicode.org/Public/UNIDATA/Blocks.txtB.2 Informative references[ASVS]OWASP Application Security Verification Standard. The OWASP Foundation. URL: https://github.com/OWASP/ASVS[certik-rari]Fei Protocol Incident Analysis. CertiK. URL: https://certik.medium.com/fei-protocol-incident-analysis-8527440696cc[chase]Malleable Signatures: New Definitions and Delegatable Anonymous Credentials. Melissa Chase; Markulf Kohlweiss; Anna Lysyanskaya; Sarah Meiklejohn. URL: https://smeiklej.com/files/csf14.pdf[EEA-L2]Introduction to Ethereum Layer 2. Enterprise Ethereum Alliance, Inc. URL: https://entethalliance.org/eea-primers/entry/5696/[EF-MEV]Maximal Extractable Value (MEV). Ethereum Foundation. URL: https://ethereum.org/en/developers/docs/mev/[EIP-3529]Reduction in Refunds. Ethereum Foundation. URL: https://eips.ethereum.org/EIPS/eip-3529[futureblock]Future-block MEV in Proof of Stake. Ethereum Foundation. URL: https://archive.devcon.org/archive/watch/6/future-block-mev-in-proof-of-stake/?tab=YouTube[License]Apache license version 2.0. The Apache Software Foundation. URL: http://www.apache.org/licenses/LICENSE-2.0[postmerge-mev]Why is Oracle Manipulation after the Merge so cheap? Multi-Block MEV. ChainSecurity. URL: https://chainsecurity.com/oracle-manipulation-after-merge/[solidity-reports]Reporting a Vulnerability, in Security Policy. Ethereum Foundation. URL: https://github.com/ethereum/solidity/security/policy#reporting-a-vulnerability[WSE]Symbolic Execution. WikiPedia. URL: https://en.wikipedia.org/wiki/Symbolic_execution\nB.2 Informative references[ASVS]OWASP Application Security Verification Standard. The OWASP Foundation. URL: https://github.com/OWASP/ASVS[certik-rari]Fei Protocol Incident Analysis. CertiK. URL: https://certik.medium.com/fei-protocol-incident-analysis-8527440696cc[chase]Malleable Signatures: New Definitions and Delegatable Anonymous Credentials. Melissa Chase; Markulf Kohlweiss; Anna Lysyanskaya; Sarah Meiklejohn. URL: https://smeiklej.com/files/csf14.pdf[EEA-L2]Introduction to Ethereum Layer 2. Enterprise Ethereum Alliance, Inc. URL: https://entethalliance.org/eea-primers/entry/5696/[EF-MEV]Maximal Extractable Value (MEV). Ethereum Foundation. URL: https://ethereum.org/en/developers/docs/mev/[EIP-3529]Reduction in Refunds. Ethereum Foundation. URL: https://eips.ethereum.org/EIPS/eip-3529[futureblock]Future-block MEV in Proof of Stake. Ethereum Foundation. URL: https://archive.devcon.org/archive/watch/6/future-block-mev-in-proof-of-stake/?tab=YouTube[License]Apache license version 2.0. The Apache Software Foundation. URL: http://www.apache.org/licenses/LICENSE-2.0[postmerge-mev]Why is Oracle Manipulation after the Merge so cheap? Multi-Block MEV. ChainSecurity. URL: https://chainsecurity.com/oracle-manipulation-after-merge/[solidity-reports]Reporting a Vulnerability, in Security Policy. Ethereum Foundation. URL: https://github.com/ethereum/solidity/security/policy#reporting-a-vulnerability[WSE]Symbolic Execution. WikiPedia. URL: https://en.wikipedia.org/wiki/Symbolic_execution\n","tags":["Blockchain, ETH, Security"]},{"title":"什么是生物标志物Biomarker?","url":"/2022/12/04/Healthcare-2022-12-05-what-is-biomarker/","content":"什么是生物标志物Biomarker?\n生物标志物Biomarker在临床研究、临床医疗被广泛使用，尤其是它在某些条件下可以作为临床试验主要终点的替代。\n\n简介生物标志物对药物和医疗设备的开发至关重要。尽管它们具有巨大的价值，在研究和临床实践中使用它们所涉及的基本定义和概念却非常混乱。此外，生物标志物的复杂性也被认为是理解慢性疾病和营养的一个限制。几年前，这个问题达到了顶峰。在美国食品和药物管理局（FDA）和美国国立卫生研究院（NIH）的一次联合领导会议上，每个联邦机构的领导人对生物标志物在不同使用环境中的适当定义有不同的印象。因此，他们成立了一个联合工作组，以形成共同的定义，并通过不断更新的在线文件–”生物标志物、终点和其他工具”（BEST）资源公开提供这些定义。\n定义术语Biomarker这个词由biological marker组成，不同的组织对生物标志物有不同定义，但是基本上表达的意义不尽相同。美国NIH的定义是：“一种可客观测量和评估的特征，作为正常生物过程、致病过程或对治疗干预的药理学反应的指标”。WHO牵头的定义是：“可以在体内或其产出物中被测量的任何物质、结构或过程，这些物质、结构或者过程会影响或可预测后续结果及疾病发生率”。后续WHO扩展了定义，不仅考虑到疾病的发病率和结果，还考虑到治疗、干预，甚至是无意的环境暴露，如化学品或营养物质的影响。一般而言，生物标志物可以指代医学标志的广泛子集，即从病人内外部观察到的医学状态的客观指标，这些指标可以被准确和重复测量。医学标志和医学症状是不同的，症状仅仅指患者自己感知的健康或疾病的迹象。\n分类生物标志物可以按生物学分类：可分为核酸类(基因组、循环肿瘤DNA、DNA甲基化、转录组、长链非编码RNA、微小RNA、信使RNA等)、蛋白类(蛋白组、肿瘤抗原、细胞因子等)、抗体类(肿瘤相关抗原自身抗体等)、代谢产物、细胞因子、微生物、外泌体、核小体等多种类别。\n例子生物标志物的例子包括从脉搏和血压这些基本的化学指标到更复杂的血液和其他组织的实验室检测。医学标志在临床实践中有着悠久的使用历史–与医学实践本身一样古老，而生物标志只是现代实验室科学允许我们可重复测量的最客观、可量化的医学标志。在临床研究中使用生物标志物，特别是实验室测量的生物标志物，是比较新颖的，这种做法的最佳方法仍在发展和完善之中。\n临床应用风险生物标志物在改善药物开发过程以及更大的生物医学研究事业中发挥着关键作用。了解可测量的生物过程和临床结果之间的关系，对于扩大我们治疗所有疾病的武器库，以及加深我们对正常、健康生理学的理解至关重要。至少从20世纪80年代开始，在癌症和心脏病等重大疾病的大型试验中，使用生物标志物作为替代结果的必要性已被广泛讨论。美国食品和药物管理局继续促进生物标志物在基础和临床研究中的使用，以及对潜在的新生物标志物的研究，以便在未来的试验中作为替代物使用。然而，尽管生物标志物具有良好的潜力来加速药物开发，减少对无效实验治疗的接触等等，但当试验设计者将其与临床终点相混淆时，生物标志物会带来巨大风险。只有当我们完全了解生物过程的正常生理学、疾病状态下该过程的病理生理学以及干预措施–药物、设备或其他–对这些过程的影响时，生物标志物才能真正替代临床相关终点。由于我们很少有机会了解这些过程的全貌，因为总是有更多的细节我们不知道或不了解，所以作为替代终点的生物标志物需要不断地重新评估。使用生物标志物的研究应该始终以临床结果为最终衡量标准，至少要对生物标志物相关的成功进行回顾性分析。如果不对代用终点和真正的临床终点之间的关系进行持续的重新评估，我们就有可能再次批准整类药物，这些药物要么没有额外的好处，要么更糟糕的是会伤害病人。\n外部链接\nBEST (Biomarkers, EndpointS, and other Tools) Resource\nWhat are biomarkers?\n肿瘤早期预警生物标志物的研究与思考\n\n","categories":["healthcare"],"tags":["biomarker, healthcare"]},{"title":"SAP的主数据模块和物料管理","url":"/2021/03/11/ERP-2021-03-12-SAP-MM/","content":"SAP的数据SAP把数据分为主数据和事务型数据。SAP的主数据一般有：\n\n物料主数据\n客户主数据\n供应商主数据\n价格&#x2F;条件主数据\n仓库管理主数据\n\nMaterial Master在SAP中，每个物料都有一个特征叫&quot;material type&quot;, 用于不同的用途。\n","categories":["ERP"],"tags":["ERP, SAP"]},{"title":"Eyes on FHIR","url":"/2022/11/19/Standard-2022-11-20-Eyes-on-FHIR/","content":"什么是 Eyes on FHIR ？\n从事医疗信息化的朋友可能听说过FHIR(Fast Healthcare Interoperability Resouce), 但是在具体实现和应用上可能没有相关经验。下面这篇文章里，我们一起来看看美国的一个Patient Care project在眼科是怎么落地FHIR的。\n\n背景2020年美国《21世纪医疗法》最终法规要求健康技术供应商必须支持2个公开的API能够使患者访问自己的健康数据。这两个API需要遵循HL7 FHIR标准。随着社区力量的驱动，FHIR的流行度和成熟度在美国逐渐成型，使得健康信息数据互操作性得到了很大的提升。\nEyes on FHIR的目的虽然在美国FHIR的发展有很大进展，但是真实世界项目的落地尤其是临床专科的应用仍十分有限，主要原因是没有统一的实施指引（Implementation Guidance)。所以这个项目的目标就是开发必要的国际标准使FHIR能够真正地给某个专科带来价值。此项目的实施方式主要通过把眼科尽量全面的临床词典映射到FHIR格式，最后形成实施标准来帮助实现系统间的互联互通。\n\n具体流程如下：\n\n列出一系列眼科真实世界的用例，尤其是有互操作性痛点的案例。用例的形式需要考虑如下情形：\n\n涉及的元素\nActors 操作人\nPre-condition 先决条件\nWorkflow 工作流\nPost-condition 后置条件\nExceptions 例外\nData elements 数据元素\nNecessary commentary 必要的备注\n\n\n专用词汇的定义\nFHIR 资源\n临床专业术语， 如SNOMED-CT, LOINC, DICOM\n\n\n局限和差距的发现\n临床上，在某些地方的设备代码并不全面\n如何做FHIR profile\n\n\n\n\n收集和整理如上信息，包括编码系统等。\n\n把上述信息转化为FHIR Profiles, 配置是否是必须项。\n\n开发实施指引IG, 甄别如何使用FHIR资源解决某个特定的互操作性问题。\n\n发布实施指引\n\n参加三年一次的技术连接会，演示用例和测试互操作性。\n\n参加三年一次的HL7-FHIR投票，获准正式发布实施指引。\n\n转化IG, 让更多真实世界的案例可以使用此指引。\n\n\n项目预期结果\n建立治理、质控和伦理的原则；促成眼科相关各界人事代表组成委员会共同发展未来路线图。\n临床和研究影响力。作为第一个临床专科实施项目，藉此机会展示FHIR的临床好处；记录可以重复利用的技术、协议、流程等材料。\n\n此项目使用的场景\n日常临床关护沟通相关；病人旅程相关，比如：纵向病患旅程；异步通信；远程医疗设备等\n医师临床工具， 如SMART app&#x2F;CDS hooks&#x2F;integrating remote monitoring\n患者注册数据收集， 如批量数据导出\n临床工作流优化，如自动支付、审计、优先授权、行政管理。\n辅助生命科学和研发，如辅助临床招募，数据统一。\nAI图像诊断工作流，如下图：\n\n\n具体细节功能\nProviding guidance for the FHIR representation of a comprehensive set of codified clinical findings to facilitate interoperable exchange of these data elements between systems.\nProviding guidance for the FHIR representation of a comprehensive set of codified diagnoses to facilitate interoperable exchange of these data elements between systems.\nProviding guidance for the FHIR representation of a comprehensive set of codified retinal therapeutics to facilitate interoperable exchange of these data elements between systems.\nExchanging information between and EHR (&#x2F;PMS) and a diagnostic device where:EHR Supports DICOM Modality Worklist, Storage and Display (With no PACS)Based on IHE’s Unified Eyecare Workflow ‘real world model’ (RWM) II\nExchanging information between and EHR (&#x2F;PMS) and a diagnostic device where:EHR Supports DICOM Modality Worklist and Integrates with a PACSBased on IHE’s Unified Eyecare Workflow ‘real world model’ (RWM) I\nExchanging information between and EHR (&#x2F;PMS) and a diagnostic device where:EHR Implements HL7 Only (no DICOM support) and Integrates with a PACS)Based on IHE’s Unified Eyecare Workflow ‘real world model’ (RWM) III\nSending a referral containing clinical information from and&#x2F;or to any combination of the following practitioners: ophthalmologist, optometrist, general practitioner, specialist\nSending a referral from a clinician to a healthcare service (eg - an ambulatory service centre when booking for cataract surgery.\nExchanging information between and EHR and a clinical registry (eg - FRB!) - where:The registry is highly structured；Not all registry required data points are routinely captured in a structured format in the EHR；Patient details must be de-identified.\nSending in bulk either all or part of an entire clinical record to a clinical registry.\nSending select data elements from select patients (de-identified) in bulk to a research institute &#x2F; life science body.\nThe referral of a patient to a clinical trial.\nAutomating prior authorization for anti-VEGF injections.\nReal time clinical trial recruitment using CDS hooks (eg for treatment-naive wet-AMD)\nSMART on FHIR app to compile and display relevant information into a single screen &#x2F; application\n\n外部链接\nEyes on FHIR\n\n","categories":["standard"],"tags":["FHIR, Standard"]},{"title":"Eyes on FHIR Implement Guide（1）","url":"/2022/12/05/Standard-2022-12-06-Eyes-on-FHIR-IG-1/","content":"FHIR眼科实施指南IG（1）\n2021年8月，FHIR patient care小组发布了FHIR眼科实施指南0.1.0版本，此版本只针对眼底相关疾病，并且通过了FHIR connectathon测试，可以双向与真实世界的EMR、诊断设备和PACS通信。\n\nIG主要内容实施指南的主要内容可分为：\n\n患者旅程\n概貌（Profiles）\n15个真实用例\n指引规范\n术语\n贡献者\n产出物\n\n患者旅程本文将详细介绍患者旅程部分。目前0.1版本只涵盖了白内障和青光眼患者旅程，下面将分段介绍。\n白内障用例问题声明白内障手术的方法和术后结果的评价目前没有一个统一的标准。手术大夫使用不同的设备、数据集和信息系统来存储数据，造成了数据孤岛。国际上目前也没有一个通用的技术来共享白内障手术的数据。这就阻碍了这个领域的专业从业人员之间的合作，分享和研究等工作。\n用例手术大夫使用眼科管理系统记录患者白内障手术的术前数据、术中数据和术后数据。利益关系人包括手术大夫、患者群体、支付者、权威机构和研究者，他们需要追踪手术结果并评估类似工作，这么做主要的好处是可以分享白内障医疗实践和研究成果给其他眼科医生，并且进一步鉴别问题人群的需求，评估不同手术设备和耗材的差异与技术区别性。针对不同的用户，好处分别是：\n\n眼科大夫\n\n可以接收随访的数据来进一步观察患者术后的情况\n逐步累计的数据可以帮助医生自我审查手术的结果，提高自身技术\n同事间相互交流对比\n最大限度地减少用于记录、发送、检索和追踪的认知和实践的工作量\n\n\n特检医师（验光师）\n\n增强在术前和术后对患者的沟通教育，提升服务质量\n更容易和医生建立好合作关系\n标准化和自动化审计和收集信息来提升或改善病患关系\n最小化数据整理的工作\n\n\n患者\n\n可以收到自己的健康数据\n可以把个人眼科信息集中到各类眼健康应用，比如配镜场景\n最大限度减少个人信息管理的工作\n\n\n\n青光眼用例患者旅程一个老年开角型青光眼（condition）患者需要经常性的监控。大部分检测发生在特检科，那里他需要经历一系列的检查，如视敏度，眼压（observation）等，和一系列诊断检查如视野，眼底照片，OCT等，以及风险评估。青光眼作为慢性病，较好的管理方式取决纵向和多模态数据的收集，如examination, testing, imaging(Observation, ImageStudy, DiagnosticReport)。患者复诊的频率一般从3-12个月之间。任何一个指标如果出现恶化，患者一般会被眼科大夫（ServiceRequest, Referral note）重新评估治疗方案或手术激光干预(Procedure)。所以这些过程中记录的数据就十分重要，方便后续随访和慢病管理。对于不同用户，使用这样用例的好处分别是：\n\n眼科大夫\n可以远程监控患者的进展情况\n更加方便从患者慢病全流程上做结果评估\n同事间的交流\n最大限度地减少用于记录、发送、检索和追踪的认知和实践的工作量\n\n\n特检医师（验光师）\n更加有信心检查患者，并且知道大夫能够访问检查数据和结果\n增强对患者的沟通教育，引导患者随访医生，提升服务质量\n更容易和医生建立好合作关系\n标准化和自动化审计和收集信息来提升或改善病患关系\n最小化数据整理的工作\n\n\n患者\n可以收到自己的健康数据\n可以把自己的数据集成到各种健康档案应用，如青光眼药物依从性。也可以使用其它设备如Home Icare家用手持式眼压计来富集个人的监控数据\n最大限度减少个人信息管理的工作\n\n\n\n外部链接\nPatient Journey Use Case\n\n","categories":["standard"],"tags":["FHIR, Standard,IG"]},{"title":"Eyes on FHIR Implement Guide（2)","url":"/2022/12/23/Standard-2022-12-24-Eyes-on-FHIR-IG-2/","content":"FHIR眼科实施指南IG（2）\n2021年8月，FHIR patient care小组发布了FHIR眼科实施指南（IG）0.1.0版本，此版本只针对眼底相关疾病，并且通过了FHIR connectathon测试，可以双向与真实世界的EMR、诊断设备和PACS通信。\n\nIG主要内容实施指南(Implement Guide)的主要内容可分为：\n\n患者旅程\n概貌（Profiles）\n15个真实用例\n指引规范\n术语\n贡献者\n产出物\n\n概貌（Profiles）HL7中国的wiki把profile翻译成概貌，也有翻译成配置或规范, 因为profiling翻译成概貌化似乎没有规范化更加准确。profiling用中文解释大概是把一个资源拓展并采用一些约束来限制它，以达到应用目的，拓展好的资源我们可以认识是profile. 0.1版本中有如下临床概貌已经被定义：\n眼科观测（Observations）\n基础眼科观察概貌，此ObservationBase概貌描述仅限眼科的观察，下表1是必须实现的字段。\n\n\n\n\nName\nCardinality\nType\n\n\n\nstatus\n1..1\ncode\n\n\ncategory\n0..*\nCodeableConcept\n\n\ncode\n1..1\nCodeableConcept\n\n\nsubject\n1..1\nReference(Patient)\n\n\nbodySite\n0..1\nCodeableConcept\n\n\nbodySite.extension\n0..*\nReference(BodyStructure)\n\n\nbodySite.extension.value\n1..1\nReference(BodyStructure|Ocular anatomical location)\n\n\n表一. 基础眼科观察profile必须实现的字段\n\n\n眼部解剖位置概貌。此BodyStructureEye概貌将眼部分为眼球、眼周和眼眶。由于眼部的位置需要更加细粒度的描述，所以需要使用BodyStructure.locationQualifier和BodyStructure.location绑定来精确地描述眼部解剖位置。\n\n\n\n\nName\nCardinality\nType\n\n\n\nlocation\n0..1\nCodeableConcept\n\n\nlocationQualifier\n0..*\nCodeableConcept\n\n\n表二. 眼部解剖位置概貌profile必须实现的字段\n\n临床观察（测量类的发现）\n眼压IOPObservationIOP，患者眼内(眼球内)压力的测量值(单位：mmHg)。\n\n\n\n\nName\nCardinality\nType\n\n\n\nstatus\n1..1\ncode(final or amended)\n\n\ncode\n1..1\nCodeableConcept\n\n\nsubject\n1..1\nReference(Patient)\n\n\nvalue[x]*\n1..1\nQuantity\n\n\nvalue[x].value\n1..1\ndecimal\n\n\nvalue[x].unit\n1..1\nstring\n\n\nvalue[x].system\n1..1\nuri\n\n\nvalue[x].code\n1..1\ncode\n\n\nbodySite\n0..1\nCodeConcept\n\n\nbodySite.extension\n0..*\nExtension(bodySite)\n\n\nbodySite.extension.value[x]\n1..1\nReference(BodyStructure|Ocular anatomical location)\n\n\nmethod\n0..1\ncodeableConcept\n\n\nmethod.coding\n1..*\nCoding\n\n\n表三.眼压测量概貌profile\n*[x]代表元素可以有多个类型\n\n\n视力VAObservationVisualAcuity, 可测量的检测还包括视力Visual Acuity概貌。\n\n\n\n\nName\nCardinality\nType\n\n\n\nstatus\n1..1\ncode(final or amended)\n\n\ncategory\n0..*\nCodeableConcept\n\n\ncode\n1..1\nCodeableConcept\n\n\nsubject\n1..1\nReference(Patient)\n\n\nbodySite\n0..1\nCodeConcept\n\n\nbodySite.extension\n0..*\nExtension(bodySite)\n\n\nbodySite.extension.value[x]\n1..1\nReference(BodyStructure|Ocular anatomical location)\n\n\nmethod\n1..1\ncodeableConcept\n\n\n表四.视力测量概貌\n\n临床观察（观测类的发现）其它观测类的临床发现用这个ObservationEyeRegionFinding概貌描述，此概貌也可以用Condition资源，可以来描述其它非眼科类的观察，比如：\n\n很可能与疾病无关的观察，如患者进入诊室的步态。\n可能相关的观察，如甲状腺肿大，和甲状腺相关的眼科疾病。\n高度相关的观察，如高血压。高血压是潜在非眼部或系统性的致盲原因，例如严重的高血压性视网膜病变或视网膜血管阻塞。\n\n眼科诊断&#x2F;疾病ConditionBase这个概貌用来描述过去或者当前的某个眼科疾病诊断。虽然这个概貌和上述临床其他观测类发现是参考了同一个概貌(Condition资源和眼科键值对值集组合), 但是此处所用应为实际临床诊断，而上述仅仅为临床其他类观测。并且眼部解剖位置概貌BodyStructureEye概貌在此处需组合使用。\n\n\n\nName\nCardinality\nType\n\n\n\nCondition.code\n1..1\nCodeableConcept(Ophthalmology Condition ICD10 and SNOMED codes ValueSet)\n\n\nCondition.bodySite\n0..*\nCodeableConcept(anatomical location)\n\n\nCondition.bodySite.extension\n0..*\nExtension\n\n\nCondition.bodySite.extension.value[x]\n1..1\nReference\n\n\nCondition.subject\n1..1\nReference(Patient|Group)\n\n\n表五.眼科诊断概貌\n\n眼科干预程序ProcedureBase眼科基本的干预程序，搭配眼部解剖位置概貌组合使用。\n\n\n\nName\nCardinality\nType\n\n\n\nProcedure.code\n1..1\nCodeableConcept(OphthalmologyProceduresValueSet)\n\n\nProcedure.bodySite\n0..*\nCodeableConcept(anatomical location)\n\n\nCondition.bodySite.extension\n0..*\nExtension\n\n\nCondition.bodySite.extension.value[x]\n1..1\nReference\n\n\n表六.眼科干预程序概貌\n\n诊断检查报告OphthalDiagnosticReport定义了眼科诊断报告概貌。\n\n\n\nName\nCardinality\nType\n\n\n\nDiagnosticReport.category\n1..*\nCodeableConcept(service category)\n\n\nDiagnosticReport.category.coding\n1..1\nCoding (ophthalCode)\n\n\nDiagnosticReport.category.coding.system\n1..1\nuri(固定值：http://snomed.info/sct)\n\n\nDiagnosticReport.category.coding.code\n1..1\ncode(固定值：394594003)\n\n\n表七.眼科诊断报告概貌\n\n视野检查ObservationVisualField视野检查概貌，此概貌可以单独用在视野检查观察。\n\n\n\nName\nCardinality\nType\n\n\n\nstatus\n1..1\ncode(final or amended)\n\n\ncategory\n0..*\nCodeableConcept(ObservationCategoryCodes)\n\n\ncode\n1..1\nCodeableConcept(LOINCCodes)\n\n\nsubject\n1..1\nReference(Patient)\n\n\nbodySite\n0..1\nCodeableConcept\n\n\nbodySite.extension\n0..*\nExtension(bodySite)\n\n\nbodySite.extension.value[x]\n1..1\nReference(BodyStructure|Ocular anatomical location)\n\n\n表八.视野检查概貌\n\n视野诊断报告OphthalDiagnosticReportForVisualField视野检查报告概貌。\n\n\n\nName\nCardinality\nType\n\n\n\nDiagnosticReport.category\n1..*\nCodeableConcept(service category)\n\n\nDiagnosticReport.code.coding\n1..1\nCoding(vfCode)\n\n\nDiagnosticReport.code.coding.system\n1..1\nuri(固定值：http://snomed.info/sct)\n\n\nDiagnosticReport.code.coding.code\n1..1\ncode(固定值：103752008)\n\n\nDiagnosticReport.result\n0..*\nReference(Observation | VF Observations)\n\n\n表九.视野检查报告概貌\n\nOCT黄斑检查ObservationOCTMacula, OCT黄斑检查概貌，一般和诊断报告一起使用。\n\n\n\nName\nCardinality\nType\n\n\n\nstatus\n1..1\ncode(final or amended)\n\n\ncategory\n0..*\nCodeableConcept(ObservationCategoryCodes)\n\n\ncode\n1..1\nCodeableConcept(LOINCCodes)\n\n\nsubject\n1..1\nReference(Patient)\n\n\nbodySite\n0..1\nCodeableConcept\n\n\nbodySite.extension\n0..*\nExtension(bodySite)\n\n\nbodySite.extension.value[x]\n1..1\nReference(BodyStructure|Ocular anatomical location)\n\n\n表十.OCT黄斑检查概貌\n\nOCT黄斑诊断报告OphthalDiagnosticReportOCTMacula, OCT黄斑诊断报告。\n\n\n\nName\nCardinality\nType\n\n\n\nDiagnosticReport.category\n1..*\nCodeableConcept(service category)\n\n\nDiagnosticReport.code.coding\n1..1\nCoding(maculaCode)\n\n\nDiagnosticReport.code.coding.system\n1..1\nuri(固定值：http://loinc.org)\n\n\nDiagnosticReport.code.coding.code\n1..1\ncode(固定值：57119-0)\n\n\nDiagnosticReport.result\n0..*\nReference\n\n\n表十一.黄斑诊断报告概貌\n\nOCT RNFL视网膜神经纤维层检查ObservationOCTRNFL, OCT视网膜神经纤维层检查概貌，一般和诊断报告一起使用。\n\n\n\nName\nCardinality\nType\n\n\n\nstatus\n1..1\ncode(final or amended)\n\n\ncategory\n0..*\nCodeableConcept(ObservationCategoryCodes)\n\n\ncode\n1..1\nCodeableConcept(LOINCCodes)\n\n\nsubject\n1..1\nReference(Patient)\n\n\nbodySite\n0..1\nCodeableConcept\n\n\nbodySite.extension\n0..*\nExtension(bodySite)\n\n\nbodySite.extension.value[x]\n1..1\nReference(BodyStructure|Ocular anatomical location)\n\n\n表十二.OCT RNFL检查概貌\n\nOCT黄斑诊断报告OphthalDiagnosticReportOCTRNFL, OCT视网膜神经纤维层诊断报告。\n\n\n\nName\nCardinality\nType\n\n\n\nDiagnosticReport.category\n1..*\nCodeableConcept(service category)\n\n\nDiagnosticReport.code.coding\n1..1\nCoding(rnflCode)\n\n\nDiagnosticReport.code.coding.system\n1..1\nuri(固定值：http://loinc.org)\n\n\nDiagnosticReport.code.coding.code\n1..1\ncode(固定值：86291-2)\n\n\nDiagnosticReport.result\n0..*\n[Reference](Observation\n\n\n表十三.OCT RNFL诊断报告概貌\n\n外部链接\nEyes on FHIR Profiles\n\n","categories":["standard"],"tags":["FHIR, Standard,IG"]},{"title":"企业架构师 vs 解决方案架构师 vs 技术架构师","url":"/2020/11/14/architecture-2020-11-15-%E4%BC%81%E4%B8%9A%E6%9E%B6%E6%9E%84%E4%B8%8E%E4%BC%81%E4%B8%9A%E6%9E%B6%E6%9E%84%E5%B8%88/","content":"前言本文主要简介一下企业架构师EA, 技术架构师TA, 解决方案架构师SA的区别, 同时探讨各自需要具备的能力和挑战.\n企业架构师 vs 技术架构师 vs 解决方案架构师简单来说, 企业架构师从企业全局的角度出发发现和定义问题, 解决方案架构师把问题转化为一个解决方案, 技术架构师实现具体的解决方案.网上有一张图描述了EA, TA, SA三者的区别:\n上图从软件的生命周期, 细节的涉及度, 关注的程度这三维来度量和区别三者.\n**技术架构师Technical Architect(TA)**主要关注在某个技术实现.TA可能在企业中同时负责多个项目, 一般不会关注整个软件生命周期. TA需要有实际的编码能力, 为开发团队提供技术指导, 定义标准和最佳实际.TA的技术能力是考核他的一个关键指标, 所以大部分TA的技术栈比较专注,比如Java架构师, .NET架构师, IT基础架构师. 这些可以从上图的虚线可以看出.\n**解决方案架构师Solutions Architect(SA)**一般被分配到企业中的某个项目中, 以确保项目在每个生命周期中保持技术完整性, 目标和方案的一致性. SA一般不写代码, 因为协调各个技术活动是他的主要工作. SA需要参与计划(initiative)的所有方面和活动: 从概念定义, 到需求分析, 到实现, 再到把系统交付到运营和业务单元. 所以SA一般是一个通才, 为所有这些活动做出明智的贡献。 当然, 大部分项目不需要单独配备一名SA, 因为如果技术栈唯一, 那么一般只需要TA就能够解决问题. 但是如果和技术相关的风险是巨大的,那么配备一名SA是明智的. 这种情况具体可以被分为: 不确定的需求, 未被验证的技术实现或存在多个技术实现, 外包项目给离场开发团队等. \n**企业架构师Enterprise Architect(EA)**负责整个企业。她严格地描述企业的业务实体，它们的属性以及它们与外部环境之间的关系(竞争对手)。EA关心整个生命周期，以及每一项已采用或预期采用的实施技术。同样，她也研究各个项目，以确保整个企业具有完整性和一致性。但是，EA可能需要考虑的详细程度还很低而且很肤浅，因此她必须将除策略级别的决策之外的所有决策委派给特定工作的专家。EA的一个特例是企业IT架构师（EITA），他关注企业内部信息和技术的整体观点。但是，上面提到的有关EA角色的所有内容也适用于EITA角色。请注意，该框架未能在EA和EITA之间做出明显的区分。添加第四维会有所帮助，但是该图将变得混乱。\nSA的角色与职责由于TA, SA, EA的职责不同, 这里主要挑一下介于中间的SA的角色责任.\n技术领导SA需要时一名技术领导, 他需要在如下方面担任职责:\n\n架构设计\n开发支持\n监督与指导\n技术开荒\n需求管理\n\n技术顾问在业务层面, SA还需要作为技术顾问支持不同的业务单元:\n\n管理stakeholders\n参与售前\n参与架构评估\n面试候选人\n参与新架构发现\n提供高阶成本估算\n诊断测试阶段的问题\n\n业务分析&#x2F;产品OwnerSA还需要担任一部分BA的工作, 根据领域不同, 可能还需要一些行业知识:\n\n行业市场的知识, 如竞争对手, 法务法规的知识\n产品知识, 有产品和解决方案路线图, 懂产品策略\n懂用户\nbacklog管理\n给BA或PO提供其他建议\n\n部分开发职责SA的开发属性主要是为了做POC和code review.\nSA所需技能SA所需要的技能比较全面和高标准, 对个人的综合素质有较高的要求.\n展示能力\n公共演讲能力\n白板技能\n\n沟通能力\nstakeholder管理\n解释能力\n协商能力\n冲突管理\n社交\n\n时间管理\n优先级管理\n时间管理\n\n思维能力\n抽象思维\n战术与策略\n做决定\n评判思维\n抗压能力\n\n自我开发\n持续学习\n自律\n个人成长计划\n快速学习能力\n\n关系管理\n同理心\n情商\n\n编码能力\nPOC\n熟悉各个技术栈\n\n分析与设计\n能用不同设计解决不同的问题\n\n\n","categories":["architecture"],"tags":["architecture"]},{"title":"数据中台的价值","url":"/2021/01/01/architecture-2021-01-02-%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E7%9A%84%E4%BB%B7%E5%80%BC/","content":"企业为什么需要数据中台?\n与业务更加紧密\n提供数据服务\n提供业务价值\n快速开发数据服务\n围绕业务场景\n提供统一数据\n赋能业务更智能\n构建统一数据资产\n打通数据孤岛\n\n数据仓库, 数据湖, 数据中台的区别从不同使用对象的角度, 我们可以认为三种有本质的区别, 数据仓库+商业智能能够给决策者提供智能报表, 以协助决策者做出决策. 数据湖或者数据平台是以数据作为服务, 对数据分析师或者其他开发者提供数据集服务, 而数据中台则是直接面对业务, 提供的服务希望能够更加贴近业务, 提供数据产品, 一般以API的形式提供服务.\n从提供服务的出发点角度, 数据中台以业务需要什么服务为出发点, 而数据湖则是提供我有什么数据.\n从不同度量来衡量各个服务的不同, 数据中台关注于用户对数据服务的满意度, 而数据平台关注于数据的质量.\n客观事实: 市场的变化远远快于数据的采集速度业务数据化-&gt;数据业务化组织架构的调整中台架构的提出对企业的组织架构产生了巨大的影响，有了与中台相适应的组织架构，企业才能很好地完成中台建设并从中受益。中台架构有一很鲜明的特点，那就是它彻底破除了应用系统的边界，从企业的全业务领域着手，切分出业务中心，每一个业务中心所支撑的不是一个孤立的应用系统，而是企业在该领域的全部核心业务，所以每一个业务中心都需要非常专业的团队来负责，团队必须对这部分业务非常了解，而且必须站在企业的全局去支撑和把控这一业务领域。\n","categories":["architecture"],"tags":["architecture"]},{"title":"业务架构与设计原则","url":"/2020/12/27/architecture-2020-12-08-bussiness-architecture/","content":"企业业务架构企业的业务架构定义了企业的结构，这个结构包含了企业的治理结构，业务流程，服务和产品，业务信息和利益相关者。业务架构为实现企业战略目标勾画出一个可行的、方向性的系统。所以简单地可以认为企业业务架构是一个组织工作的蓝图。通过使用组织架构，企业可以以较小的代价和时间来面对挑战和处理问题，这是因为我们已经知道业务中所有重要的依赖，关系和信息流。企业不管大小都应该有业务架构。 业务架构不一定需要全放面描述或者按照一定的标准创建。设计和描述组织的工作是复杂和困难的， 所以我们需要一些工具来帮助加速这一过程。\n开源工具\nCollection 常见设计原则 https://principles.design/\nArchi 架构图工具 https://www.archimatetool.com/\nCausal Loop Diagram 流程图工具。 https://nocomplexity.com/causalloopdiagram/\nCamunda Modeler 用于编辑BPMN(Business Process Model and Notation)的桌面工具。https://camunda.com/download/modeler/\nProtege 用于规划拓扑图 https://protege.stanford.edu/\nDrawIO 是一个线上项目用于创建流程图。类似国内的有processon. https://app.diagrams.net/\n\n业务架构设计原则有一个清晰的业务原则对于成功的企业至关重要。有原则也是在限定的时间和资源下构建好的架构的基础，这些原则需要所有的利益相关者都参与和同意。\n一些常见的原则solution space解决的空间**原则：**永不尝试使用技术的手段解决非技术的问题。**原因：**技术当然可以帮助解决一些问题， 但是技术永远不可能是完备的手段。任何非技术的活动，过程，行为改变等等都可能是解决问题的手段。**言下之意：**解决问题的时候不要仅仅盯着技术。\nstart simple简单先行**原则：**简单先行 **原因：**简单开发一个产品其实是赢得对于构建下一步产品的权利。**言下之意：**当投入大量时间和金钱时，复杂性就会出现，简单的调整也会产生更大的影响。当产品变得成熟时，修复向后兼容性的问题将更加复杂。\n快速构建MVP（Minimal Viable Product)**原则：**如果你的MVP需要一年的时间构建， 那么这不是MVP **原因：**最好MVP不要超过一个月构建。**言下之意：**MVP不是用来贩卖的，而是从后续的阶段中学习如何贩卖。\nmake it easy first then make it fast先简单做然后快速做**原则：**先简单做， 然后快速做，最后做的漂亮. **原因：**开发新项目的成本是很高的，做一个优秀的产品更加复杂和昂贵。所以在做MVP的时候，尝试着让产品易用和易改。 **言下之意：**一些性能之类的问题可以后面再考虑。\nuse open data, open standards, open source and open innovation使用公开数据，公开标准，公开源代码，公开创意 **原则：**use open data, open standards, open source and open innovation.  **原因：**这个原则提供了一个框架使用公开的方式去技术赋能开发。 言下之意： \n\n采用并扩展存在的公开标准\n尽量以API的方式开放数据和功能， 越大的社区使用你的产品越好\n把投资软件当做公益\n尽可能的开源代码\n\nStrategic focus焦距策略 **原则：**投资决策受业务需求驱动。 **原因：**一个业务领头和面向业务的架构在满足战术目标，不断改变的需求和客户期待上更可能成功。 **言下之意：**架构需要完全和整个公司的战略目标对齐。\nmake things open保持分享 **原则：**make things open: it makes things better. **原因：**尽可能地分享我们在做的东西。和同事， 和用户，和世界分享代码，分享设计，分享想法，分享创意，分享失败。越多的人关注你越可能你的产品可以成功。\nmaximise benefit to the enterprise最大化企业的利益 **原则：**信息管理的决定是为了最大化企业的利益。 **原因：**这个原则内嵌了“服务高于个人”的意思。从企业的角度做决定永远比从组织的角度出发对企业有更长远的价值。 **言下之意：**达到最大化企业的利益需要我们修正计划和管理信息的方式。技术不能完全解决这个改变。所以组织可能需要做一些牺牲来满足企业利益， 比如调整开发的优先级，改变一些喜欢的工具等等。\nReliability可靠性 **原则：**信息系统需要可靠，准确，及时。\nReuse and Improve复用与提高 **原则：**复用与提高  **原因：**避免资源浪费，只有在不满足需求的时候才升级原有的解决方案。 言下之意： \n\n尽可能使用、修改、扩展现有工具，平台和框架。\n开发模块化的软件是提倡的方式。\n\nReuse before buy, buy before build**原则：**尽可能复内部it资产，不满足的情况下才考虑买外部IP, 最后才是定制化构建新的产品。原因： \n\n购买标准的IT解决方案只要它们不是快要淘汰都比自定制化便宜， 并且享有后续维护。\n定制化的产品后续的维护可能是否昂贵。言下之意： \n为了保证IT资产能够尽可能复用，业务单元必须保证治理部门不认为业务实践与行业标准实践有重大的不一致。\n有些商业化的软件是可以配置的，需要评估配置功能的复杂度是否和自己定制的复杂度相近。\nlicense合规性也是需要考虑的。\n\nUser Experience Continuity**原则：**我们交付的产品要尽可能保持体验的延续性。**原因：**用户不管在公司内部还是外部都会使用不同应用，有不同的使用体验, 我们不想让用户使用自己的产品有使用20年前产品的感觉。言下之意： \n\n主动关注身边发生的新技术和新方法。\n保持好奇心，敢于尝试新鲜事物，积极听取他人意见或建议。\n积极采用新颖的方式来处理问题及机会。\n不断对现状提出问题，挑战传统的工作方法和思维方式。\n敢于为制定新政策、采取新措施或尝试新方法承担可控风险。\n\nRoutine Tasks are automated where appropriate 尽可能自动化日常活动 **原则：**尽可能自动化日常活动  **原因：**自动化节约人力，提高效率和有更高的容错。 言下之意： \n\n需要专业知识去分析可以自动化流程的过程。\n不是日常的任务不需要自动化。\n自动化流程可以是一个流程接着另一个流程， 业务单元需要集成到整个工作流当中。\n\nGive before receiving **原则：**先于奉献  **原因：**奉献是建立一段关系的真正方式。仅仅关注于从联系中可以得到什么是不能构建相互共赢和可持续关系的。 **言下之意：**维护和利益相关者的关系是十分必要的。\nEveryone matters in Information management 信息管理和所有人有关 **原则：**组织中所有参与信息管理决策的人都需要参与完成业务指标  **原因：**信息系统的用户是关键利益相关者，是实际使用技术解决业务需求的人。为了保障信息系统和业务对齐，所有企业中的组织都需要参与到信息系统环境中的各方面。 **言下之意：**为了整体上作为一个团队，利益相关者需要接受开发信息系统环境的责任。保障必要的资源来完成这条原则。\nBe Collaborative **原则：**相互协助  **原因：**If you want to go fast, go alone. If you want to go far, go together. 言下之意： \n\n让各个不同领域的专家参与项目\n跨部门合作\n把好的工作经验，结果，进展记录下来，并且分享它们。\n\n","categories":["architecture"],"tags":["architecture"]},{"title":"微软的软件架构设计原则","url":"/2021/03/19/architecture-2021-03-20-%E5%BE%AE%E8%BD%AF%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/","content":"构架原则微软官网有一个篇短文专门讲解Web架构设计原则site. 且把它翻译一下, 品位下.\n常见的设计原则Separation of ConcernsSeparation of Concerns字面翻译是关注点的分离, 其实很多情况下, 我们的架构设计都遵循这个原则. 这个原则指出软件应该根据工作类型的不同被分离. 这个原则是Edsger W. Dijkstra在论文On the role of scientific thought提出的. 原则上将一个计算机程序分割成不同模块的程序, 分割一个程序的不同关注点,一个模块只关心一个点. 同时业务行为设计也要和基础架构, UI展示层分离. 这样业务逻辑的分离可以保证业务可以被单独测试和进化, 而不是和其他实现紧密耦合. \nEncapsulation 封装应用的不同部分需要使用封装来隔离, 而通过外部接口保持直接的协作不受内部实现的改变的影响.\nDependency inversion 依赖反转\n高层次的模块不要依赖于低层次的模块，都应该依赖于抽象(接口)。\n抽象(接口)不应该依赖于具体，而具体要依赖于抽象。普通类依赖关系是直接依赖, 如下图:\n\n\n但是为了系统松耦合, 可测试, 模块化和可维护, 使用依赖反转原则将类的依赖反转, 使原本依赖类B的类A依赖自己控制的接口A, 而类B也依赖接口A. 这样就实现了依赖反转. 下图比微软官网更加清晰:\n![](/images/screenshots/screen_shot_2021-03-19.png)\n这样的实现, 虽然在代码实现时需要多余的实现, 但是在运行时它的依赖关系并没有改变. 依赖反转原则也为依赖注入提供了可能.\n\nExplicit dependencies 显式依赖这条原则也很明显\nSingle responsibility 单一责任单一责任是针对面向对象设计, 对象只有一个责任, 也只有一个理由做改变. 也就是说只有当对象的责任发生改变时, 对象的才能被改变. 有了这个原则, 当一些行为需要新的实现时, 就应是写新的类, 而不是在原有的类上增加代码. 采用这个原则, 我们的应用也可以往微服务的方向发展.\nDon’t repeat youself (DRY)“干”的原则时很常见. 保持只有一个true source很重要.\nPersistence ignorance 持久化无关代码和持久化技术无关.\nBounded contexts有界上下文是领域驱动设计的核心模式.  它们可以将大型应用程序或组织分解为独立的概念模块，通过这种方式来解决复杂性问题。 每个概念模块表示各自独立的上下文（因此有界），并且可以独立改进。 理想情况下，每个有界上下文都应该能够为其中的概念自由选择它自己的名称，并对其自己的持久性存储具有独占访问权限。\n","categories":["architecture"],"tags":["architecture"]},{"title":"Hadoop MapReduce 实现","url":"/2018/10/12/bigdata-2018-10-13-hadoop-mapreduce-%E5%AE%9E%E7%8E%B0-md/","content":"虽然Hadoop现在有一点过时, 但是一般的金融公司还是会用它出隔日的报表(离线计算), 本文主要关注其Map Reduce的实现.\n函数编程Map/Reduce的思想借鉴于函数式编程. Map是进行过滤和排序, 比如把一组学生按名字排序到队列, 一个名字一个队列, 然后Reduce方法进行总结, 比如对队列里的名字做统计, 得出名字出现频率. 这种思想是split-apply-combine的一种特例(见pandas groupby).\n我们知道多线程编程的局限在于访问共享资源的竞争问题, 一般需要锁, 信号量(semaphore)等技术去协调, 不然死锁等问题将会出现.\n但是我们可以完全换个思路, 比如消除需要访问共享资源的限制, 这样我们就不需要锁之类的技术了.这也是函数计算的一个基本概念. 数据通过函数的参数传递, 同一时间只有一个激活的函数运行,这样就避免了冲突.\n可以把函数连接作有向无环图Direct Acylic Graph, 由于函数没有隐藏的依赖, 这样多个DAG就可以并行运行.\nMap&#x2F;Reduce函数Map/Reduce是一种特殊(简单)的DAG. 图如下所示: 每个map函数把一组数据按key分为key/value对, 然后不同key的元素跑到不同的计算节点, 在那里进行reduce合并.\n\nmap(input_records) &#123;emit(k1, v1)...emit(k2, v2)...\nreduce(key, values) &#123;aggregate = initialize()while (values.has_next)&#123;    aggregate = merge(values.next)&#125;collect(key, aggregate)&#125;\n可以有多个`map/reduce`组合替代一个并行的算法:\n![https://s3.ap-southeast-1.amazonaws.com/kopei-public/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-14%20%E4%B8%8B%E5%8D%884.25.43.png](https://s3.ap-southeast-1.amazonaws.com/kopei-public/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-14%20%E4%B8%8B%E5%8D%884.25.43.png)\n\n分布式文件系统(HDFS)Hadoop需要分布式文件系统, 用于处理大文件的顺序读写.每一个大文件会被分割成块, 存储在不同数据节点.主节点NameNode会记录所有文件的目录结构和各个块所在的位置. 主节点作为中心控制点一般会有hot standby的复制.\n想要读取文件, 客户端会计算所需块在文件的偏移位置, 得出块的索引, 然后对NameNode做出请求, 然后NameNode会返回哪个DataNode有数据, 客户端就会直接和DataNode联系.\n想要写入一个文件, 客户端会先和NameNode通信, 作为响应, NameNode会告诉客户端现在有哪些DataNode并且谁是主节点和哪些是从复制. 然后客户端就会把文件上传到所有DataNode, 不过DataNode这时还只会存储在buffer, 等到所有节点都存完缓存, 客户端发起commit给主节点, 主节点就会提交更新, 同时通知从节点更新, 等到所有从节点都commit, 主节点就会返回客户端提交成功. (所以DFS写是强一致性) 最后客户端还需告诉NameNode所有更新信息. 包括块分布的位置和元信息都会写入NameNode的操作日志operation log. 这个日志十分重要, 可以用于灾后恢复. NameNode也会通过不间断地checkpoint维护它的持久化状态.\n当NameNode挂了, 所有的写操作将会失效, 读操作可能不受影响, 只要在客户端与DataNode的句柄有效. 需要恢复NameNode, 从节点会从上一次的checkpoint状态恢复, 并做操作日志回放.\n当一个DataNode挂了, NameNode会从心跳中检查到, NameNode就会把它从集群中移除, 然后把它存储的chunk在其他节点写入. 这样做才能维护hadoop所需的replication factor.\n如果这个挂掉的DataNode后来恢复了, 那么将会重新加入集群, 它会给NameNode报告所有它有的块, 每一个块是有版本号的, 所以NameNode可以检查是否这个DataNode数据是否已经过时, 如果是那么这个节点将会被后续回收.\n","categories":["big data"],"tags":["big data"]},{"title":"Spark RDD转成Dataset的两种方式","url":"/2020/07/10/bigdata-2020-07-11-create-dataframe-from-rdd-in-spark/","content":"RDD to DatasetsSpark SQL支持两种方式把RDD转为Datasets. 第一种是使用反射reflection取得到RDD的schema, 这种方式需要预先知道数据的结构。如果是scala的接口，RDD包含case class(定义了表的结构)可以自动转化RDD到dataframe。第二种方式是通过可编程接口对运行时的RDD进行构建datasets的schema， 这种方法更加动态。当case classes没有预先定义（比如，记录的结构被编码成了字符串）,一个DataFrame可以通过如下三步创建：\n\n从原RDD创建一个新的RDD Rows;\n通过创建StructType来代表结构和第一步的Rows对应上去.\n通过SparkSession.createDataFrame方法把结构应用到Rows上.\n\n","categories":["big data"],"tags":["big data"]},{"title":"Pandas UDF and Function Api in Spark","url":"/2020/07/22/bigdata-2020-07-23-pyspark-for-pandas/","content":"Apache Arrow in PySparkSpark可以使用Apache Arrow对python和jvm之间的数据进行传输， 这样会比默认传输方式更加高效。为了能高效地利用特性和保障兼容性，使用的时候可能需要一点点修改或者配置。\n为什么使用Arrow作为数据交换中介能够提升性能？普通的python udf需要经过如下步骤来和jvm交互：\n\njvm中一条数据序列化\n序列化的数据发送到python进程\n记录被python反序列化\n记录被python处理\n结果被python序列化\n结果被发送到jvm\njvm反序列化并存储结果到dataframe\n\n所以python udf会比java和scala原生的udf慢。但是使用pandas udf可以克服数据传输中需要的序列化问题，关键是使用了Arrow. spark使用arrow把JVM中的Dataframe转为可共享的buffer, 然后python也可以把这块共享buffer作为pandas的dataframe, 所以python可以直接在共享内存上操作。以上，我们总结一下，使用arrow主要有两个好处：\n\n因为直接使用了共享内存，不在需要python和jvm序列化和反序列化数据。\npandas有很多使用c实现的方法， 可以直接使用。\n\nSpark DataFrame和Pandas DataFrame的转化首先需要配置spark, 设置spark.sql.execution.arrow.pyspark.enabled, 默认这个选项是不打开的。还可以开启spark.sql.execution.arrow.pyspark.fallback.enabled来避免如果没有安装Arrow或者其它相关错误。Spark可以使用toPandas()方法转化为Pandas DataFrame; 而使用createDataFrame(pandas_df)把Pandas DataFrame转为Spark DataFrame.\nimport numpy as npimport pandas as pdspark.conf.set(&#x27;spark.sql.execution.arrow.pyspark.enabled&#x27;, &#x27;true&#x27;)pdf = pd.DataFrame(np.random.rand(100,3))df = spark.createDataFrame(pdf)# 使用arrow把spark df转化为pandas dfresult_pdf = df.select(&quot;*&quot;).toPandas()\n\nPandas UDF(矢量UDF)Pandas UDF是用户定义的函数， Spark是用arrow传输数据并用pandas来运行pandas UDF， pandas UDF使用向量计算，相比于旧版本的row-at-a-timepython udf, 最多增加100倍的性能. 使用pandas_udf修饰器装饰函数，就可以定义一个pandas UDF.对spark来说，UDF就是一个普通的pyspark函数。从spark3.0开始， 推荐使用python类型(type hint)来定义pandas udf.定义类型的时候，StructType需要使用pandas.DataFrame类型， 其他一律使用pandas.Series类型。\nimport pandas as pdfrom pyspark.sql.functions import pandas_udf@pandas_udf(&quot;col1 string, col2 long&quot;)def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -&gt; pd.DataFrame:  s3[&#x27;col2&#x27;] = s1+s2.str.len()  s3[&#x27;col1&#x27;] = &#x27;sss&#x27;  return s3  df = spark.createDataFrame(      [[1, &quot;a string&quot;, (&quot;a nested string&quot;,)]],      &quot;long_col long, string_col string, struct_col struct&lt;col1:string&gt;&quot;)      df.printSchema()df.select(func(&quot;long_col&quot;, &quot;string_col&quot;, &quot;struct_col&quot;)).printSchema()df.select(func(&quot;long_col&quot;, &quot;string_col&quot;, &quot;struct_col&quot;)).show()     +--------------------------------------+|func(long_col, string_col, struct_col)|+--------------------------------------+|                              [sss, 9]|+--------------------------------------+\n\nSeries to Series 类型的UDF当类型提示可以被表达为pandas.Series -&gt; pandas.Series时，称为Series to SeriesUDF这种类型的pandas UDF的输入和输出必须要有相同的长度， PySpark会把数据按列分成多个batch, 然后对每个batch运行pandas UDF, 然后组合各自的结果。\n&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; from pyspark.sql.functions import col, pandas_udf&gt;&gt;&gt; from pyspark.sql.types import LongType&gt;&gt;&gt; def multiply_func(a: pd.Series, b:pd.Series) -&gt; pd.Series:...     return a*b... &gt;&gt;&gt; multiply = pandas_udf(multiply_func, returnType=LongType())&gt;&gt;&gt; x = pd.Series([1,3,4,5])&gt;&gt;&gt; df = spark.createDataFrame(pd.DataFrame(x, columns=[&#x27;x&#x27;]))&gt;&gt;&gt; df.select(multiply(col(&#x27;x&#x27;),col(&#x27;x&#x27;))).show()+-------------------+|multiply_func(x, x)|+-------------------+|                  1||                  9||                 16||                 25|+-------------------+\n\nSeries迭代器 -&gt; Series迭代器 类型的UDF当类型提示可以被表达为Iterator[pandas.Series] -&gt; Iterator[pandas.Series]时，称为Iterator[Series] to Iterator[Series]UDF.\nfrom typing import Iteratorimport pandas as pdfrom pyspark.sql.functions import pandas_udf&gt;&gt;&gt; pdf = pd.DataFrame([1,2,3], columns=[&#x27;x&#x27;])&gt;&gt;&gt; df = spark.createDataFrame(pdf)&gt;&gt;&gt; dfDataFrame[x: bigint]&gt;&gt;&gt; @pandas_udf(&#x27;long&#x27;)... def plus_one(iterator: Iterator[pd.Series]) -&gt; Iterator[pd.Series]:...     for x in iterator:...             yield x+1... &gt;&gt;&gt; df.select(plus_one(&#x27;x&#x27;)).show()+-----------+|plus_one(x)|+-----------+|          2||          3||          4|+-----------+\n\n多个Series迭代器 -&gt; Series迭代器 类型的UDF当类型提示可以被表达为Iterator[Tuple[pandas.Series,...]] -&gt; Iterator[pandas.Series]时，称为Iterator[Tuple[pandas.Series,...]] to Iterator[Series]UDF.\n&gt;&gt;&gt; from typing import Iterator, Tuple&gt;&gt;&gt; @pandas_udf(&#x27;long&#x27;)... def multiply_two_cols(...     iterator: Iterator[Tuple[pd.Series, pd.Series]]) -&gt; Iterator[pd.Series]:...     for a,b in iterator:...             yield a*b... &gt;&gt;&gt; df.select(multiply_two_cols(&#x27;x&#x27;,&#x27;x&#x27;)).show()+-----------------------+|multiply_tow_cols(x, x)|+-----------------------+|                      1||                      4||                      9|+-----------------------+\n\nSeries -&gt; Scalar 类型的UDF当类型提示可以被表达为pandas.Series -&gt; Scalar时，称为Series to ScalarUDF.Scalar具体的类型必须是原生python类型如int, float等等， 或者是numpy的数据类型如numpy.int64, numpy.float64这种UDF可以被用于groupBy(), agg(), pyspark.sql.Window.\n&gt;&gt;&gt; from pyspark.sql import Window&gt;&gt;&gt; df = spark.createDataFrame([(1,1.0), (1,2.0),(2,3.0),(2,4.0),(2,10.0)], (&#x27;id&#x27;,&#x27;v&#x27;))&gt;&gt;&gt; dfDataFrame[id: bigint, v: double]&gt;&gt;&gt; @pandas_udf(&#x27;double&#x27;)... def mean_udf(v: pd.Series) -&gt; float:...     return v.mean()... &gt;&gt;&gt; df.select(mean_udf(&#x27;v&#x27;)).show()+-----------+|mean_udf(v)|+-----------+|        4.0|+-----------+&gt;&gt;&gt; df.groupby(&#x27;id&#x27;).agg(mean_udf(&#x27;v&#x27;)).show()+---+-----------------+| id|      mean_udf(v)|+---+-----------------+|  1|              1.5||  2|5.666666666666667|+---+-----------------+&gt;&gt;&gt; dfDataFrame[id: bigint, v: double]&gt;&gt;&gt; df.show()+---+----+| id|   v|+---+----+|  1| 1.0||  1| 2.0||  2| 3.0||  2| 4.0||  2|10.0|+---+----+&gt;&gt;&gt; w = Window.partitionBy(&#x27;id&#x27;).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)&gt;&gt;&gt; df.withColumn(&#x27;mean_v&#x27;, mean_udf(&#x27;v&#x27;).over(w)).show()+---+----+-----------------+                                                    | id|   v|           mean_v|+---+----+-----------------+|  1| 1.0|              1.5||  1| 2.0|              1.5||  2| 3.0|5.666666666666667||  2| 4.0|5.666666666666667||  2|10.0|5.666666666666667|+---+----+-----------------+\n\nSpark的Pandas函数APISpark有一些函数可以让python的函数通过pandas实例直接用在spark dataframe上。内部机制上类似pandas udf, jvm把数据转成arrow的buffer, 然后pandas可以直接在buffer上操作。但是区别是，这些函数api使用起来就像普通pyspark api一样是作用在dataframe上的, 而不像udf那样作用于一个column. 实际使用的时候，一般是DataFrame.groupby().applyInPandas()或者DataFrame.groupby().mapInPandas()\nGrouped Map apiSpark的dataframe在groupby后使用普通的pandas函数， 如df.groupby().applyInPandas(func, schema))， 普通的pandas函数需要输入是pandas dataframe, 返回普通的pandas dataframe. 上面这写法会把每个分组group映射到pandas dataframe.df.groupby().applyInPandas(func, schema))过程其实分为三步， 典型的split-apply-combine模式：\n\nDataFrame.groupBy分组数据\n分组的数据映射到pandas dataframe后，apply到传入的函数\n组合结果成一个新的pyspark Dataframe使用groupBy().applyInPandas(), 用户需要做两件事：\n写好pandas函数\n定义好pyspark dataframe结果的schema\n\n&gt;&gt;&gt; def subtract_mean(pdf):...     v = pdf.v...     return pdf.assign(v=v-v.mean())... &gt;&gt;&gt; df.groupby(&#x27;id&#x27;).applyInPandas(subtract_mean,schema=&#x27;id long, v double&#x27;).show()+---+------------------+                                                        | id|                 v|+---+------------------+|  1|              -0.5||  1|               0.5||  2|-2.666666666666667||  2|-1.666666666666667||  2| 4.333333333333333|+---+------------------+\n\nMap api也可以对pyspark dataframe和pandas dataframe做map操作，DataFrame.mapInPandas()是对当前的DataFrame的取一个迭代器映射普通到pandas函数。这个普通pandas函数必须是输入输出都是pdf.\n&gt;&gt;&gt; def filter_func(iterator):...     for pdf in iterator:...             yield pdf[pdf.id == 1]... &gt;&gt;&gt; df.mapInPandas(filter_func, schema=df.schema).show()+---+---+| id|  v|+---+---+|  1|1.0||  1|2.0|+---+---+\n\nCo-grouped Map api这个api可以使两个pyspark dataframe组合后使用pandas函数\nimport pandas as pddf1 = spark.createDataFrame(    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],    (&quot;time&quot;, &quot;id&quot;, &quot;v1&quot;))df2 = spark.createDataFrame(    [(20000101, 1, &quot;x&quot;), (20000101, 2, &quot;y&quot;)],    (&quot;time&quot;, &quot;id&quot;, &quot;v2&quot;))def asof_join(l, r):    return pd.merge_asof(l, r, on=&quot;time&quot;, by=&quot;id&quot;)df1.groupby(&quot;id&quot;).cogroup(df2.groupby(&quot;id&quot;)).applyInPandas(    asof_join, schema=&quot;time int, id int, v1 double, v2 string&quot;).show()# +--------+---+---+---+# |    time| id| v1| v2|# +--------+---+---+---+# |20000101|  1|1.0|  x|# |20000102|  1|3.0|  x|# |20000101|  2|2.0|  y|# |20000102|  2|4.0|  y|# +--------+---+---+---+\n","categories":["big data"],"tags":["big data"]},{"title":"Arrow and Pyarrow","url":"/2020/08/19/bigdata-2020-08-20-arrow-and-pyarrow/","content":"前言Apache Arrow是一个用于内存分析的跨语言开发平台。它定义了一种标准的、语言无关的列式内存数据格式。这种格式支持平整的和嵌套的数据结构。它还提供了一些计算库，零拷贝流式消息和内部进程通信。Arrow的主要用处可以是大数据的快速移动和处理。由于是开发平台，Arrow包含了许多组件：\n\nArrow列式内存格式：一个标准和高效的内存表示。可用于平的和嵌套的数据结构，做到了语言无关。\nArrow IPC格式：一种高效的序列化格式，并且带有元信息，可用于进程和异构环境间的通信\nArrow Flight RPC协议：基于Arrow IPC格式，用于远程服务交换arrow数据与应用定义的语义数据\nC++, C, C#, Go, Python, Matlib, Java等等库\nGrandiva: 一个LLVM编译器\nPlasma对象存储：一个共享内存blob存储\n\n本文主要展示一些python的实践案例和源码解读， 希望能够总结以期有进一步了解Arrow.\nDive into下面让我们去看一看pyarrow的源代码（1.0.0)\npyarrow项目目录![](&#x2F;images&#x2F;screenshots&#x2F;Screen Shot 2020-08-28 at 3.53.07 PM.png)\npip安装的pyarrow少了一些cython编写的pyx代码，这些文件被编译成pxd或so后可以被py代码import, 比如from pyarrow.lib import (ChunkedArray, RecordBatch, Table)是从lib.so中导入的。\npyarrow._init_.py源码解读首先导入版本号，如果不是通过包安装，那么版本通过解析git describe确定版本。接着导入cython的pyarrow.lib库，由于Cython有个bug(https://github.com/cython/cython/issues/3603), 这里暂时关掉gc。然后有一个show_versions的函数可以查看c++版本信息：\n&gt;&gt;&gt; pa.show_versions()pyarrow version info--------------------Package kind: manylinux2010Arrow C++ library version: 1.0.0Arrow C++ compiler: GNU 8.3.1Arrow C++ compiler flags:  -fdiagnostics-color=always -O3 -DNDEBUGArrow C++ git revision: b0d623957db820de4f1ff0a5ebd3e888194a48f0Arrow C++ git description: apache-arrow-0.16.0-1340-gb0d623957\n然后导入Cython定义的各种类型，导入buffer和IO相关。关于Pyarrow的memory和IO, 下面会介绍。导入异常，导入序列化相关，到这lib模块导入完毕。然后从hdfs.py，ipc.py, filesystem.py, serialization.py,types.py导入相关模块，定义启动plasma server入口函数和一些其他的包工具函数。\npyarrow的内存和IO管理本节主要总结pyarrow的内存管理和IO管理，涉及buffer, memory pool和file-like&#x2F;stream-like对象\n访问和分配内存在pyarrow.__init__.py可以看到代码的引入:\n# Buffers, allocationfrom pyarrow.lib import (Buffer, ResizableBuffer, foreign_buffer, py_buffer,                         Codec, compress, decompress, allocate_buffer)from pyarrow.lib import (MemoryPool, LoggingMemoryPool, ProxyMemoryPool,                         total_allocated_bytes, set_memory_pool,                         default_memory_pool, logging_memory_pool,                         proxy_memory_pool, log_memory_allocations,                         jemalloc_set_decay_ms)\npyarrow.BufferBuffer对象是C++代码arrow::Buffer的封装，作为基础工具管理C++中的arrow内存。一个buffer代表一段连续的内存空间。大部分buffer拥有他们各自的内存，但是也有例外。Buffer对象可以允许高级array类安全地和属于或不属于他们的内存交互。arrow::Buffer允许一个buffer访问另一个buffer通过zero-copy, 同时保持内存的生命周期和清晰的父子关系。arrow::Buffer有很多种实现，但是对外接口是一致的：一个数据指针和长度。有点类似python自带的buffer和memoryview对象\n# https://github.com/apache/arrow/blob/67983cf56f/python/pyarrow/io.pxi&gt;&gt;&gt; import pyarrow as pa&gt;&gt;&gt; data=b&#x27;aaaaaaaaaaaaaaaaaaaaaa&#x27;&gt;&gt;&gt; buf = pa.py_buffer(data)  # buf是zero-copy的data对象memory view, buf不会分配内存.&gt;&gt;&gt; buf&lt;pyarrow.lib.Buffer object at 0x7fabc0e05d30&gt;&gt;&gt;&gt; buf.size22&gt;&gt;&gt; buf.to_pybytes()  # 这个转化是会复制数据。b&#x27;aaaaaaaaaaaaaaaaaaaaaa&#x27;\n外部的内存，只要有指针和size，保持接口一致，也可以通过foreign_buffer()来访问。在创建buffer之后，可以通过memoryview或python buffer装换，这种转化是zero-copy.\nMemory Pools所有内存分配和释放(malloc&#x2F;free)都可通过arrow::MemoryPool来追踪。代码在memory.pxi\n&gt;&gt;&gt; import pyarrow as pa&gt;&gt;&gt; pa.total_allocated_bytes()0&gt;&gt;&gt; buf = pa.allocate_buffer(1024,resizable=True)&gt;&gt;&gt; pa.total_allocated_bytes()1024&gt;&gt;&gt; buf.resize(2048)&gt;&gt;&gt; pa.total_allocated_bytes()2048&gt;&gt;&gt; buf=None&gt;&gt;&gt; pa.total_allocated_bytes()0\n\n输入输出Arrow C++库有几个抽象接口用于不同IO类型：\n\n只读流\n随机可访问只读文件\n只写流\n随机可访问只写文件\n可读可写可随机访问文件\n\n在pyarrow.__init__.py可以看到代码的引入:\n# I/Ofrom pyarrow.lib import (HdfsFile, NativeFile, PythonFile,                         BufferedInputStream, BufferedOutputStream,                         CompressedInputStream, CompressedOutputStream,                         TransformInputStream, transcoding_input_stream,                         FixedSizeBufferWriter,                         BufferReader, BufferOutputStream,                         OSFile, MemoryMappedFile, memory_map,                         create_memory_map, have_libhdfs,                         MockOutputStream, input_stream, output_stream)from pyarrow.lib import (ChunkedArray, RecordBatch, Table, table,                         concat_arrays, concat_tables)\n\n为了能够和python自带file对象行为一致，arrow定义了NativeFile(其实是个stream).代码在io.pxiNativeFile是所有arrow流的基类，arrow流可以是可读，可写，也可以支持seek.NativeFile暴露的方法用于读写python的数据对象，然后把他们变成stream传递给其他arrow工具，比如Arrow IPC.cython代码中定义了好几种NativeFile子类:\n\nOSFile, 使用操作系统的描述符\nMemoryMappedFile, 使用memory maps做zero-copy读和写。\nBufferReader, 把对象转成arrow buffer使用Zero-copy reader\nBufferOutputStream, 内存中写数据，最后生成buffer\nFixedSizeBufferWriter, 再一句生成的buffer中写数据\nHdfsFile, hadoop生态读写数据\nPythonFile, 在C++中交互python文件对象,可以对python文件对象使用c++的方法，但是可能有GIL的限制。\nCompressedInputStream and CompressedOutputStream, 从流中压缩和解压数据。\n\n高级APIinput streamsinput_streams()函数可以从各种输入创建可读NativeFile\n&gt;&gt;&gt; buf = memoryview(b&#x27;some data&#x27;)&gt;&gt;&gt; stream = pa.input_stream(buf)&gt;&gt;&gt; stream.read(4)b&#x27;some&#x27;&gt;&gt;&gt; stream.read()b&#x27; data&#x27;&gt;&gt;&gt; stream.read()b&#x27;&#x27;\n\noutput streams同理，output_stream把stream写成文件。\n&gt;&gt;&gt; with pa.output_stream(&#x27;example1.dat&#x27;) as stream:...     stream.write(b&#x27;some data&#x27;)... 9&gt;&gt;&gt; f = open(&#x27;example1.dat&#x27;, &#x27;rb&#x27;)&gt;&gt;&gt; f.read()b&#x27;some data&#x27;\n\nOSFile和Memory Mapped Files对于在磁盘上的文件读写，pyarrow提供标准系统级别的文件api和memory-mapped文件。memory-mapped是在用户态创建虚拟空间来映射磁盘上的内容。通过对這段虚拟内存的讀取和修改, 实现对文件的讀取和修改。使用虚拟内存映射进行文件读写有几个好处：\n\n可以不用读取整个文件进入物理内存，文件已经在虚拟内存中\n可以用对内存的操作命令来操作文件，更加高效\n由于实际上这个mapped文件还是文件，与进程无关，所以这段虚拟内存可以共享给多个进程。\n\n&gt;&gt;&gt; mmap = pa.memory_map(&#x27;example1.dat&#x27;)&gt;&gt;&gt; mmap.read()b&#x27;some data&#x27;&gt;&gt;&gt; mmap.seek(5)5&gt;&gt;&gt; buf=mmap.read_buffer(4)  # read into arrow buffer&gt;&gt;&gt; buf.to_pybytes()b&#x27;data&#x27;\n\n内存中buffer读写&gt;&gt;&gt; writer = pa.BufferOutputStream()&gt;&gt;&gt; writer.write(b&#x27;hello, friends&#x27;)&gt;&gt;&gt; buf = writer.getvalue()&gt;&gt;&gt; reader = pa.BufferReader(buf)&gt;&gt;&gt; reader.seek(7)&gt;&gt;&gt; reader.read(7)b&#x27;friends&#x27;\n\nplasmaplasma是arrow的一个共享对象存储，plasma只能用在单机上，客户端和服务端使用unix domain socket通信。plasma中的对象是不可变的。\npyarrow.plasma源码这个文件一上来要导入TensorFlow相关库，暂时跳过。主要功能函数式，用来启动plasma server.\ndef start_plasma_store(plasma_store_memory,                       use_valgrind=False, use_profiler=False,                       plasma_directory=None, use_hugepages=False,                       external_store=None):# plasma_store_memory定义存储大小# use_valgrind定义是否使用valgrind和use_profiler互斥# use_profiler定义是否测试性能# plasma_directory定义mmap文件位置# use_hugepages是否使用大文件存储，需要划分文件格式# external_store溢出的对象存储到外部位置，由于plasma超过预设空间时候会溢出对象。 \n这个函数会默认创建/tmp/test_plasma-plasma.sock用于客户端sock连接, 然后就是普通的shell命令行选取参数启动server.\n使用plasma共享pandas dataframe由于arrow支持平整或嵌套的数据结构，尤其适合pandas dataframe或者numpy(使用tensor), 然后我们可以把arrow格式的数据持久化到plasma用于共享对象，实现数据的高效读取。核心代码就三步，1.创建plasma对象，2.存入转为recordbatch的dataframe, 3.读取plasma的dataframe对象。\n# basic plasma clientclass PlasmaClient:    def __init__(self, location=&quot;plasma&quot;, *args, **kwargs):        self.client = plasma.connect(location)    def get_object_id(self, name):        name = self.pad_str_with_20char(name)        id = plasma.ObjectID(name)        return id    def pad_str_with_20char(self, name):        if len(name) &lt; 20:            name = f&quot;&#123;name:&lt;20&#125;&quot;        else:            name = name[:20]        return name.encode()    def decode_hex_bytes(self, hex):        return bytes.fromhex(hex).decode()    def get_object(self, object_id):        obj = self.client.get(object_id)        return obj    def set_object(self, obj):        self.client.put(obj)    def create_buffer(self, obj, object_id):        object_size = len(obj)        buf = memoryview(self.client.create(object_id, object_size))        for i in range(object_size):            buf[i] = i % 128    def seal_buffer(self, object_id):        self.client.seal(object_id)    def get_buffer(self, object_id):        [buffer] = self.client.get_buffers([object_id])        return buffer    def list_objects(self):        return self.client.list()\n\n然后写一个子类能够使用plasma存储对象来读取dataframe:\nclass PlasmaPandas(PlasmaClient):    def __init__(self, name, location=&quot;plasma&quot;):        super().__init__(location)        self.object_id = self.get_object_id(name)    def dataFrame2recordBatch(self, df):        return pa.RecordBatch.from_pandas(df)    def create_plasma_obj_from_record_batch(self, record_batch):        # get the size of record batch and schema        mock_sink = pa.MockOutputStream()        stream_writer = pa.RecordBatchStreamWriter(mock_sink, record_batch.schema)        stream_writer.write_batch(record_batch)        stream_writer.close()        data_size = mock_sink.size()        buf = self.client.create(self.object_id, data_size)        # write dataframe into store        stream = pa.FixedSizeBufferWriter(buf)        stream_writer = pa.RecordBatchStreamWriter(stream, record_batch.schema)        stream_writer.write_batch(record_batch)        stream_writer.close()        self.seal_buffer(self.object_id)    def get_df_by_name(self):         [data] = self.client.get_buffers([self.object_id])        arrow_buffer = pa.BufferReader(data)        # Convert object back into an Arrow RecordBatch        reader = pa.RecordBatchStreamReader(arrow_buffer)        record_batch = reader.read_next_batch()        result = record_batch.to_pandas()        return result    def get_df_column(self, column): #可以直接在arrow中取column, 性能比pandas快5-10倍         [data] = self.client.get_buffers([self.object_id])        arrow_buffer = pa.BufferReader(data)        # Convert object back into an Arrow RecordBatch        reader = pa.RecordBatchStreamReader(arrow_buffer)        table = reader.read_all()        try:            result = table.select([&quot;index&quot;, column]).to_pandas()        except KeyError:            return pd.DataFrame()        return result            def store_df_in_store(self, df):         rb = self.dataFrame2recordBatch(df)        self.create_plasma_obj_from_record_batch(rb)\n在应用中使用：\npp = PlasmaPandas(&#x27;obj_key&#x27;)df = pd.read_csv(&#x27;example.csv&#x27;)pp.store_df_in_store(df)df = pp.get_df_column(&#x27;col_name&#x27;)\n","categories":["big data"],"tags":["big data"]},{"title":"从Pandas到Spark","url":"/2020/07/23/bigdata-2020-07-24-from-pandas-to-spark/","content":"前言本文主要讨论如何把pandas移植到spark, 他们的dataframe共有一些特性如操作方法和模式。pandas的灵活性比spark强， 但是经过一些改动spark基本上能完成相同的工作。同时又兼具了扩展性的优势，当然他们的语法和用法稍稍有些不同。\n主要不同处：分布式处理pandas只能单机处理， 把dataframe放进内存计算。spark是集群分布式地，可以处理的数据可以大大超出集群的内存数。\n懒执行spark不执行任何transformation直到需要运行action方法，action一般是存储或者展示数据的操作。这种将transformation延后的做法可以让spark调度知道所有的执行情况，用于优化执行顺序和读取需要的数据。懒执行也是scala的特性之一。通常，在pandas我们总是和数据打交道， 而在spark,我们总是在改变产生数据的执行计划。\n数据不可变scala的函数式编程通常倾向使用不可变对象， 每一个spark transformation会返回一个新的dataframe(除了一些meta info会改变）\n没有索引spark是没有索引概念的.\n单条数据索引不方便pandas可以快速使用索引找到数据，spark没有这个功能，因为在spark主要操作的是执行计划来展示数据， 而不是数据本身。\nspark sql因为有了SQL功能的支持， spark更接近关系型数据库。\n两者的一些操作例子projectionspandas的投影可以直接通过[]操作\n&gt;&gt;&gt; person_pd[[&#x27;age&#x27;,&#x27;name&#x27;]]   age     name0   23    Alice1   21      Bob2   27  Charlie3   24      Eve4   19  Frances5   31   George\n\n\npyspark也可以直接[]来选取投影， 但是这是一个语法糖， 实际是用了select方法\n&gt;&gt;&gt; res[[&#x27;Quarter&#x27;]].show()+-------+|Quarter|+-------+|Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012|+-------+only showing top 20 rows&gt;&gt;&gt; res.select(&#x27;Quarter&#x27;).show()+-------+|Quarter|+-------+|Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012||Q1 2012|+-------+only showing top 20 rows\nsimple transformationsselect实际上接受任何column对象， 一个column对象概念上是dataframe的一列。一列可以是dataframe的一列输入，也可以是一个计算结果或者多个列的transformation结果。以改变一列为大写为例：\n&gt;&gt;&gt; ret = pd.DataFrame(person_pd[&#x27;name&#x27;].apply(lambda x: x.upper()))&gt;&gt;&gt; ret      name0    ALICE1      BOB2  CHARLIE3      EVE4  FRANCES5   GEORGE\nimport pyspark.sql.functions as sfresult = persons.select(  sf.upper(persons.name))\n增加一列def create_salutation(row):  sex = row[0]  name = row[1]  if sex == &#x27;male&#x27;:    return &#x27;Mr &#x27;+name  else:    return &quot;Mrs &quot;+name   result = persons_pd.copy()result[&#x27;salutation&#x27;] = result[[&#x27;sex&#x27;,&#x27;name&#x27;]].apply(create_salutation, axis=1, result_type=&#x27;expand&#x27;)resultage\theight\tname\tsex\tsalutation0\t23\t156\tAlice\tfemale\tMrs Alice1\t21\t181\tBob\tmale\tMr Bob2\t27\t176\tCharlie\tmale\tMr Charlie3\t24\t167\tEve\tfemale\tMrs Eve4\t19\t172\tFrances\tfemale\tMrs Frances5\t31\t191\tGeorge\tfemale\tMrs George\nspark sql有个控制流方法when可以使代码更简洁， 配合withColumn增加一列\nresult = persons.withColumn(    &quot;salutation&quot;,    sf.concat(sf.when(persons.sex == &#x27;male&#x27;, &quot;Mr &quot;).otherwise(&quot;Mrs &quot;), persons.name).alias(&quot;salutation&quot;))\n\n","categories":["big data"],"tags":["big data"]},{"title":"Data Integration","url":"/2021/04/09/bigdata-2021-04-10-data-integration-tools/","content":"数据集成简介企业总是有从各个系统里集成数据的需求, 数据集成就是一门技术用来提供企业层面的统一和一致的数据视图. 数据集成的目的是维护数据源整体上的数据一致性, 解决企业数据孤岛问题, 提高信息共享和利用的效率. 数据集成的核心任务是将有关联的异构数据源集成到一起, 使用户能够透明地方式访问这些数据.\n数据集成的分类从集成方式的角度来分, 数据集成可分为:\n\n点对点集成\n总线式集成\n离线批量集成\n流式数据集成\n\n点对点数据集成点对点一般采用接口方式对接, 适合连接对象比较少的情况, 具有开发周期短, 技术难度低的优势. 问题是当连接对象变多, 连接路径将会指数级增长, 效率和维护成本会变大. 主要的维护成本在于不能集中管理和监控接口服务, 如果交换协议不一致将会遇到开发困难.点对点的集成是紧耦合的，当一个连接变化时，所有与其相关的接口程序都需要重新开发或调试。\n总线式数据集成总线式数据集成是通过在中间件上定义和执行集成规则，其拓扑结构不再是点对点集成形成的无规则网状，而主要是中心辐射型的（Hub型）星型结构或总线结构.总线结构通过与点对点集成架构相比，采用总线架构可以显著减少编写的专用集成代码量，提升了集成接口的可管理性。不同连接对象如果连接方式有差异，可以通过总线完全屏蔽掉，做到对连接对象透明，无需各个连接对象关心。通过总线结构，把原来复杂的网状结构变成简单的星形结构，极大提高了硬件的可靠性和可用性。\n\n第一代总线集成工具: EDI电子数据交换系统企业直接按照通用的消息格式发送信息，接收方也需要按统一规定的语法规则，对消息进行处理，并引起其他相关系统的EDI综合处理。一般EDI用于企业间交换交易凭证和发票等数据, 是无纸化办公的实现.\n\n第二代总线集成工具: ESB企业服务总线ESB的使用标志着企业的应用集成进入了SOA时代（SOA是一种面向服务的集成架构）。SOA架构的其主要特征是基于一系列Web标准或规范来开发接口程序，包括UDDI、SOAP、WSDL、XML、REST，并采用支持这些规范的中间件产品作为集成平台，从而实现了一种开放而富有弹性的应用集成方式。ESB是对web服务（WebService）的注册、编排和管理。WebService是一种跨编程语言、跨操作系统平台的远程调用技术，是web的一种标准。可以理解为：WebService是一个应用程序向外界暴露了一个能通过Web调用的API接口，我们把调用这个WebService的应用程序称作客户端，把提供这个WebService的应用程序称作服务端。客户端进行服务的远程调用前，需要知道服务的地址与服务有什么方法可以调用。因此，WebService服务端通过一个文件（WSDL）来说明自己家里有啥服务可以对外调用，服务是什么，服务中有哪些方法，方法输入的参数是什么，返回值是什么，服务的网络地址是什么，通过什么方式来调用等。WSDL是一个基于XML的语言，用于描述WebService及其函数、参数和返回值，它是WebService客户端和服务器端都能理解的标准格式。\n\n\n离线批量数据集成在传统数据集成的语境下，离线批量数据集成，通常是指基于ETL工具的离线数据集成，ETL即数据的提取（Extract)、转换(Transform)和加载(Load)。ETL的实现有多种方法，常用的有三种：\n\n第一种是借助ETL工具：例如：Informatic、IBM CDC、talend、kettle、Nifi等，借助工具可以快速的建立起ETL工程，屏蔽了复杂的编码任务，提高了速度，降低了难度，但是缺少灵活性。\n第二种是SQL编码实现：SQL的方法优点是灵活，提高ETL运行效率，但是编码复杂，对技术要求比较高。\n第三种是ETL工具和SQL组合实现：综合了前面二种的优点，会极大地提高ETL的开发速度和效率。\n\n流程数据集成流式数据集成也叫流式数据实时数据处理，通常是采用Flume、Kafka等流式数据处理工具对数据库进行实时监控和复制，然后根据业务场景做对应的处理（例如去重、去噪、中间计算等），之后再写入到对应的数据存储中。\n网络数据集成网络数据集成也叫网络数据采集，指通过网络爬虫或网站公开API等方式从网站上获取数据信息的过程。网页爬虫，即一种按照一定的规则，自动地抓取互联网信息的程序或者脚本，一般分为通用网络爬虫和聚焦网络爬虫两种。网页爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。聚焦爬虫的工作流程较为复杂，需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接并将其放入等待抓取的URL队列。网页爬虫支持文本文件、图片、音频、视频等非结构化数据、半结构化数据从网页中提取出来，存储在本地的存储系统中。目前网络上有许多网页爬虫，Octoparse、WebCopy、HTTrack、Getleft、Scraper等.\n表格总结|—————–+————+—————–+—————-|\n\n\n\n方式\n优点\n缺点\n备注\n\n\n\n点对点\n1.开发周期短 2.技术难度低\n1.维护成本高 2.紧耦合 3.接口协议局限\n基本不选择\n\n\n总线式\n1.集中管理  2.多协议 3. 解耦和结构星型化 4.可声明\n1.存在单点问题,需要高可用 2.开发成本比点对点高\n适合API类型的数据集成\n\n\n离线批量\n1.吞吐量大 2.可自定义数据模型 3.可集成数仓\n1.需要一定编码能力 2.数据有延迟 3.ETL一般只支持关系型数据\n适合数仓系统\n\n\n流式数据集成\n1.实时 2.吞吐量达每秒百MB 3.支持NoSQL\n1.架构较复杂 2.需要devops能力\n一般使用Kafka\n\n\n—————–+————+—————–+—————-\n\n\n\n\n\n","categories":["big data"],"tags":["big data"]},{"title":"阿里云和aws的网路特点","url":"/2018/05/05/cloud-2018-05-06-aliyun%E5%92%8Caws%E7%9A%84%E7%BD%91%E8%B7%AF/","content":"本文主要介绍aws网络产品, 并对比阿里云的产品阿里云高速通道支持不同区域vpc直连，这个比aws的vpc peering好， vpc peering只能用于同一区域，且是不同ip段连接。aws每个区域的默认 VPC 数量是5个, 每个vpc默认子网有200个, VPC 的网段从&#x2F;16 到 &#x2F;28.aws的弹性网路接口（ENI)，是一个虚拟网卡，只能用于VPC中的实例。阿里云叫弹性网卡。我们可以在虚机默认的主网卡上额外添加网路接口，由于它是弹性的，所以可以把它从一个实例移除， 然后放到另一个虚机上，并保留原有的网路属性。 那么ENI有哪些网路属性呢？ 一个弹性网路接口可以有如下属性：\n有一个主要IPv4私有地址\n一个或多个辅助私有地址\n每一个私有IP可以有一个弹性IP\n一个公有IP(取决于子网是否分配公有IP的属性设置）\n一个或多个IPv6地址\n一个或多个安全组\n一个MAC地址\n一个源&#x2F;目标检查标记\n一个描述\n\naws的弹性IP不支持IPv6， 弹性IP地址只能在一个特定区域中使用。一个运行着一个弹性IP的实例是不收取ip费用的，但是闲置的弹性IP aws将按小时收费。而aliyun总是收取EIP保有费的（除非EIP和VPC中虚机绑定），包年包月还不能释放；后付费可以用按量或者按带宽选择。aws VPC flow logs. flow log用于捕获有关传入和传出 VPC 中网络接口的 IP 流量的信息。流日志数据使用 Amazon CloudWatch Logs 存储。创建流日志后，您可以在 Amazon CloudWatch Logs 中查看和检索其数据。可以为 VPC、子网或网络接口创建流日志。如果为子网或 VPC 创建流日志，则会监视 VPC 或子网中的每个网络接口。使用aws的Direct Connect除了常规的优势： 减少流量费、网速保证、私有连接外，它比vpn连接最大的优势是，支持10G带宽，而vpn只能到4Gbps. 实现direct connect需要2个组件：物理链路和virtual interface(VIF)。想要把流量通过direct connect 路由到VPC, 那么需要在aws这边创建私有VIF；如果需要连接公有的aws服务，那么需要创建公有VIF.每个VIF有如下组件：\n\nVirtual Local Area Network ID. 这个VLAN id是唯一的\n需要连接的IP地址， 支持IPv6.\n只支持BGP路由协议\n\naws LoadBalancer有三种类型： Classic, Network, Application LoadBalancer. 见下图：![https://s3.ap-southeast-1.amazonaws.com/kopei-public/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-05-06%20%E4%B8%8B%E5%8D%888.37.46.png](https://s3.ap-southeast-1.amazonaws.com/kopei-public/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-05-06%20%E4%B8%8B%E5%8D%888.37.46.png)\naws CloudFront 是一个能加速你的静态和动态内容的web服务.除了能分发静态内容,还能分发动态内容和流内容.aws Route 53的5中路由方式:\nsample routing. 简单匹配dns数据库中的记录, 比如一个域名对应一个IP\nweighted routing. 加权路由是按比例分发流量\nlatency-based routing. 基于延迟时间选择延迟低的分发流量\ngeolocation routing. 根据用户的IP所在地分发流量, 有时候可能定位用户IP, 那么需要定义一个默认路由, 方式路由出现no answer.\nfailover routing. 失效转移路由.作为辅助路由, 当主要路由健康检查失败, 那么切换到这个路由.\n\naws有三种提供vpn连接的方式：\nVirtual Private Gateway。VGW是高可用和可扩展的。 给VPC绑定一个VGW, 就可以通过IPsec建立安全连接。 VGW支持静态路由和BGP方式， 如果是静态路由，那么对方网路的IP段不能和VPC相同。\nAWS VPN CloudHub. CloudHub是高可用和可扩展的。如果有多个站点需要建立安全通信，可以使用CloudHUB, 这样不仅可以访问VPC内资源，还可以在站点间建立通信。\n第三方的software VPN\n\n","categories":["cloud"],"tags":["cloud"]},{"title":"使用阿里云的批量计算做SGE","url":"/2018/05/13/cloud-2018-05-14-aliyun-sge/","content":"前言阿里云批量计算支持SGE集群, 版本是GE6.2, 只支持centos. 使用aliyun镜像市场已经打包好的镜像可以方便的起一个集群, 然后使用batchcompute_sgesdk管理和定制自己想要的sge集群特性, 比如动态扩展执行节点.\nSGE是什么?Sun Grid Engine (SGE)是一个经典的UNIX批量计算调度系统. SGE可以使用网格有效地利用计算资源, 把节点的CPU当成slots来分配资源, 而不用管计算资源是何种结构. 理论上, 只要网络稳定, 一个简单的sge安装就可以满足小量用户使用而不需要任何维护. 但是SGE的背后是复杂的, 真正掌握SGE可能需要6到12个月. SGE具备一个典型的批量计算特征:\n\n接受外部job请求\n可以暂存未运行job\n把暂存的job送到一个或多个执行节点\n管理job运行, 中心化存储结果文件\nLOGsSGE的架构主要围绕两个概念构建: queue队列和parallel environment平行环境.\n\nsge架构SGE由头节点和计算节点组成, master host头节点运行sge_master. 这个守护进程控制GE的调度和组件(队列和job). sge_master维护着组件状态表, 用户访问权限等. sge_master也会处理从执行节点周期性传来的job状态, 负载等信息. 通常头节点也是提交节点和管理节点.计算节点通常运行着sge_execd, 通常是执行节点execution host, 但是也可以是管理节点或提交节点, 不同于在master做管理节点, 把运行节点当管理节点需要注册这台hostqconf -ah &lt;hostname&gt;; 提交节点就是用户提交job运行的节点, qconf -as &lt;hostname&gt;可以把一个计算节点作为提交节点. 在运行的job还有一个守护进程sge_shepherd, 它会作如下操作:\n\n由sge_execd唤醒用于处理job\n设置脚本前置的参数(跑Prolog)和PE\n系统调用setuid成为用户\n用子进程开始job, 控制job\n处理停止,恢复,终止信号, 处理checkpointing\n跑Epilog脚本, 清理环境, 关闭PE\n\nsge queue队列是一类对资源需求相似job的容器, 他们能够同时在多个节点运行. 逻辑上说, 队列是并行环境的孩子, 虽然它可以有多个父亲. 一个队列可以在一个节点上, 也可以在多个节点上.在多个节点上的队列叫做服务器场队列, 使用上如同在一个节点上一样. 一个节点也可以有多个队列. 应该把同一个属性的job分到同一个队列, 这样队列上属性的改变会影响到对应job, 比如暂停一个队列会暂停所以队列中的job. 默认安装的队列名是all.q, 可以定义额外的队列用于不同计算资源需求, 这样sge调度器可以选择合适的job到合适的网格节点运行.  \nparallel environment并行环境（PE）是SGE的核心概念，代表了一系列设置，可以告诉Grid Engine如何启动，停止和管理由使用此环境的队列运行的作业. 可以使用PE设置一个队列所有job分配的最大slots, 也可以设置parallel messaging的参数, 用于并行计算. 使用如下命令可以管理PE:\nqconf -spl ##show all peqconf -sp &lt;PE name&gt;qconf -mp &lt;PE name&gt;qconf -Ap ./my-PE-template.txtqconf -ape &lt;PE name&gt;\n创建PE的时候有几个关键属性要注意:\n\nslots: job可以占用的最大slots\nallocation_rule: 此设置控制如何将作业槽分配给主机。它可以有四个可能的值：\na number: 每个host会给job槽分配固定数字的slot, 直到满足job slots要求.\n$fill_up: 会把当前host所有slots分配给job, 不够再到下一个host请求资源, 直到满足\nround_robin: 使用轮询的方式从每个host索取资源, 直到满足job需求\npe_slots: 只占用某一台节点的资源. 这就意味着sge只会把job调度到某个满足slots要求的节点\n\n\ncontrol_slaves: 控制MPI slaves\njob_is_first_task: job是否是并行计算的一部分\naccounting_summary: 如果control_slaves设为True, 可以使用这个配置查看子任务的统计信息\n\n简单使用sgeqsub使用qsub提交job后, 调度器会对job做调度并且直接返回qsub是否调度成功. 例子:\n$ qsub -cwd -b y -o my.txt -q all.q@iZuf63555dkqqm58d7dc9nZ hostnameYour job 8 (&quot;hostname&quot;) has been submitted$ qstat -j 8==============================================================job_number:                 8exec_file:                  job_scripts/8submission_time:            Tue May 15 11:39:27 2018owner:                      rookieuid:                        500group:                      rookiegid:                        500sge_o_home:                 /home/rookiesge_o_log_name:             rookiesge_o_path:                 /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/rookie/binsge_o_shell:                /bin/bashsge_o_workdir:              /home/rookiesge_o_host:                 iZuf668cj7e7k2ws1y6pdkZaccount:                    sgecwd:                        /home/rookiemail_list:                  rookie@iZuf668cj7e7k2ws1y6pdkZnotify:                     FALSEjob_name:                   hostnamestdout_path_list:           NONE:NONE:my.txtjobshare:                   0hard_queue_list:            all.q@iZuf63555dkqqm58d7dc9nZenv_list:                   script_file:                hostnameerror reason    1:          can&#x27;t get password entry for user &quot;rookie&quot;. Either the user does not exist or NIS error!scheduling info:            (Collecting of scheduler job information is turned off)\n上述job执行失败, 由于新创建的用户没有加入sge entry. 如果执行成功会在执行节点的当前目录看到my.txt, 默认是用户家目录. 参数详解:\n\n-o : 输出文件的路径. 如果不指定文件名, 默认采用.o\n-e : 指定错误文件, 默认采用 .e格式\n-b y&#x2F;n: 执行的是脚本还是二进制job.\n-N [name]: job 名称\n-A [account name] 这个job的资源消耗记在谁的头上\n-r [y,n]: 这个job是否可重新跑, 默认y\n-cwd: qsub的-cwd选项告诉Sun Grid Engine，该作业应该在调用qsub的相同目录中执行\n-q: 指定节点上的队列\n-S [shell path]: 指定使用哪个shell\n-pe  []: 执行并行job时需要指定CPU核数.  \n-l resource&#x3D;value,.. : 指定资源需求, 使job在满足需求的队列上运行. -l可以在qsub, qsh, qrsh, qlogin, qalter上使用. resource这个键值可以是queue或host相关, 可以是queue相关的资源属性:\nqname\nhostname\nnotify\ncalendar\nmin_cpu_interval\ntmpdir\nseq_no\ns_rt\nh_rt\ns_cpu\nh_cpu\ns_data\nh_data\ns_stack\nh_stack\ns_core\nh_core\ns_rss\nh_rsshost相关是资源属性有:\nslots\ns_vmem\nh_vmem\ns_fsize\nh_fsize\n\n\n\n输出参数有三个参数可以配置输出流\n\n-e path_list: 设置标准错误输出流路径\n-j y[es]n[o]: 合并标准输出和标准错误流\n-o path_list: 设置标准输出路径\n\n执行脚本中设置参数在job的执行脚本头上写入#$可以设置job的qsub参数, 这样就不需要在命令行输入参数.\n指定队列qsub -q queue_name jobqsub -q queue_name@hostname jobqsub -q queue_name@@hostgroupname jobqsub -q &#x27;*@@hostgroupname&#x27; job # 可以使用通配符匹配\n\nSGE 环境变量继承关系execd -&gt; shepherd -&gt; shell -&gt; job, 后续继承的环境变量可以被覆盖\n默认job参数, 在提交节点设置$HOME&#x2F;.sge_request可以设置以上这个文件来让job启用默认参数, 比如邮件通知方式:\n-M &lt;email-address&gt;  # -M root@localhost 邮件会给执行节点root发邮件-m baes  ## will notify whether job is begin, aborted, end, suspend.  -v PYTHONPATH ## environment variables-V   # pass all environment variables, 这个参数可能有bug-pe smp 2 ## pe settings\n如下三个文件是默认参数读取的文件, 可以被覆盖.$SGE_ROOT&#x2F;$SGE_CELL&#x2F;common&#x2F;sge_request$HOME&#x2F;.sge_request$PWD&#x2F;.sge_request\nqdel job_idqhost 查看所有节点状态qstat 查看queue和job的状态qstat输出的jobstate有d(eletion),  E(rror), h(old), r(unning), R(estarted),q(ueued), s(uspended), S(uspended), t(ransfering), T(hreshold) or w(aiting).  qstat -explain c -j &lt;job_id&gt;可以查看具体job跑失败的原因. 下面是一些例子:\nqstat -u &#x27;*&#x27; Displays list of all jobs from all users.qstat -g c    show available nodes and loadqstat -u joeuser  -- useful in seeing list of jobs from particular user. Especially when particular user job are having troublesqstat -u hpc1***: Displays list of all jobs belonging to user hpc1***qstat -f: gives full information about jobs and queues. Provides a full listing of the job that has the listed Job ID (or all jobs if no Job ID is given).  See qstat -f below.qstat -j job_number -- provide detailed information why the pending job is not being scheduled. See qstat -j belowqhost -Fqstat -g t -- command is useful for showing where all of your parallel tasks are running, otherwise you only see where the &quot;master&quot; task (MPI task #0) is running.qstat -s p shows pending jobs, which is all those with state &quot;qw&quot; and &quot;hqw&quot;.qstat -s h shows hold jobs, which is all those with state &quot;hqw&quot;.\n\n用户权限管理SGE有4个角色: Managers, Operators, Owners, Users.\n\nManagers: 就是admin, 具有有权限\nOperators: 除了没有add, delete, modify队列, 具有manager所有权限.\nOwners: 队列拥有者, 可以对他所有的队列做任何操作.\nUsers: 没有管理集群和队列的权限, 只能使用队列.\n\n配置用户Access list只要用户在一个提交节点和一个执行节点有ID, 那么就可以使用SGE. 但是管理员可以限制用户对某些队列的访问限制, 也可以限制对一些工具的使用比如PE. 指定访问权限需要定义User Access List, 可以使用unix的user和group定义user access list. 然后根据这个list来限制对资源的读写权限.\nqconf -au username[,...] access-list-name[,...]qconf -sul ##查看所有user access list\n\n使用sdk调整执行节点\n调整队列\n调整执行节点\n调整host subgroups\n\nqconf -ahgrp &lt;hostgroupname&gt;  ## add host group to group list\n\nSGE节点组的概念就像Unix操作系统一样, 可以把节点分组, 组名用@开头. 组和组可以嵌套, 叫做subgroups(和&#x2F;etc&#x2F;group不同)\nqconf -shgrpl@allhosts\n\n执行节点动态拓展使用qstat -u &#39;*&#39; -s p检查qw或hwq队列的长度, 相应地启动执行节点进行动态拓展; 同时当队列为空时, 减少执行节点到一定数目\n","categories":["cloud"],"tags":["cloud"]},{"title":"DNS和Route53简介","url":"/2018/10/26/cloud-2018-10-27-DNS-and-Route53/","content":"DNS基本概念DNS(Domain Name Service)是人们使用英特网的基础服务，DNS提供的服务就像一个电话本一样, 计算机可以用它找到域名对应的IP地址。DNS采用层级的结构， 不同层采用.来分层。顶层是root, 用一个.表示，后面就是TLD了。Top Level Domain(TLD)顶级域名是域名中的最后一个部分， 比如.com. TLD又分为通用顶级域名和地域顶级域名如.cn. Internet Corporation for Assigned Names and Numbers(ICANN)负责管理和分配部分顶级域名， 这些顶级域名下可以再分配我们常见的域名，这些域名会在Network Information Center(InterNIC)注册， 每个域名会在一个叫Whois的数据库注册， 以维护域名的唯一性。这里有一个误区，域名如example.com, 一般所说的二级域名是应该是example(而国内的运营商叫做一级域名, .com叫顶级域名)， 其他的二级域名&#x2F;三级依次往后叫.\nhost和Subdomain有了域名， 域名拥有者可以把自己的服务或主机定义成host, 比如大部分的web服务都可以通过www这个host访问。TLD是可以被按层级扩展成多个子域名的。如example.com中example就是SLD(Second-level domain), 又如sina.com.cn中.com是SLD. SLD和host主要的区别在于host定义的是一个资源，而SLD是一个域名的扩展。不管是SLD还是host, 我们都从域名的左边读起， 可以看到越左边的部分意义越具体。\nName Server名字服务器就是实际把域名解析成Ip的服务器。由于域名实在太多，名字服务器需要转发解析请求到其他服务器。如果某个域名是这台名字服务器管理的， 那么这个NS的解析相应我们认为是权威的。（authoritative）\nZone filezone file区域档案是DNS服务器存储域名和IP映射记录的文本。一个zone file定义了一个dns域, 多个zone file通常用来定义一个域。每个文件中的记录称为资源记录（resource record）。zone file有两个指令需要注意， 一个是$ORIGIN参数设定， 代表了本NS管理的域。$TTL表示解析记录在缓存中默认过期时间。\nDNS Record Types\nSOA Start Of Authority, 每个区域文件的一条强制记录， 记录每个域的dns基本信息，具体包括：\n这个区域的DNS server名称\n这个区域的管理员\n当前文件的版本\n二级域名服务器重试、更新、过期信息的时间设置\nRR的TTL默认时间\n\n\nA and AAAA. A把一个host映射到IPv4地址， AAAA映射到IPv6地址。\nCNAME 别名.可以为你的A或者AAAA记录映射的服务取别名.\nMX(Mail Exchange), 邮件交换主机记录. 此记录是用来宣告一个域底下哪一个A记录为专门负责邮件进出. 由于一个网域底下的MX记录可以超过一笔, 所以, 在众多MX记录里要排列出优先順序就必须倚靠MX记录里的另一项设定—Preference值, 值越小, 优先权越高, 最小的值为0. 同时MX不能指向CNAME.\nNS(Name Server). 指定哪个Name Server可以得到某个域名的权威解析, 用于TLD顶级域名服务器解析会用到.\nPTR(Pointer)反向解析, 把IP解析到域名.\naws还支持一种叫alias的record, 指向aws的某个公网服务。\n\nFully Qualified Domain Name(FQDN)按ICANN的标准FQDN是需要按.结尾的，虽然通常我们并没有这么做. 具体语法如下图所示\n浏览器解析DNS步骤浏览器输入域名后， 从域名解析到实际的IP, 会走如下步骤：\n\n计算机先检查浏览器缓存是否存在， 如果是使用chrome, 可以在地址栏输入chrome://net-internals/#dns查看缓存信息。\n浏览器的缓存有一些限制， 比如缓存的条目数只有1000等等，所以如果不命中缓存， 那么就会查询本地hosts文件是否存在对应的ip。\n如果还是不中那么检查服务器端设置的域名解析服务器Resolving Name Servers（/etc/resolv.conf设置的DNS首选项）缓存是否命中。\n如果还是没有命中，那么就会查询Resolving Name Servers(通常是ISP供应商提供)。后续还会往root服务器迭代查询， root服务器又会重定向到TLD服务器，TLD再重定向到Domain-Level Name Servers等等。 但是基本上是本地设置的DNS服务器帮助用户做了和上层服务的交互。下图很好解释流整个dns解析流程.\n\nRoute53提供的服务Route53提供三个服务：域名注册，DNS服务，健康检查。\n使用Route53和其他服务提高系统韧性\n每个区域有一个负载均衡器， 均衡器下的服务器分布在不同可用区。\n每个可用区都需要是自动伸缩。\n负载均衡器需要设置健康检查。\n每个负载均衡器上面是Route53， Route53设置别名记录alias record指向每个负载均衡器， 同时设置路由规则采用最小延时规则， 开启每个均衡器的健康检查。\n所有静态和动态内容使用CDN缓存。\n\n","categories":["cloud"],"tags":["cloud"]},{"title":"AWS的VPC组件","url":"/2018/11/04/cloud-2018-11-05-aws-internet-gateway/","content":"IGWInternet Gateway(IGW)是一个水平扩展, 冗余, 高可用的aws VPC组件. 主要的作用是能让你的VPC和英特网连接. 一个IGW可以作为VPC路由表中的target, 会把虚机的IP地址做NAT(网络地址转换), 以让英特网认为私有的虚机具有公网的IP, 从而做到私有虚机和英特网能够通讯.\nDHCPDynamic Host Configuration Protocal(DHCP)提供了一个标准用于通过TCP&#x2F;IP传输配置信息给主机.\n","categories":["cloud"],"tags":["aws, vpc, igw, acl"]},{"title":"一台虚机多个IP","url":"/2018/12/09/cloud-2018-12-10-one-instance-multiple-ip/","content":"一个云主机可以被分配多个IP, 这样做的好处有:\n\n一台虚机运行多个web应用, 然后每个ip一个应用,每个ip一张ssl证书\n防火墙或者负载均衡器需要多个IP\n如果一台虚机宕机, 那么可以把多个ip的一个转移到另一台备用主机, 做到高可用.下面以AWS为例子介绍一台主机多个IP是怎么工作的.\n\n多个IP地址是如何工作的?ip需要绑定到网卡, 以下主要以IPv4为例做介绍\n\n多个IPv4私有地址可以绑定到任何网路接口, 网路接口可以绑定或解绑到主机.\nIPv4的地址必须是在子网的网段内\n网路接口是有安全组的, 所以对应的IP就应该遵循这个安全组规则.\n多个IP地址可以被分配给网路接口, 网路接口可以是绑定到运行着或者非运行着的主机.\n一个已经被分配的ip可以从一个网卡分配到另一个网卡.\n虽然主机的主网卡不能被移走, 但是主网卡的第二个ip可以被移动到另一个网卡.\n每一个IPv4地址可以被分配一个弹性IP, 反之亦然\n当一个Ipv4的私有地址被分配到另一个网卡时, 它对应的弹性IP会跟随移走.\n当一个IPv4私有地址从网卡移走, 那么弹性IP会自动和这个IP解绑.\n\n","categories":["cloud"],"tags":["cloud"]},{"title":"数据仓库ETL的两个设计主线","url":"/2020/11/21/data-warehouse-2020-11-22-etl-toolkit/","content":"前言数据仓库ETL有两个设计主线, 其一为规划与设计主线, 其二为数据主线.\n规划&amp;设计主线规划与设计分为四步骤:需求&#x2F;现状 -&gt; 架构 -&gt; 实现 -&gt; 测试&#x2F;发布. 每个步骤又有相应的具体问题考虑.\n*** 需求与现状 ***\n\n业务需求\n数据评估和数据源现状\n监察需求\n安全需求\n数据集成\n数据延迟\n归档和沿袭\n最终用户提交界面\n可用的开发技能\n可用的管理技能\n已有的许可证\n\n*** 架构 ***\n\n手工编码还是使用ETL工具\n批处理还是流数据处理\n水平任务依赖还是垂直任务依赖\n自动调度\n异常处理\n质量控制\n恢复与重启\n元数据\n安全\n\n*** 实现 *** \n\n硬件\n软件\n编码\n文档\n特定质量检查\n\n*** 测试&#x2F;发布 ***\n\n开发系统\n测试系统\n生产系统\n提交过程\n升级方案\n系统快照和回滚过程\n性能调优\n\n数据主线数据流:抽取 -&gt; 清洗 -&gt; 规格化 -&gt; 提交. 每个数据流步骤都可以用运行步骤来监控:\n\n调度\n作业执行\n异常处理\n恢复和重启\n质量检查\n发布\n支持\n\n*** 抽取 ***\n\n读取源数据模型\n连接并访问数据\n调度源系统，截取通知和后台程序\n捕获变化数据\n将抽取的数据集结到磁盘\n\n*** 清洗 ***\n\n强制列属性\n强制结构\n强制数据和数值规则\n强制复杂业务规则\n创建元数据来描述数据质量\n将清洗后的数据集结到磁盘\n\n*** 规格化 ***\n\n业务标志（在维表中）的规格化\n业务度量和绩效指标（在事实表中）的规格化\n复制\n家庭关联信息的规格化（Householding）\n国际化\n将规格化的数据集结到磁盘\n\n*** 提交 ***\n\n加载平面型和雪花型维度\n生成时间维度\n加载退化维度\n加载子维度\n加载缓慢变化维（包括类型 1、 2、 3）\n规格化维和规格化事实\n处理迟到维和迟到事实\n加载多值维\n加载ragged层次维\n加载维中的文本型事实\n为事实表运行代理健pipeline\n加载三种基础事实表粒度\n加载和更新聚合表\n将提交数据集结到磁盘\n\n","categories":["data warehouse"],"tags":["db, data warehouse"]},{"title":"现有基因大数据云平台简单比较","url":"/2018/12/10/cloud-2018-12-11-current-genomics-solutions/","content":"大背景大药厂一般会搭建自己的内部分析平台或者数据仓库, 用于聚合不同的数据, 这些数据可能来自化学或生物的论文, 可穿戴设备, 或基因组学数据等等. 基于这些数据得出一些统计分析, 药厂能够加速创新研发, 同时保持在AI方面的跟进. 但是在整合基因数据到药厂自己的分析系统时, 大部分公司往往力不从心. 这里面主要原因是由于基因数据的多维复杂度和数据量, 使得药厂需要一批既懂生信又懂计算机科学的人才, 而这样的人往往又不好招. 所以用于分析基因数据的专有云面世了, 本文主要对现有的国外基因数据分析平台做一个简单比较.\nPasS比较|—————–+————+—————–+—————-|\n\n\n\n产品\n商业&#x2F;学术\n特点\n缺点\n\n\n\nGoogle Variant Transforms\n商业\ngoogle出品\n只支持部分变异varaints, 查询有限制, 企业级支持有限\n\n\nCloudera + Databricks\n商业\n基于spark+hadoop的云, 企业级大数据咨询服务, 与大部分IasS是合作伙伴\nNGS数据经验有限, 不能直接支持基因序列查询, spark基于列式数据库不适合基因这样有顺序的数据结构\n\n\nPLINK\n学术\n开源,  偏学术分享\n缺少企业级支持, 需要devops去部署和集成\n\n\nBroad Fire Cload\n学术\n出自broad, 有workflow工具支持查询hail, 支持定制化\n缺少企业级支持, 需要devops部署和集成系统, 产品被设计成一个学术型沙盒而不是企业级产品\n\n\n—————–+————+—————–+—————-\n\n\n\n\n\nHail\n都有\n开源, Broad出品\n对变异有限制, 缺少企业级支持,数据需要建索引,需要devops团队支持\n\n\nParadigm4\n商业\n数据库技术先进, 支持可穿戴设备的数据\n没有NGS数据的经验, 缺少针对基因数据库的扩展性设计和分析引擎设计\n\n\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n\n\n\n\n\nSeven Bridges Sonar\n商业\n二级分析平台\n缺少针对基因数据库的扩展性设计和分析引擎设计\n\n\nBC Platforms\n商业\n平台强于micro-array数据,对于基因型有较好的服务\n缺少针对基因数据库的扩展性设计和分析引擎设计, 缺少NGS数据经验\n\n\n—————–+————+—————–+—————-\n\n\n\n\n\n结论现在市面上还没有一款产品能够解决上述大药厂的烦恼, 要真正能够开发一款这样的产品, 可能需要做到如下几点:\n\n易于部署\n易于集成. 方便集成药厂现有的工作流&#x2F;工具, 无论是通过API方式还是其他方式.\nETL的能力. 标准化各种基因数据的能力.\n最好有一个开放的社区支持.\n较平的学习曲线\n易于自定义工作流\n能和其他大数据工具和机器学习平台整合.\n\n","categories":["cloud"],"tags":["cloud, big data"]},{"title":"Data Governance Guide","url":"/2021/05/06/data-warehouse-2021-05-07-data-governance-guide/","content":"What is data governance?根据DGI定义: Data Governance is a system of decision rights and accountabilities for information-related processes, executed according to agreed-upon models which describe who can take what actions with what information, and when, under what circumstances, using what methods 数据治理指的是对数据相关事宜的决策制定与权利控制。具体来说，数据治理是处理信息和实施决策的一个系统，即根据约定模型实施决策，包括实施者、实施步骤、实施时间、实施情境以及实施途径与方法。数据治理是业务流程, 角色, 规则, 标准和指标的集合, 为了让企业能够有效, 高效地使用信息以到达企业的目标. 数据治理需要建立流程和职责使得数据的质量和安全性在业务使用数据时中得到保证. 数据治理定义了谁可以在什么情况下对什么数据进行何种数据的操作. 一个好的数据治理框架需要覆盖策略, 战术, 和操作人员的角色和职责.\n一般情况下, 业务会驱动数据治理的策略, 如哪些数据需要仔细的监管. 所以业务规则是数据治理策略制定的基础之一. 举个例子, 如果一个业务驱动的需求是保证健康相关数据的隐私性, 那么治理策略就应该保证数据流被安全地处理. 留存需求如历史上谁在什么时候改了信息等, 这些需求需要能够满足相关监管的要求, 如GDPR和CCPA.\n良好数据治理的好处\n通用数据的理解. 良好治理能够提供一致的角度, 通用的术语对于数据来说.\n数据质量提升. \n数据地图. 帮助理清数据的来源位置\n360视图. \n一致性合规.\n容易访问.\n\n","categories":["data warehouse"],"tags":["db, data governance"]},{"title":"Slow Change Dimension in DW","url":"/2021/06/17/data-warehouse-2021-06-18-SCD/","content":"What is Slow Change DimensionSCD缓慢变化维是指维度的值会随着时间缓慢变化. 当维度值变化时, 我们可以直接把旧的值覆盖, 也可以存储一行新的值(把旧的记录无效), 还可以加一个字段使用列式存储新的值. 所以基本上SCD有几种类型:\n\ntype 1, 覆盖旧的值, 不保留历史信息\ntype 2, 保留旧的值, 使用一个字段表示它无效, 插入一行新的维度记录. 每行增加effective_date, expiry_date字段\ntype 3, 保留旧的值, 仅增加几个新的字段存储几个历史版本值, 增加effective_date字段\ntype 4, 把历史数据存入另一个表中, 当前维表保存最新的数据\n\nComparison of type2 and type3type2通常情况下可以满足大部分需要保留历史数据的需求, 但是如果一次插入行的记录过多的话, 也可以考虑使用type3. type3的确定是只能保留固定的版本数量, 但是我们不知道需要几个版本记录需要保留.\nHow slow is slow?一般缓慢的程度以一个季度作为分界点, 如果维度一个月就会变化, 就应该以其他方式处理. 但是如上的表述往往是不准确的, 一个维度中可能有几个维度是快速变化的,而有几个不怎么变化, 那么还需要从几个方便考虑:\n\n维度表的行数, 如果行数越多可能越有可能不是缓慢变化维\n维表中一个维度属性和其它属性的耦合关系, 越松耦合越可能这个属性是快速变化维\n第三是如果维表中其它属性都不怎么变化, 而某个属性经常变化, 那么这个属性是快速变化维\n\n针对SCD中某几个属性是快速变化维的情况, 我们有单独的处理方式.\nRapidly Change Dimension那么我们怎么处理快速变化维呢? 很简单, 只需要把这个维度放入事实表就行. 这种维度属性我们叫degenerate dimension退化维度.\n","categories":["data warehouse"],"tags":["data warehouse, scd"]},{"title":"Data Quality Overview","url":"/2021/06/18/data-warehouse-2021-06-19-Data-Quality/","content":"The importance of data quality数据质量对于数据报表至关重要, 数据的准确性(Accuracy), 完整性(Completeness), 一致性(Consistency), 精确性(Precision)和时效性(timeliness)这几个指标是评价数据质量的核心指标. 数据没有质量我们将构建数据仓库的意义将不复存在, 没有人相信一个数据不正确的数据仓库.\nData cleaning &amp; matching数据清洗用于处理脏数据, 同时也用于识别相同的数据, 一般用到三种逻辑:\n\nexact\nfuzzy\nrule-based, 包括incoming data, cross-reference, and internal rules\n\nAction to violated data当一个数据违反数据规则时, 我们有多种处理方案: \n\n拒绝数据进入仓库\n允许数据进入仓库\n修正数据\n\nDQ process数据质量的控制一般分为三步: \n\n检查, \n报告,\n修正.\n\n如下图所示是一般的DQ流程.\n","categories":["data warehouse"],"tags":["data warehouse, data quality"]},{"title":"Metadata in Data Warehouse","url":"/2021/07/12/data-warehouse-2021-07-04-metadata-database/","content":"What is metadata?Metadata元数据是描述数据的数据。举个例子，一是原始单反拍摄的照片一般是RAW格式，这个格式的文件会附带一些属性信息，如Etag, 描述照片拍摄的时间，拍摄的相机，曝光ISO, 分辨率等等信息，这些信息就是照片的元数据。\nWhy do we need metadata?为什么需要元数据, 主要为了向用户解释数据和数据仓库, 让用户更好地理解数据仓库. \nMetadata in data warehouse在数据仓库中存在7种元数据:\n\n数据结构元数据，描述每张表的表结构. 描述NDS, DDS, ODS和staging库中所有表结构, 包括collation(字符序). 这些信息大部分可以通过object catalog view拿到, 但是ETL元数据, DQ元数据, 数据定义元数据也需要结构信息, 所以我们需要单独创建这个库.\n数据定义和数据映射元数据，分别描述事实表和维度表的字段意义和来源。为了避免混淆和误解, 必须有一个清晰, 全公司都能理解的字段定义. 数据映射元数据有时候也叫数据血缘（data linage metadata）, 数据血缘可以用于数据影响性分析。\n数据源的元数据，描述原始数据库表结构和字段意义. 具体包含数据类型, 字符序, 主键, 外键, 视图, 索引和分区.\nETL元数据，描述每个ETL流程中的每个数据流. 描述数据的流向, 所经过的转换, 父流程和定时任务.\n数据质量元数据，描述数据质量规则，对应的风险和措施.\n审计元数据，包含了所有数据仓库中的流程和活动.\n用量（Usage metadata）元数据，描述数据仓库的使用情况.\n\n","categories":["data warehouse"],"tags":["metadata, data warehouse"]},{"title":"企业级主数据治理框架","url":"/2021/10/24/data-warehouse-2021-10-25-master-data-governance/","content":"1. Introduction 介绍1.1 什么是主数据？主数据是以与业务活动相关的通用和抽象概念形式提供业务活动语境的数据，包括业务活动中涉及内部和外部对象的详细信息(定义与标识符)。一般主数据使用业务实体来表示，如客户、产品、雇员、供应商等数据，实体是客观世界的对象(人、组织、地方或事物等)，实体被实例以数据&#x2F;记录的形式表示。\n1.2 为什么需要管理主数据？常见的主数据管理驱动因素有：\n\n满足组织数据的需求。组织中的多个业务领域需要访问相同的数据集，并且这些数据集是完整的、最新的、一致的。\n管理数据质量。主数据管理通过使用统一的表示来定义对组织至关重要的实体，以降低由于数据不一致、质量和差异等问题对决策失误和机会错失的风险。\n管理数据集成的成本。主数据管理需要将关键实体集成到统一的系统中，减少对因对关键实体定义和识别方式变化而产生的额外成本。\n\n图1. 主数据语境关系图\n\n1.3 主数据管理的目标和原则1.3.1 企业主数据管理的目标有：\n确保组织在各个流程中拥有完整、一致、最新和权威的主数据。\n促使企业在各个单元和各应用系统之间共享主数据。\n通过采用标准的、通用的数据模型和整合模式，降低数据使用和数据整合的成本及复杂性。\n\n1.3.2 企业主数据管理应遵循如下指导原则：\n共享数据。需要组织的全局管理，以实现广泛地共享主数据。\n所有权。主数据所有权属于整个组织，而不是属于某个应用系统或部门。在企业，主数据的所有权由信息化发展委员会做最终解释。相关业务记录系统的数据所有权属于相关业务部门，经过数据整合后在数据共享平台分发的主数据所有权转移为整个组织。\n质量问题。主数据需要持续的数据质量监控和管理。\n管理职责。企业信息化发展委员会拥有对主数据的远景和战略制定和监管，下设数据治理委员会负责策略发展和战略目标对齐，数据治理执行层负责具体地数据管理职责，控制和保证主数据的质量。\n变更控制。在给定的时间点，主数据的值代表组织对准确和最新内容的最佳理解。改变数据值的取值规则，应该在有关数据治理执行层监督下谨慎进行，修改数据实体版本需要通过数据治理委员会审批通过，并且做到变更可追溯。\n权限。任何主数据的使用和分发需要通过数据治理委员的审批，并由数据治理执行层执行并妥善管理权限。特定业务主数据的使用需要数据治理委员会分管领导亲自审批，例如人员信息主数据需要首席人力资源官审批才能给下游系统共享使用。\n\n1.4 主数据治理管理层组织架构企业主数据治理组织架构分为管理层和执行层，管理层设信息化发展委员会负责整个集团的数据治理愿景设计和战略规划与监管。信息化发展委员会下设数据治理委员会，负责向上对齐目标，向下监督数据治理的执行和策略发展。数据治理的执行层负责具体的主数据治理工作，包括计划、开发、维护和运营。整个主数据治理组织架构设置如图 2。\n图2. 主数据治理组织架构图\n\n1.5 主数据共享系统架构企业的主数据共享架构采用业务系统本地管理主数据，数据共享平台作为公共平台集成和共享数据，对主数据做治理和展示，并将问题数据反馈给相关业务系统。\n图3. 数据共享平台主数据共享架构\n\n1.6 评估主数据管理情况公司需要定期评估当前主数据管理的能力、成熟度和有效性。需要从如下几点识别公司主数据管理的情况：\n\n哪些角色、组织、地点和事物等实体被反复引用\n哪些数据被用来描述人、组织、地点和事物等实体\n数据是如何被定义和设计的，以及数据颗粒度细化程度\n数据在哪里被创建或源于哪个系统，在哪里被存储、提供和访问\n数据通过组织内的系统时是如何发生变化的\n谁使用了这些数据，为了什么目的\n用什么标准来衡量数据及其来源的质量和可靠性\n\n2.Activities 活动2.1 主数据管理的关键步骤主数据管理的关键步骤包括数据模型管理、数据采集、数据验证、标准化和数据丰富、实体解析、数据分发和共享。\n2.1.1 数据模型管理清晰一致的逻辑数据模型定义对主数据管理至关重要。数据模型必须是企业级定义的术语，并对整个组织所进行的业务相关联。源系统中数据定义的术语不能在企业具有全局意义，所以不能依赖源系统的定义。每个主数据模型都要有严格的版本定义和辅助字段以帮助确定某一时刻此版本主数据是对企业级实体的最佳理解。推荐的主数据附加辅助属性有：\n|—————–+—————|\n\n\n\n主数据元属性\n描述\n\n\n\n正式名称\n通过管理层审核的中英文名，统一企业内部语意\n\n\n—————–+—————\n\n\n\n数据源提供者\n提供数据源系统\n\n\n—————–+—————\n\n\n\n数据源数据创建时间\n标识数据值的创建时间\n\n\n—————–+—————\n\n\n\n数据源数据更新时间\n标识数据值的更新时间\n\n\n—————–+—————\n\n\n\n实体模型版本号\n官方确定的主数据版本号\n\n\n—————–+—————\n\n\n\n实体数据的创建时间\n从源数据整合后的创建时间\n\n\n—————–+—————\n\n\n\n实体数据的更新时间\n分发的数据集最新更新日期\n\n\n—————–+—————\n\n\n\n表1.主数据附加属性\n\n\n2.1.2 数据采集从规划、评估和合并数据源数据到主数据系统必须是一个可靠、可重复的过程。数据采集活动包括:\n\n接受并应对新的数据源采集需求\n使用数据清洗和数据分析工具进行快速、即时、匹配的数据质量评估\n评估数据并将数据整合的复杂性传递给需求者，以帮助他们进行成本效益分析\n尝试数据采集及评估其匹配规则的影响\n为新数据源确定数据质量指标\n确定维护和监控数据质量的责任人\n完成与整体数据管理环境的集成\n\n2.1.3 数据验证、标准化和数据丰富\n验证。识别明显错误或者不正确的数据，并采用对应措施（删除或合并）。\n标准化。确保数据内容符合标准参考数据、标准格式。\n数据丰富。添加可以用于改进实体解析服务的额外属性。\n实体解析和标识符管理。实体解析是指确定两个或多个对现实世界对象的表示是否为同一个对象的过程。通过确定两个或多个表示之间的相似性来确定是否为同一对象。实体解析包括一系列活动（实例提取、实例准备、实例解析、身份管理、关系分析），这些解析活动能够使实体、实例的身份以及实体、实例之间的关系持续地被管理。实例可以通过一个全局标识符把等价的实例关联起来。\n\n2.1.4 数据分发与共享主数据分发与共享需要严格的权限管理和数据安全使用声明。确保下游系统只使用最小的、必要的、可控的数据。\n2.2 评估和评价数据源现有应用中的数据是主数据管理工作的基础。理解这些数据的结构和内容的过程很重要，评估数据源主要是为了评估数据质量，具体包括数据的完整性、唯一性、有效性、一致性、准确性和及时性。对于有些数据实体，可以采用购买标准化数据来实现主数据管理工作，这些参考数据可以和内部数据进行比较，以此来改善企业内部数据的质量。\n2.3 定义架构方法主数据管理的架构方法取决于业务战略、现有数据源平台以及数据本身，特别受数据的血缘和波定性以及延迟性影响。依据企业现有的系统状况，由于缺少部分必要的业务记录系统，数据共享中心架构显得尤为重要，推荐开发数据共享平台，以便管理员可以在数据共享中心维护部分主数据并且共享数据给下游系统。\n2.4 建模主数据主数据管理是一个持续的整合过程。为了实现一致的结果，必须在主题域中为数据建模，定义企业级的主题域实体和属性。主数据定义的术语和属性应该和整个组织所进行的业务相关联，而不是单单取决于某个源系统的数据。构成主数据的属性也应该定义粒度，并且粒度在整个组织具有意义。如果多个数据源存在不同的命名属性，在企业级模型中必须整合成单一属性，并且数据值处于适当的语境中。\n2.5 定义管理职责和维护过程主数据管理组织架构如 1.4 章所述，具体的管理职责和维护方式如图 4 所示，技术解决方案和管理流程需要并存于主数据管理的工作中，技术解决方案用在主记录标识符的匹配、合并和管理工作，管理流程用来对记录进行修复和缺失补进。比如，相关业务部门作为业务系统和数据的所有者负有业务系统数据管理职责，当业务源数据经过主数据共享平台治理，数据所有权发生变更，相关业务数据也成为企业级主数据，同时主数据管理模块向源系统提供问题数据必要的反馈。\n\n图4. 管理职责和维护流程图\n\n2.6 建立治理制度，推动主数据使用在初步完成主数据治理工作后，需要在系统之间建立单向闭环的数据流，以保持系统之间值的一致性。强制要求各下游系统使用主数据，保持主数据的一致。\n3. Guidelines 实施指南3.1 遵循主数据架构企业主数据建设需要遵循如上章描述的体系架构，整合方式需要考虑企业的组织架构、记录系统的数量、数据访问延迟性要求以及消费系统的需求。\n3.2 监测数据流动当数据在共享环境中流动时，应监控相关数据流，以达到如下目的：\n\n显示数据如何在整个组织中共享和使用\n在应用系统中识别数据血缘关系\n辅助进行数据问题的溯源\n展示数据整合和消费整合技术的有效性\n通过消费数据来评估源系统数据传输的及时性\n确定在集成组件中执行业务规则和转换的有效性\n\n3.3 设定主数据共享协议为了确保恰当的访问和使用，数据共享协议需要被建立，协议规定：哪些数据可以被共享、在何种条件下可以被共享和使用。数据共享平台负责人应当建立服务水平协议和指标(SLA), 以衡量共享数据的可用性和分享数据的质量；并建立标准的沟通方法，使所有受到影响的相关方了解问题的存在和补救工作状况。\n3.4 组织和文化变革提高主数据的可用性和质量需要先考虑组织的准备情况、组织的未来使命和愿景。文化变革才是数据治理的中心课题：确定哪些决定由哪些人负责，哪些工作由哪些人负责。本框架建议使用图 2 进行组织和文化架构调整，以适应主数据治理的挑战。在执行层面，当主数据治理执行层认为数据源存在风险，并通知该数据的本地管理员需要开展整改工作时，本地管理员需要及时做出相应的修补措施。\n4. Metrics 度量指标企业的主数据治理状况可以参考如下指标度量：\n4.1 数据质量和遵从性数据质量仪表盘可以用来描述数据质量，其中应该说明实体或相关属性的置信度，包括数据准确性，及时性、唯一性、有效性、完整性和一致性。\n4.2 数据变更活动审核可信的数据血缘对于提高数据共享环境中的数据质量十分重要，需要指标展示数据值的变化率，并且适当调整主数据管理进程中的算法。\n4.3 数据获取和消费数据由上游系统供应，由下游系统和流程使用。需要有记录显示和追踪哪些系统共享数据，哪些系统消费数据。\n4.4 服务水平协议(SLA)应建立服务水平协议并传到给贡献者和订阅者，以提供相应的流程支持，技术问题和数据问题解释。\n4.5 数据管理专员覆盖率这个指标关注对数据内容负责的团队，识别人员覆盖率对数据管理方面的差距。\n4.6 拥有和维护成本从解决方案的角度来看，成本包括基础设施，软件许可证，支持人员，咨询费，培训等。\n4.7 数据共享量和使用情况需要跟踪纳入主数据的数据量和使用情况，以确定数据共享环境的有效性。具体指标有流入和流出数据的定义、纳入和订阅数量和速率。\n","categories":["data governance"],"tags":["master data, data governance"]},{"title":"Three type of data model","url":"/2021/08/08/data-warehouse-2021-08-09-data-modeling-type/","content":"Three Types of Data Models一般主要有三种数据模型数据模型定义:\n\n概念模型\n逻辑模型 \n物理模型\n\n这三种模型可以使用工具来建模工具列表. 我们通常先创建概念数据模型，然后再做逻辑数据模型。\n概念数据模型概念数据模型用于定义高层业务抽象和概念。通常是业务所有者绘制，建模时不用考虑具体系统的约束。使用SQL Server的SSMS可以很好地建模，一般使用SSMS创建概念模型的流程是：\n\n创建数据库关系图\n在可编程性栏目下， 创建用户定义数据类型(可选)\n在关系图画布下，创建只包含主键的维表和事实表。\n使用拖拉关联事实表和维表外键关系\n自动调整表大小和自动布局\n\n逻辑数据模型逻辑数据模型用于指定实体的所有属性，并且识别实体之间的关系。逻辑模型通常是数据架构师定义的，用于业务分析。\n物理数据模型物理数据模型是把具体的逻辑数据模型使用某个数据库去具体实现。数据库开发者通常使用这个物理模型去做具体的开发工作。\n","categories":["data warehouse"],"tags":["data warehouse, data model"]},{"title":"增量ETL源数据的几种方式和细节","url":"/2022/01/09/data-warehouse-2022-01-10-%E5%A2%9E%E9%87%8FETL%E6%BA%90%E6%95%B0%E6%8D%AE%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%92%8C%E7%BB%86%E8%8A%82/","content":"Introduction 介绍ETL是常见的把数据从 OLTP 系统导入数据仓库的方式，一般分为全量，增量，固定范围和推送方式。本文主要讲如何增量抽取，和抽取过程中遇到其它一些问题的解决方法。\n增量抽取增量抽取一般可以使用表中列的时间戳，标识符，交易日期和触发器等组合来判断是否需要增量抽取。使用时间戳，标识符和交易日期的方法大致一样，通过比较元数据中记录的上一次成功etl点和当前时间或位置做比较，进行增量抽取。\n原始数据记录删除了数仓该如何处理？\n方法 1：比较原始数据和数仓表的主键，如果发现原始数据库主键已经不存在，那么需要在数据表做软删除。\n方法 2：如果原始数据库支持删除触发器，那么数仓只需 etl删除审计表或者事件表，标记记录已删除就行。（此方法也是用update,insert）如果希望保存所有的更改记录，使用temporal table时态表是个很好的选择，但是需要业务系统支持。\n\n使用固定范围etl如果有些数据源没有自增键或者可用的时间戳，可用系统赋值的RowID作为参考值，固定范围etl数据。在SQL Server中可用如下代码定位，\nSELECT %%physloc%% AS [%%physloc%%],       sys.fn_PhysLocFormatter(%%physloc%%) AS [File:Page:Slot]FROM Table1\n但是数据的更新还是需要使用checksum来比对，如果字段较多性能还是会有瓶颈。\nalter table table1 add col4 as checksum(col1, col2, col3)alter table table2 add col4 as checksum(col1, col2, col3)goselect * from table1select * from table2select * from table1 t1where not exists( select * from table2 t2where t1.col4 = t2.col4 )go","categories":["data warehouse"],"tags":["etl, data warehouse"]},{"title":"Redis使用总结","url":"/2017/05/08/database-2017-05-09-redis-summary/","content":"Redis是我们常用的内存型key-value数据库, 它有着强大的读写能力(benchmark是几万IOPS). 而最初redis的源码只有2万行, 十分适合学习和研究. 本文主要总结一下redis的devops相关使用. 我觉得使用了redis时主要关心其单线程模型和内存的调度.\n单线程模型Redis使用了单线程架构和I&#x2F;O多路复用模型来实现高性能的内存数据库服务, 当几个客户端同时发送命令时, redis会先将命令存在队列, 然后通过epoll作为IO复用实现, 通过事件一条一条的执行命令.单线程一个好处是避免了线程切换的开销和竞争(单线程自然没有CPU核切换的问题). 但是单线程有一个问题, 对于每个命令的执行时间是有要求的。如果某个命令执行过长，会造成其他命令的阻塞，对于Redis这种高性能的服务来说是致命的，所以Redis是面向快速执行场景的数据库。\n数据类型和内部编码Redis是KV数据库, 这个V的类型大致可以分为5类: Hash&#x2F;String&#x2F;List&#x2F;Set&#x2F;Zset&#x2F;, 每一种数据结构都有多种内部编码实现, 通过type可以看类型, 通过object encoding可以看编码实现.\n127.0.0.1:6379&gt; object encoding hello&quot;embstr&quot;\n一种数据类型有多种实现, 实际使用时到底用了哪一种方法Redis会帮用户做出选择, 使用时可以不必关心细节. 这样做也方便redis更新具体encoding实现的代码.\n内存理解需要了解Redis内存使用情况, 可以使用info memory. Redis进程内消耗主要包括：自身内存+对象内存+缓冲内存+内存碎片.\n\nredis自身存储很小, 可以忽略不计.\n对象存储: 对象内存是Redis内存占用最大的一块，存储着用户所有的数据.\n缓冲内存主要包括：客户端缓冲、复制积压缓冲区、AOF缓冲区。\nRedis默认的内存分配器采用jemalloc，可选的分配器还有：glibc、tcmalloc。内存分配器为了更好地管理和重复利用内存，分配内存策略一般采用固定范围的内存块进行分配.比如当保存5KB对象时jemalloc可能会采用8KB的块存储，而剩下的3KB空间变为了内存碎片不能再分配给其他对象存储。\n\n持久化Redis支持RDB和AOF两种持久化机制，持久化功能有效地避免因进程退出造成的数据丢失问题，当下次重启时利用之前持久化的文件即可实现数据恢复。RDB持久化是把当前进程数据生成快照保存到硬盘的过程，触发RDB持久化过程分为手动触发和自动触发。bgsave命令是执行RDB制作镜像的命令：Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短。运行bgsave命令对应的Redis日志如下：\n* Background saving started by pid 3151* DB saved on disk* RDB: 0 MB of memory used by copy-on-write* Background saving terminated with success\nRedis内部还存在自动触发RDB的持久化机制，例如以下场景：\n\n使用save相关配置，如“save m n”。表示m秒内数据集存在n次修改时，自动触发bgsave。\n如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点\n执行debug reload命令重新加载Redis时，也会自动触发save操作。\n默认情况下执行shutdown命令时，如果没有开启AOF持久化功能则自动执行bgsave。AOF（append only file）持久化：以独立日志的方式记录每次写命令在buffer,然后通过某种机制fsync到硬盘，日志的格式与客户端发给Redis的网络文本一样.重启时再重新执行AOF文件中的命令达到恢复数据的目的。AOF的主要作用是解决了数据持久化的实时性.AOF的工作流程操作：命令写入（append）、文件同步（sync）、文件重写（rewrite）、重启加载（load）.在进行AOF重写工作的时候, redis也会fork一个子进程,由子进程进行文件重写, 主进程继续处理新的请求并将新的aof写入aof_rewrite_buffer, 在子进程完成重写工作后将aof_rewrite_buffer和新的aof合并,形成最终的aof文件.现在主流redis持久化是使用AOF, 但是为了更加快速可以两种技术混合使用.\n\nredis发生阻塞当Redis阻塞时，这时应用方会收到大量Redis超时异常，比如Jedis客户端会抛出JedisConnectionException异常.(CacheCloud是搜狐开源的监控工具可以看看). 发生阻塞一般分为两大原因:\n\n内部原因: 不合理地使用API或数据结构、CPU饱和、持久化阻塞等。不合理使用API是指对复杂度较大的指令执行大量数据操作. redis提供slowlog get &#123;n&#125;查询慢查询, 默认命令执行时间在10ms以上的命令会存在一个定长为128的队列中。慢查询本身只记录了命令执行时间，不包括数据网络传输时间和命令排队时间，因此客户端发生阻塞异常后，可能不是当前命令缓慢，而是在等待其他命令执行. 发现是滥用api造成慢查询后, 可以采取两个方法:\n修改为低算法度的命令，如hgetall改为hmget等，禁用keys、sort等命令。\n调整大对象：缩减大对象数据或把大对象拆分为多个小对象，防止一次命令操作过多的数据. redis-cli --bigkeys可以统计大对象.CPU饱和是指单线程跑满了整个CPU, CPU饱和是非常危险的，将导致Redis无法处理更多的命令，严重影响吞吐量和应用方的稳定性.redis-cli --stat可以查看统计. 当CPU饱和, 垂直扩展是没有用的, 需要水平集群化分摊IOPS.如果只有几百或几千IOPS的Redis实例就接近CPU饱和是很不正常的，有可能使用了高算法复杂度的命令。还有一种情况是过度的内存优化，这种情况有些隐蔽，需要我们根据infocommandstats统计信息分析出命令不合理开销时间.\n\n\n\n对于开启了持久化功能的Redis节点，需要排查是否是持久化导致的阻塞。持久化引起主线程阻塞的操作主要有：fork阻塞、AOF刷盘阻塞、HugePage写操作阻塞。可以执行info stats命令获取到latest_fork_usec指标，表示Redis最近一次fork操作耗时，如果耗时很大，比如超过1秒，则需要做出优化调整，如避免使用过大的内存实例和规避fork缓慢的操作系统等.\n\n外部原因: CPU竞争、内存交换、网络问题等.CPU竞争主要分为: 进程竞争, 绑定CPU竞争.当Redis父进程创建子进程进行RDB&#x2F;AOF重写时，如果做了CPU绑定，会与父进程共享使用一个CPU。子进程重写时对单核CPU使用率通常在90%以上，父进程与子进程将产生激烈CPU竞争，极大影响Redis稳定性。因此对于开启了持久化或参与复制的主节点不建议绑定CPU。内存交换(swap)对redis有极大的性能影响, 可以查看cat /proc/&#123;redis process id&#125;/smaps | grep Swap内存交换信息.如果交换量都是0KB或者个别的是4KB，则是正常现象，说明Redis进程内存没有被交换。预防内存交换的方法有：\n保证机器充足的可用内存。\n确保所有Redis实例设置最大可用内存（maxmemory），防止极端情况下Redis内存不可控的增长。\n降低系统使用swap优先级，如echo 10&gt;/proc/sys/vm/swappiness网路问题分为: 连接拒绝、网络延迟、网卡软中断等。 拒绝连接的情况又有网路闪断, redis拒接连接和连接溢出. 可以查看redis-cli -p 6384 info Stats | grep rejected_connections查看redis拒绝连接数, 默认连接数是10000. 连接溢出是指操作系统或者Redis客户端在连接时的问题. 操作系统一般会对进程使用的资源做限制，其中一项是对进程可打开最大文件数控制，通过ulimit -n查看，通常默认1024。由于Linux系统对TCP连接也定义为一个文件句柄，因此对于支撑大量连接的Redis来说需要增大这个值，如设置ulimit -n 65535，防止Too many open files错误;系统对于特定端口的TCP连接使用backlog队列保存, linux默认是128, Redis默认的长度为511，通过tcp-backlog参数设置。如果Redis用于高并发场景为了防止缓慢连接占用，可适当增大这个设置，但必须大于操作系统允许值才能生效, 使用echo 511&gt;/proc/sys/net/core/somaxconn命令进行修改。\n\n\n\nSentinelRedis Sentinel是Redis的高可用实现方案.\n","categories":["database"],"tags":["database, redis"]},{"title":"扩展数据库注意事项","url":"/2018/10/02/database-2018-10-03-%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E4%BA%8B%E9%A1%B9/","content":"关系型数据库扩展是一个永恒的话题, 尤其是partitioning和一致性方面的问题. 以下结合网上大神资料,做一个大致的总结.\n索引想要快速查询, 必须建立合适的索引. 检查经常查询的sql语句, 分析sql是否使用了正确的索引十分重要. 如mysql可以临时开启general_log一两天, 查出使用最频繁的sql, 使用explain sql查看查询执行计划, 然后可以确定是否需要加索引. \nselect version(); //查看版本set global general_log=1;//开启查询日志, 需要管理员权限.set global log_output=&#x27;TABLE&#x27;;//general_log支持输出到tableselect * from mysql.general_log;//查询所有sql语句的使用select * from (select argument, count(*) as number from mysql.general_log  group by argument)t order by number DESC;//降序排列使用最多的sql语句\n\n反规范化join是十分费时的操作, 可以使用冗余数据, 做到同样的join查询效果.\n数据库复制数据库的复制是数据扩展性常见话题, 具体操作流程是通过change log把主数据库的改变replay到从数据库中, 保证从数据库有多个数据的复制. 这里主要的注意点是如何保证数据的一致性问题.\n表的分区分表分库包含两种方式: 垂直&#x2F;水平分割. 垂直分割是把一张表的列分开, 成为两张表;或者是把不同表放在不同的数据库中. 这里需要注意的是垂直分割的表如果有join需求, 那么不要做分割.水平分割是把一个表中的不同行分入不同的表库中, 这里主要注意把相同业务逻辑的行放在一起.\n事务处理最好能把OLAP和OLTP频繁操作的表分为不同的库, 也就是所谓的读写分离. 如果是OLTP频繁的操作, 那么需要把长时间操作移到业务逻辑实现, 同时设定合适的Isolation隔离等级.如果使用缓存配合数据库, 那么在用户提交的时候需要验证数据的时间戳, 保证提交时读取的数据是最新的. 如果不是那么需要终止提交, 刷新缓存中数据后再重新提交.\nORM使用ORM可以简化持久层逻辑, 但是对于扩展性并不是很友好. 在使用ORM的时候需要调优考虑如下问题:\n\n当一个对象被引用的时候, 这个对象会被何种程度引用\n如果一个集合被引用的时候, 需要考虑O&#x2F;R映射器是否会包含整个集合数据?\n当一个对象需要扩展使用join时, 需要考虑如何选择, 是选择多个single-join查询还是单个multiple-join查询.\n\n","categories":["database"],"tags":["db"]},{"title":"一次奇怪的Docker Daemon Error","url":"/2018/01/17/docker-2018-01-18-strange-docker-error/","content":"前言环境：\ncat /etc/centos-releaseCentOS Linux release 7.1.1503 (Core) ansible --versionansible 2.3.1.0  config file = /etc/ansible/ansible.cfg  configured module search path = Default w/o overrides  python version = 2.7.5 (default, Aug  4 2017, 00:39:18) [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)]\n\n问题出现用ansible安装部署docker时发现有容器一直起不来。安装的docker daemon是按docker官网https://docs.docker.com/engine/installation/linux/docker-ce/centos/#install-docker-ce安装的，然后log发现报错Error response from daemon: OCI runtime create failed: unable to retrieve OCI runtime error。\n排查搜索查看issue,发现是安装的docker-ce版本太新!而centos太旧，Centos7.1选择docker-ce-17.06.3.ce这个版本可用，17.12还是太新！\ndocker infoServer Version: 18.01.0-ceStorage Driver: devicemapper\n\n措施，其他问题再现卸载docker-ce, 重新安装。然后发现容器还是起不来！尝试删除&#x2F;var&#x2F;lib&#x2F;docker, 报错\nerror: driver \\&quot;devicemapper\\&quot; failed to remove root filesystem for 6f009dff997d9fe3f19c736d6dd662d7ff55cea2ec04ac5bba287b83684cac5b: remove /var/lib/docker/devicemapper/mnt/0efe8e6bc86a2ff1e1877979275c36d119995043ce231aeed661c15d26873692: device or resource busy\n\n解决看看到底是mount到其它什么地方了\nfind /proc/*/mounts | xargs grep  0efe8e6bc86a2ff1e187797927grep: /proc/1449/mounts: No such file or directory/proc/7280/mounts:/dev/mapper/docker-8:6-67114038-0efe8e6bc86a2ff1e1877979275c36d119995043ce231aeed661c15d26873692 /var/lib/docker/devicemapper/mnt/0efe8e6bc86a2ff1e1877979275c36d119995043ce231aeed661c15d26873692 xfs rw,relatime,nouuid,attr2,inode64,logbsize=64k,sunit=128,swidth=128,noquota 0 0ps 7280  PID TTY      STAT   TIME COMMAND  7280 ?        Ssl    0:03 /usr/libexec/colordkill -9 7280 yum install -y docker-ce-17.06.3.ce systemctl start docker\n\n\n\n\n\n","categories":["docker"],"tags":["docker"]},{"title":"使用Docker Cloud自动化部署","url":"/2018/02/01/docker-2018-02-02-docker-cloud-autodeploy/","content":"Docker cloud介绍Docker cloud有点像云上的jenkins docker自动化构建部署工具，可以把云上的git仓库，部署的节点虚机整合在一起，做到提交代码就可以自动构建，测试，部署。\n例子: 设置aliyun作为部署节点docker cloud可以结合github, bitbucket仓库做到每次有新的pull request都进行一次重新构建。但是由于我本地直接用pycharm结合docker remote进行开发，所以开发测试完，我的image也已经构建好，所以我没有把git仓库和docker cloud整合，而是直接push到docker hub private repo。然后设置aliyun作为部署节点，当有image更新时自动重新部署。\n\n设置aliyun node\n\n选中图中Infrastructure下的Node选项\n点击bring your own node, 将截图中脚本在aliyun的虚机上运行。注意！！这里虚机必须没有安装过docker daemon. 同时安全组开通6783&#x2F;tcp，6783&#x2F;udp和2375&#x2F;tcp端口。\n等待5分钟，查看timeline, 直到log显示下图，正确部署节点。注意！！由于cloud.docker.com是在aws上，当中可能网络timeout,导致部署失败。这就需要删除dokcercloud-agent, 重新安装。\n\nyum remove dockercloud-agentrm -rf /etc/dockerrm -rf /etc/dockercloud-agent\n设置自动部署service\n\n从hub仓库中创建一个service, 设置成auto redeploy. 注意当前只支持image的latest tag自动重新部署。\n选中AUTOREDEPLOY， 其他可以默认设置。\n\n\n重新push image, 测试是否设置成功，在service的timeline应该看到如下截图，说明设置成功。\n\n\n总结这里只是简单介绍一下docker cloud的应用。还有一些其他功能比如swarm等商业功能没有触及，但是总体感觉docker越来越商业化。\n","categories":["docker"],"tags":["docker"]},{"title":"三种docker数据持久的方式","url":"/2018/02/18/docker-2018-02-19-docker-container-data-persistent/","content":"docker提供三种方式把数据挂载到容器\nvolumesvolumes是宿主机上docker管理的文件系统， 比如在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;。 一个volume可以同时被多个容器挂载。使用docker volume create --driver可以指定远程的文件系统。\nbind mounts宿主机其他任意不是docker管理的文件系统\ntmpfstmpfs是挂载在宿主机的内存中，可以用于临时挂载非持久化数据，比如secret. –tmpfsdocker17.08以后可以考虑使用–mount同一参数\n\n","categories":["docker"],"tags":["docker"]},{"title":"Grid and Flex Design","url":"/2019/03/18/frontend-2019-03-19-Grid-and-Flex/","content":"Flex Background SummaryFlexbox layout布局主要提供了一种为容器(container)中项目(item)布局, 对齐和分布空间的方式. 这种flex的布局方式可以不需要知道空间的大小或者动态改变大小.Flex主要的想法是给容器能够灵活地改变容器内项目的高度&#x2F;宽度&#x2F;排序, 使其能适应当前的空间.一个flex的容器能够把它的项目扩展到多余的空间, 或者收缩大小防止屏幕变小时项目溢出.Floxbox的布局和传统的布局不同, 它不固定排列的方向(而block基于垂直方向, inline基于水平方向).Flowbox布局比较适合组件和小型布局, 而Grid适合更大的布局.\nGrid Background SummaryGrid Layout是一个二维的布局系统, 既可以处理行又可以处理列. 应用GridCSS的父元素称为Grid Container, 子元素称为Grid Items.implicit grid and explicit grid隐含和显式网格, 隐含网格指网格项目多出来或者网格项目布局在显式网格外面的情况.\nGrid属性表Grid 容器的属性:\ndisplay: grid|inline-grid;grid-template-columns: &lt;track-size&gt; ... | &lt;line-name&gt; &lt;track-size&gt; ...;grid-template-rows: &lt;track-size&gt; ... | &lt;line-name&gt; &lt;track-size&gt; ...;grid-template-areas: &quot;&lt;grid-area-name&gt;|.|none| ...&quot;                     &quot;...&quot;;grid-template: none|&lt;grid-template-rows&gt;/&lt;grid-template-columns&gt; | &lt;line-names&gt;?&lt;string&gt;&lt;track-size&gt;?               &lt;line-names&gt;?+/&lt;explicit-track-list&gt;?;grid-column-gap: &lt;line-size&gt;grid-row-gap: &lt;line-size&gt;grid-gap: &lt;grid-row-gap&gt; &lt;grid-column-gap&gt;justify-items: start|end|center|stretch;align-items: start|end|center|stretch;place-items: &lt;align-items&gt; / &lt;justify-items&gt;;justify-content: start|end|center|stretch|space-around|space-between|space-evenly;align-content: start|end|center|stretch|space-around|space-between|space-evenly;place-content: &lt;align-centent&gt;/&lt;justify-centent&gt;;grid-auto-columns: &lt;track-size&gt; ...;grid-auto-rows: &lt;track-size&gt; ...;grid-auto-flow: row|column|row dense|column dense;grid: &lt;grid-template&gt; | &lt;grid-template-rows&gt; / [ auto-flow &amp;&amp; dense? ] &lt;grid-auto-columns&gt;? | [ auto-flow &amp;&amp; dense? ] &lt;grid-auto-rows&gt;? / &lt;grid-template-columns&gt;\nGrid 项目的属性:\ngrid-column-start: &lt;number&gt;|&lt;name&gt;|span &lt;number&gt;|span &lt;name&gt;|autogrid-column-end: &lt;number&gt; | &lt;name&gt; |span &lt;number&gt; | span &lt;name&gt; |autogrid-row-start: &lt;number&gt; | &lt;name&gt; |span &lt;number&gt; | span &lt;name&gt; |autogrid-row-end: &lt;number&gt; | &lt;name&gt; |span &lt;number&gt; | span &lt;name&gt; |autogrid-column: &lt;start-line&gt; / &lt;end-line&gt; | &lt;start-line&gt; / span &lt;value&gt;;grid-row: &lt;start-line&gt; / &lt;end-line&gt; | &lt;start-line&gt; / span &lt;value&gt;;grid-area: &lt;name&gt; | &lt;row-start&gt; / &lt;column-start&gt; / &lt;row-end&gt; / &lt;column-end&gt;;justify-self: start | end | center | stretch;align-self: start | end | center | stretch;place-self: &lt;align-self&gt;/&lt;justify-self&gt;;\n\n","categories":["frontend"],"tags":["frontend"]},{"title":"React函数式组件的状态","url":"/2019/04/17/frontend-2019-04-18-React-function-component-state/","content":"React无状态组件React的Component分为有状态的class component和无状态的function component, class component的好处是可以完全控制组件的生命周期, 坏处是写起来麻烦. function component的好处是可以使用高阶函数式的编程方式编写代码, 缺点是没有状态可以控制.所以一般需要状态初始化或者其他一些状态操控时, 以前可以用recompose, 使用HOC让组件带有状态, 但是后来这个库的作者加入了React团队, v16.8版本后, 我们应该使用Hooks来管理组件状态和生命周期.\nrecompose和hooks写法对比使用recompose给组件设置状态及其他类似componentDidMount的功能时, 需要先定义好相应的状态和生命周期函数, 然后compose进组件:\nconst &#123; Component &#125; = React;const &#123; compose, lifecycle, branch, renderComponent &#125; = Recompose;const withUserData = lifecycle(&#123;  state: &#123; loading: true &#125;,  componentDidMount() &#123;    fetchData().then((data) =&gt;      this.setState(&#123; loading: false, ...data &#125;));  &#125;&#125;);const enhance = compose(  withUserData);const User = enhance((&#123; name, status &#125;) =&gt;  &lt;div className=&quot;User&quot;&gt;&#123; name &#125;—&#123; status &#125;&lt;/div&gt;);const App = () =&gt;  &lt;div&gt;    &lt;User /&gt;  &lt;/div&gt;;\n而如果使用Hooks那么改写起来方便一点.\nimport React, &#123;useState, useEffect&#125; from &#x27;react&#x27;const User = (&#123;name, statue&#125;) =&gt;&#123;    const [loading, toggleLoading] = useState(true); //set default loading=true    useEffect( () =&gt;&#123;        fetchDate().then((data) =&gt; loading = false) ; //useEffect替代componentDidMount, 主要不会合并状态!    &#125;    return (&lt;div className=&quot;User&quot;&gt;&#123; name &#125;—&#123; status &#125;&lt;/div&gt;)    &#125;    const App = () =&gt;  &lt;div&gt;    &lt;User /&gt;  &lt;/div&gt;;&#125;","tags":["React, hooks"]},{"title":"Apollo client的缓存机制","url":"/2019/04/07/frontend-2019-04-09-apollo-cache/","content":"Apollo client2.0的缓存实现Apollo client2.0使用apollo-client-inmemory作为客户端数据的缓存实现, 主要使用包中的InMemoryCache作为data store来缓存数据. InMemoryCache除了作为客户端缓存的功能外, 还有一个好处是只有当遵循特定的标识符规则(给缓存加特定的id), 每次对后端做mutation后可以自动更新缓存.\nInMemoryCache的配置引入cache:\nimport &#123;InMemoryCache&#125; from &#x27;apollo-client-inmemory&#x27;;const cache = new InMemoryCache();\n\nInMemoryCache的构造器可以有如下配置:\n\naddTypename: boolean, 指定是否需要在document中添加__typename, 默认为true.\ndataIdFromObject, 由于InMemoryCache是会normalize数据再存入store, 具体做法是先把数据分成一个个对象, 然后给每个对象创建一个全局标识符_id, 然后把这些对象以一种扁平的数据格式存储. 默认情况下, InMemoryCache会找到__typename和边上主键id值作为标识符_id的值(如__typename:id). 如果id或者__typename没有指定, 那么InMemoryCache会fall back查询query的对象路径. 但是我们也可以使用dataIdFromObject来自定义对象的唯一表示符:\n\nimport &#123; InMemoryCache, defaultDataIdFromObject &#125; from &#x27;apollo-cache-inmemory&#x27;;const cache = new InMemoryCache(&#123;  dataIdFromObject: object =&gt; &#123;    switch (object.__typename) &#123;      case &#x27;foo&#x27;: return object.key; // use `key` as the primary key      case &#x27;bar&#x27;: return `bar:$&#123;object.blah&#125;`; // use `bar` prefix and `blah` as the primary key      default: return defaultDataIdFromObject(object); // fall back to default handling    &#125;  &#125;&#125;);\n\nfragmentMatcher, fragment matcher默认使用heuristic fragment matcher\ncacheRedirects(以前叫cacheResolvers, customResolvers), 在发出请求之前将查询重定向到缓存中的另一个条目的函数映射。\n\n自动缓存更新假设我们有一个query:\n&#123;    post(id: &#x27;4&#x27;)&#123;        id        score    &#125;&#125;\n然后我们再做一个mutation\nmutation &#123;    updatePost(id: &#x27;4&#x27;)&#123;        id        score    &#125;&#125;\n如果保持这个id匹配, 每次更新都会自定更新data store中score字段的数据, 如果query有多个字段, 那么只要mutation的结果数据尽量保持更新前一次的query的数据一致, 就可以利用上诉特点保持cache的数据鲜活.\n和Cache直接交互可以使用apollo client的类方法直接对cache做读写操作. 方法有: readQuery, readFragment, writeQuery,writeFragment.\n\nreadQuery, 从cache中读取数据, 有一个字段没有存在则会报错.\n\nconst &#123; todo &#125; = client.readQuery(&#123;  query: gql`    query ReadTodo($id: Int!) &#123;      todo(id: $id) &#123;        id        text        completed      &#125;    &#125;  `,  variables: &#123;    id: 5,  &#125;,&#125;);\n\nreadFragment, 读取已有数据的片段, 如果某个字段不存在则报错\n\nconst todo = client.readFragment(&#123;  id: &#x27;5&#x27;,  fragment: gql`    fragment myTodo on Todo &#123;      id      text      completed    &#125;  `,&#125;);\n\nwriteFragment和writeQuery, 用法和read差不多, 除了需要多一个参数data:\n\nclient.writeFragment(&#123;  id: &#x27;typename:5&#x27;, //复合键用于表示cache中数据  fragment: gql`    fragment myTodo on Todo &#123;      completed    &#125;  `,  data: &#123;    completed: true,  &#125;,&#125;);\n\n忽略cache有两者情况可能需要绕过缓存, 一种是想直接访问后端然后写入缓存, 另一种是完全不使用缓存(适合敏感信息).\nclient.query(&#123;    query: gql(queries.getUser),    fetchPolicy: &#x27;network-only&#x27;  // &#x27;no-cache&#x27;&#125;);\nmutation后cache的更新refetchQueries是mutation后更新cache的简单方法, 但是它会从后端再做一次请求而显得不那么优秀. Mutation组件有一个updateprop可以用于手动更新cache, 而不用重新fetch.\nimport CommentAppQuery from &#x27;../queries/CommentAppQuery&#x27;;const SUBMIT_COMMENT_MUTATION = gql`  mutation SubmitComment($repoFullName: String!, $commentContent: String!) &#123;    submitComment(      repoFullName: $repoFullName      commentContent: $commentContent    ) &#123;      postedBy &#123;        login        html_url      &#125;      createdAt      content    &#125;  &#125;`;const CommentsPageWithMutations = () =&gt; (  &lt;Mutation mutation=&#123;SUBMIT_COMMENT_MUTATION&#125;&gt;    &#123;mutate =&gt; &#123;      &lt;AddComment        submit=&#123;(&#123; repoFullName, commentContent &#125;) =&gt;          mutate(&#123;            variables: &#123; repoFullName, commentContent &#125;,            update: (store, &#123; data: &#123; submitComment &#125; &#125;) =&gt; &#123;              // Read the data from our cache for this query.              const data = store.readQuery(&#123; query: CommentAppQuery &#125;);              // Add our comment from the mutation to the end.              data.comments.push(submitComment);              // Write our data back to the cache.              store.writeQuery(&#123; query: CommentAppQuery, data &#125;);            &#125;          &#125;)        &#125;      /&gt;;    &#125;&#125;  &lt;/Mutation&gt;);","categories":["frontend","apollo"],"tags":["frontend"]},{"title":"D3-selection总结","url":"/2019/04/18/frontend-2019-04-19-D3-selection/","content":"什么是D3.selections?D3的selection概念其实很简单, 就是一组元素节点. 具体代码表达就是d3.selectAll(&#39;div&#39;), 所有选中的div就是selection,有的翻译叫它选择集, 然后基于这个selection就可以做各种操作.\nD3-selection在selection上我们可以做到操作有: \n\n设置属性attribute\n设置样式\n设置属性property\n修改HTML或text内容\n等等…\n\n在绑定数据后返回的新selection上使用data join的enter,exit, append,remove, 我们可以增删改数据对应的元素.selection的方法返回一般是当前selection的副本, 或者一个新的selection, 这样能使用链式方法的形式给选中的selection做相应的处理.selection是不可变的immutable, 但是元素是可变的.\nD3-join 方法\nselection.data([data[,key]]): 将指定的数据和选中的元素进行绑定, 返回一个新的selection. data可以是任意值(数字或对象)的数组, 或者是一个返回一组矩阵的函数. 当一个数据绑定到元素时, 数据将会绑定在元素的__data__property, 这样再次select的时候,数据仍旧保持绑定.\ndata数据是会被分配给selection的每个组, 如果selection有多个组(如d3.selectAll…selectAll), 那么data参数应该以函数的形式指定. 如下图所示一个selection对象_groups数组中有多个对象.\ndata方法中含有一个key参数可以用来指定data中的数据按什么方式绑定到元素, 默认不加参数采用按索引位置顺序一一绑定.一个例子:\n&lt;div id=&quot;Ford&quot;&gt;&lt;/div&gt;&lt;div id=&quot;Jarrah&quot;&gt;&lt;/div&gt;&lt;div id=&quot;Kwon&quot;&gt;&lt;/div&gt;&lt;div id=&quot;Locke&quot;&gt;&lt;/div&gt;&lt;div id=&quot;Reyes&quot;&gt;&lt;/div&gt;&lt;div id=&quot;Shephard&quot;&gt;&lt;/div&gt;var data = [    &#123; name: &#x27;Locke&#x27;, number:14 &#125;,    &#123; name: &#x27;Ford&#x27;, number: 53 &#125;,    &#123; name: &#x27;Kwon&#x27;, number: 3 &#125;,    &#123; name: &#x27;Shephard&#x27;, number: 38 &#125;,    &#123; name: &#x27;Reyes&#x27;, number: 18 &#125;,    &#123; name: &#x27;Jarrah&#x27;, number: 88 &#125;]d3.selectAll(&quot;div&quot;)    .data(data, function (d) &#123;        return d ? d.name : this.id    &#125;)    .text(function (d) &#123;        return d.number;    &#125;);\n上面这个例子达到的效果是让每个元素的text按div的#id和data映射的关系来展示展示数字.结果如下:\n53883141838\nupdate和enter的selections以数据的顺序返回, 而exitselection保留原来的selection顺序.如果data方法不传入参数, 方法返回选中元素的数据数组.\n\nselection.join(enter[,update][,exit]): 给绑定的数据做对应的增加&#x2F;移除&#x2F;排序元素操作, 返回合并的enter&#x2F;updateselection. 想要更加颗粒度地控制join中的是三个操作, 可以显式地传入enter, update, exit方法来控制元素:\nsvg.selectAll(&quot;circle&quot;).data(data).join(  enter =&gt; enter.append(&quot;circle&quot;).attr(&quot;fill&quot;, &quot;green&quot;),  update =&gt; update.attr(&quot;fill&quot;, &quot;blue&quot;))  .attr(&quot;stroke&quot;, &quot;black&quot;);\n\nselection.enter(): 返回enter selection. 什么是enter selection呢? 其实就是数据多于元素的情况下, 需要预留位置新给元素.而这个预留的palceholder就是返回值. 在使用data()函数后, 当前selection对象会增加_enter和_exit属性,如上图所示, 绑定数据后,_enter的值是EnterNode数组, 这时候使用enter()就会进入_enter属性(就是返回enter selection了), 把还没有找到DOM的数据找出来调用append()生成最终需要生成的DOM.如下图所示, 调用append后,最终要多余生成的rect就生成了.\n\nselection.exit(): 返回exitselection, 就是返回那些没有数据可以再绑定的元素.\n\nselection.datum([value]): 读取或设置选中的selection的__data__.\nd3.selectAll(&#x27;div&#x27;).datum(33).text(d=&gt;d)  //所有的div渲染成33\n\n","categories":["frontend"],"tags":["D3, data-driven"]},{"title":"小探微前端","url":"/2022/03/18/frontend-2022-03-19-microfront-intro/","content":"前言关键好处和缺点 Key Pros&amp;Cons of Micro frontend微前端的好处类似微服务，主要表现在:\n\n分开的前端代码仓库，每个仓库代码量少\n每个小团队有更多的自主权，解耦相互依赖\n重构或更新部分代码的代价更小\n\n微前端的缺点主要有:\n\n多个前端仓库可能导致依赖包的重复，这样客户端会多下载相关医疗\n高度的自治可能会导致开发的碎片化\n\n","categories":["frontend"],"tags":["frontend, micro"]},{"title":"Kubernetes Pod含义","url":"/2018/01/03/kubernetes-2018-01-04-Kubernetes-pod/","content":"\n官网文档 https://kubernetes.io/docs/concepts/workloads/pods/pod/\n\n什么是Pod？Pod的本身含义是一群鲸鱼的意思, 而Docker的Logo刚好是🐳。所以简单说pod就是一组容器。（不一定是docker container) pod里的容器共享存储，网络和运行的容器环境。共享的东西包括cgroup, 命名空间, Ip, 端口和其他隔离方面的东西\npod内部通讯由于pod里面的容器共享ip, 容器间的通讯可以通过内部进程通信（SystemV semaphores， POSIX shared memory）或localhost.\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"Tomcat 8.5.35 简介","url":"/2017/11/28/java-2017-11-29-tomcat-8-5-35-intro/","content":"今天看了一下官网的Tomcat介绍, 特地记录一下相关信息. Tomcat是一个开源的Java Servlet Container, 这个容器提供了一个Java运行的HTTP web的环境.\n一个术语Context: 一个Context就是一个web应用.\nTomcat解压后包含的相关目录和文件\n/bin 里面的脚本用于启动, 停止, 和其他如设置环境变量的功能. *.sh是Unix系统, *.bat是Windows系统.\n/conf 里面有一些配置文件和DTD(Document Type Definition). 最重要的是Server.xml, 是容器的主要配置文件.\n/logs log文件\n/webapps 应用所在处\n\nCATALINA_HOME 和 CATALINA_BASECATALINA_HOME是tomcat安装的根目录, CATALINA_BASE代表了一个tomcat实例运行的配置根目录. 如果一台服务器有多个tomcat实例在跑, 可以使用这个参数分别启动不同实例. 总的来说, CATALINA_HOME的位置包含了静态文件如.jar和二进制文件, CATALINA_BASE位置包含了配置文件, 日志, 部署的应用和其他运行所需的依赖.使用同一个CATALINA_HOME和多个CATALINA_BASE的好处是:\n\n容易更新tomcat, 由于共享CATALINA_HOME中.jar文件和二进制文件, 我们可以只用更新一个tomcat, 但是运行多个tomcat实例.\n防止重复*.jar文件.\n共享一些脚本, 如setenv脚本\n\nRUNNING.txtRUNNING.txt这个文件包含所有用于在各个平台运行tomcat的信息, 需要仔细看一下.在Unix安装tomcat的时候, 需要把JAVA_HOME环境变量设置于JDK的基目录.\n使用tomcat开发在Servlet API Specification 2.2以后, 公认使用Web Application Archive（WAR)作为网页应用的标准格式. 所以一个打包的web应用是有标准的层级布局的。比如把HTML和JSP页面放在一起作为用户交互， 然后部署的时候管理员给你的应用设置一个context path， 比如catalog/, 那么一个请求到catalog/index.html就会取你项目根目录的index.html文件.\n标准的WAR目录结构打包一个标准的WAR， 一般会在项目中有如下目录结构：\n\nhtml, jsp, etc这些文件是用户可以在客户端访问的文件， 可以在一个文件夹或分成多个目录。\n/WEB-INF/web.xml这个文件是Web Application Deployment Descriptor, 代表应用部署的描述文件.这个xml文件描述了应用所组成的部分:包括servlet和其他组件; 还有初始化参数和容器级别的安全限制.\n/WEB-INF/classes/包含了java类文件, 包含所有没有打成jar文件的java文件.如果你的java类以package的形式组织, 那么可以在/WEB-INF/classes/com/...class找到.\n/WEB-INF/lib/这个目录包含了jar包在/WEB-INF/lib/和/WEB-INF/classes/中的所有java类是对整个应用可见的.\n\n共享Library文件大部分servlet容器如tomcat支持预安装jar文件, 然后使它们可以被web应用可见(不需要被应用引入).  $CATALINA_HOME/lib是tomcat安装共享代码的地方. Tomcat使用class loader来定位和共享这些类. 首先, Tomcat会安装java.lang.ClassLoader实现的class loader, 让web应用和容器的组成部分能够访问不同的类和资源. 这个机制实现了Servlet SpecificationV2.4的9.4和9.6章节提出的技术要求.\nclass loader的大致情况java环境中, class loader是父子树结构. 通常情况下, 当一个class loader需要加载某个类或者资源时, 它会首先去请求它的父class loader, 如果父class loader不能加载某个类或者资源才会从它自己的仓库查找. 注意, web应用的class loader可能有一点不一样, 但是总体原则类似.\nclass loader定义tomcat初始化时会创建如下class loader:\n\nBootstrap. 这个class loader包含了JVM的基本运行类和$JAVA_HOME/jre/lib/ext中的系统扩展jar包.\nSystem. 这个class loader初始化所有在CLASSPATH定义的内容. 这个路径中所有的类可以被tomcat内部类和应用看见, 除了tomcat的catalina启动脚本($CATALINA_HOME/bin/catalina.sh).这个启动脚本会忽略CLASSPATH环境变量, 而从这个脚本启动的system class loader将会从如下仓库构建:\n$CATALINA_HOME/bin/bootstrap.jar, main()函数用于初始化tomcat server和class loader实现类.\n$CATALINA_BASE/bin/tomcat-juli.jar或者CATALINA_BASE/bin/tomcat-juli.jar. 日志实现类.\n$CATALINA_HOME/bin/commons-daemon.jar Apache Commons Daemon 项目中的类.\n\n\nCommon. 这个class loader包含了所有能让Tomcat内部类和所有应用可见的类. 这个class loader的搜索路径在$CATALINA_BASE/conf/catalina.propeties文件的common.loader属性定义.common.loader=&quot;$&#123;catalina.base&#125;/lib&quot;,&quot;$&#123;catalina.base&#125;/lib/*.jar&quot;,&quot;$&#123;catalina.home&#125;/lib&quot;,&quot;$&#123;catalina.home&#125;/lib/*.jar&quot;\n\n","categories":["java"],"tags":["java, tomcat"]},{"title":"Kubernetes的service","url":"/2018/02/21/kubernetes-2018-02-22-k8s-service/","content":"\nhttps://kubernetes.io/docs/concepts/services-networking/service/\n\nService定义简单来说，service是一组pods和访问这些pods规则的逻辑抽象， 它的作用就是代理pods和端口映射（具体的实现似乎是通过复杂的iptable路由NAT）。Service是一个REST对象，这个对象将所有通过某个node端口进来TCP或者UDP包转发给下游pod， 默认上游的端口直接映射到下游pod，比如80—&gt;80，但是也可以指定下游端口为字符串（将pod端口取个名称）， 这样下游不同的pod端口可以对应同一个端口名，大大提供了便利性。service定义代码如下，通过对apiserver post这组定义，可以产生一个新的service。（对于原生k8s服务，还会生成一个同名的Endpoints用来更新pod的地址）。每个node里有一个进程kube-proxy会分配的虚拟IP（cluster ip）。kube-proxy是一个守护进程, 管理着每个节点的service的虚拟IP. service既可以管理kubernetes原生的服务（通过更新EndpointsAPI)，也可以抽象非k8s服务.(通过提供VIP)k8s的Serivce支持TCP和UDP两种协议，默认是TCP.下面这个例子执行kubectl create -f my-service.yml后，将会生产一个叫my-service Service对象和一个叫my-service的Endpoints对象（其实就是一组pods的ip)，Service负责将流量导入到某组pods，Endpoints负责具体分配给某个pod。\nkind: ServiceapiVersion: v1metadata:  name: my-servicespec:  selector:    app: MyApp  ports:  - protocol: TCP    port: 80  #service port    targetPort: 9376  # pod port\n\n抽象外部服务上面的例子使用label selector找到pods， 但是也有可能服务不是kubernetes管理的，这时候可以使用non selector service + endpoints抽象一个外部服务。\nkind: ServiceapiVersion: v1metadata:  name: my-servicespec:  ports:  - protocol: TCP    port: 80    targetPort: 9376\n没有selector的Service是不会生成Endpoint的，需要手动把对应的非k8s服务Endpoint映射到service.\nkind: EndpointsapiVersion: v1metadata:  name: my-servicesubsets:  - addresses:      - ip: 1.2.3.4    ports:      - port: 9376\n如果在云上使用类似RDS之类的服务，没有IP,可以使用externalName来定义service. 这样就会使用DNS来解析地址，不会使用代理或者端口映射。\nkind: ServiceapiVersion: v1metadata:  name: my-service  namespace: prodspec:  type: ExternalName  externalName: my.database.example.com\n虚拟IP和service代理每一个k8s节点都有一个kube-proxy服务来管理service的虚拟IP. 有三种实现service代理模式： userspace, iptables, ipvs. 在v1.0版本，service是一个4层通信协议，直到TCP层，所以代理只在用户空间进行。v1.1加入了ingress和iptables proxy, 实现了7层service.\n\nuserspace, 这个模式下当用户访问service时，iptables中有规则会把流量从service对应的port转发到kube-proxy在node创建的随机端口，随机端口再代理到下游pods。这个模式下kube-proxy会监控kubernetes master的对Service和endpoints的增删，并会给每一个service随机创建一个本地node端口，任何到这个端口的连接将会被代理到service后面pods.（具体是哪一个pod取决于service的SessionAffinity)， 默认采用轮询的方式转发流量.\niptables, 默认kube-proxy采用的方式。这个模式下kube-proxy也会监控kubernetes master的对Service和endpoints的增删，每一个service都有一套iptable规则，根据service的ip:port重定向流量到某个Service后端endpoints, endpoints也会安装iptables来选择pod。特点是iptable不需要在用户态和内核态转换， 缺点是如果原来选择的pod无效，那么不会重试另一个pod,需要readiness探针来实现重试。具体实现原理可以看地址\nipvs, 这模式是v1.9beta版。 此模式调用netlink接口创建ipvs规则，然后从service和endpoint同步ipvs规则， 当一个service被访问时，流量会被重定向到后端pods。 ipvs使用的是hash表，有着更好的性能和多种负载均衡方案：\nround robin\nleast connection\ndestination hashing\nsource hashing\nshortest expected delay\nnever queue\n\n\n\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"K8S的StatefulSets","url":"/2018/03/06/kubernetes-2018-03-07-StatefulSets/","content":"前言K8S的StatefulSets是一个负载API对象用于管理有状态的应用。StatefulSets会给每个pod维护一个独特的标记， 所以这些pods是不能交换的。注意，V1.9才是稳定版本， 可以通过apiserver发送--runtime-config禁用这个控制器。\n应用场景StatefulSets适合如下应用场景：\n\n稳定、唯一的网络标识\n稳定、持久的存储\n有序的部署和扩展\n有序的删除和终止\n有序的自动化滚动更新如果不需要持久化，应该考虑Deployment或ReplicaSet控制器.\n\n局限\n版本1.9\n如果是给pod提供存储，那么必须预先配置好PV, 或者通过PersistentVolume Provisioner提供\n删除或者向下缩减StatefulSet并不会删除相应的volume, 出于安全考虑。\n需要创建一个Headless Service（clusterIP: None）负责pods的网络标识\n\nStatefulSet的组件下面这个例子创建一个无头的service, 一个有状态的pod，挂载在声明模板的PV.\napiVersion: v1kind: Servicemetadata:  name: nginx  labels:    app: nginxspec:  ports:  - port: 80    name: web  clusterIP: None  selector:    app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata:  name: webspec:  selector:    matchLabels:      app: nginx # has to match .spec.template.metadata.labels  serviceName: &quot;nginx&quot;  replicas: 3 # by default is 1  template:    metadata:      labels:        app: nginx # has to match .spec.selector.matchLabels    spec:      terminationGracePeriodSeconds: 10      containers:      - name: nginx        image: k8s.gcr.io/nginx-slim:0.8        ports:        - containerPort: 80          name: web        volumeMounts:        - name: www          mountPath: /usr/share/nginx/html  volumeClaimTemplates:  - metadata:      name: www    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      storageClassName: &quot;my-storage-class&quot;      resources:        requests:          storage: 1Gi\n\nPod IdentityStatefulSets有一个唯一的identity，由有序的（由0开始分配给pod），稳定的网络标识符和稳定的存储组成。pod更换node也不会改变这个id。\n\n序号从0开始\n稳定的网络ID, StatefulSets中pod的主机名由$(statefulset name)-$(ordinal)规则组成，所以上述yml文件中，pods的名称分别web-0,web-1,web-2； 而StatefulSets的域名则为$(service name).$(namespace).svc.cluster.local；pod的域名则为$(podname).$(governing service domain)，$(governing service domain)是serviceName. 见下图：\n\n更新StatefulSet1.7版本以后，StatefulSet控制器支持自动更新。更新策略取决于StatefulSet对象的spec.updateStrategy字段，这个功能可以用于更新容器镜像，资源用量，标签，pods的注释(如果改了其它设置，就需要删除statefulsets重新create), 1.9支持RollingUpdate和OnDelete两种更新策略。\n\nRollingUpdate将会把statefulset的pods按倒叙更新。首先让updateStrategy使用rollingupdate\n\nkubectl patch statefulset web -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;type&quot;:&quot;RollingUpdate&quot;&#125;&#125;&#125;&#x27;statefulset &quot;web&quot; patched\n  然后更换pod的image， 这个过程中，pod将倒序终止，再更新。如果某个pod更新失败，将会对整个statefulsets做回滚。  kubectl patch statefulset web --type=&#x27;json&#x27; -p=&#x27;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;,    &quot;value&quot;:&quot;gcr.io/google_containers/nginx-slim:0.8&quot;&#125;]&#x27;statefulset &quot;web&quot; patched  可以使用kubectl rollout status sts/&lt;name&gt;查看更新状态。这个策略有一个属性partition可以用于分段更新，就是制定更新一部分pods,  使用这个特性，就可以进行金丝雀测试（灰度）。  kubectl patch statefulset web -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:3&#125;&#125;&#125;&#125;&#x27;statefulset &quot;web&quot; patched\n删除StatefulSet简单删除命令：\nkubectl delete -f file.ymlkubectl delete statefulsets &lt;statefuleset_name&gt;\n如果想要保留pods，可以使用参数--cascade=false\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"Kubernetes的volume","url":"/2018/03/07/kubernetes-2018-03-08-k8s-volume/","content":"Volume简单的说， 一个volume就是一个在磁盘或容器中的目录，volume里面有pod中容器可以访问的数据。K8S的volume主要解决容器中数据存储和容器间数据共享的问题。k8s的volume和pod有相同的生命周期。一个pod可以同时使用不同的volume类型。如果使用volume, 一个pod需要通过spec.volumes指定volume类型，spec.containers.volumeMounts指定挂载目录。volume不能再挂载到其他volume,\nk8s支持的Volumes类型\nwsElasticBlockStore\nazureDisk\nazureFile\ncephfs\nconfigMapconfigMap提供了一种往pod里注入配置的方式。如果配置文件能够在Dockerfile中add进去，我建议还是用add，而不是\ncsi\ndownwardAPI\nemptyDiremptyDir这个卷是pod在node创建时生成的空卷，生命周期和pod一致， 存储在pod所在的node，删除pod将删除emptyDir, 存储媒介和node一致，但是可以通过emptyDir.medium: Memory把它mount到内存（docker的tmpfs）。它使用场景可以作为原型或checkpointing点.apiVersion: v1kind: Podmetadata:  name: test-pdspec:  containers:  - image: k8s.gcr.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /cache      name: cache-volume  volumes:  - name: cache-volume    emptyDir: &#123;&#125;\nfc (fibre channel)\nflocker\ngcePersistentDisk\ngitRepo\nglusterfs\nhostPathhostPath把node的目录挂进pod，可以用于：容器访问Docker内部/var/lib/docker; 跑cAdvisor要用到/sys; 或者允许pod运行前检查hostPath是否存在，然后再对pod做操作。path是hostPath必须的参数，还可以通过type来指定volume类型，type的类型如下：由于是主机的路径, 所以需要注意权限问题。apiVersion: v1kind: Podmetadata:  name: test-pdspec:  containers:  - image: k8s.gcr.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /test-pd      name: test-volume  volumes:  - name: test-volume    hostPath:      # directory location on host      path: /data      # this field is optional      type: Directory\niscsi\nlocal. 1.7以后新特性，需要开启PersistentLocalVolumes, 如果是1.9还需要开启VolumeScheduling。local volume代表了本地挂载的存储设备，可以被用于创建静态PersistentVolume。相比于hostPath, local卷可以被系统通过节点自动地发现PV, 而不用手动把某个pod分配到node. 下面的例子就是使用local来创建PV: apiVersion: v1 kind: PersistentVolume metadata:   name: example-pv   annotations:     &quot;volume.alpha.kubernetes.io/node-affinity&quot;: &#x27;&#123;        &quot;requiredDuringSchedulingIgnoredDuringExecution&quot;: &#123;            &quot;nodeSelectorTerms&quot;: [                &#123; &quot;matchExpressions&quot;: [                    &#123; &quot;key&quot;: &quot;kubernetes.io/hostname&quot;,                      &quot;operator&quot;: &quot;In&quot;,                      &quot;values&quot;: [&quot;example-node&quot;]                    &#125;                ]&#125;             ]&#125;          &#125;&#x27;spec:  capacity:    storage: 100Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Delete  storageClassName: local-storage  local:    path: /mnt/disks/ssd1\nnfs. 如果pod的删除， nfs不会删除数据。NFS可以同时被多节点读写。\npersistentVolumeClaimpersistentVolumeClaim把persistentVolume挂进pod，persistentVolume是一种k8s用户声明持久化存储的方式，好处是用户不需要底层存储环境细节。\nprojected， 可以映射多个volume到同一个目录。 可以用于secret, downwardAPI, configMap.\n\napiVersion: v1kind: Podmetadata:  name: volume-testspec:  containers:  - name: container-test    image: busybox    volumeMounts:    - name: all-in-one      mountPath: &quot;/projected-volume&quot;      readOnly: true  volumes:  - name: all-in-one    projected:      sources:      - secret:          name: mysecret          items:            - key: username              path: my-group/my-username      - downwardAPI:          items:            - path: &quot;labels&quot;              fieldRef:                fieldPath: metadata.labels            - path: &quot;cpu_limit&quot;              resourceFieldRef:                containerName: container-test                resource: limits.cpu      - configMap:          name: myconfigmap          items:            - key: config              path: my-group/my-config\n\nportworxVolume\nquobyte\nrbd\nscaleIO\nsecret\nstorageos\nvsphereVolume\n\nResourcesemptyDir的存储介质取决于kubelet根目录的存储介质(&#x2F;var&#x2F;lib&#x2F;kubelet)， emptyDir和hostPath目前没有限制用量，也没有在容器间隔离数据。\nMount propagation 挂载传递1.8新特性。挂载传递能让同一pod里的容器共享volume, 甚至可以在同一node的pods间共享。\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"使用aliyun容器服务搭建mysql","url":"/2018/03/09/kubernetes-2018-03-10-k8s-mysql/","content":"前言本文简单介绍用aliyun的Kubernetes容器服务搭建mysql集群。首先确保k8s集群已经创建，并且已经开通了相关服务如NAS、容器镜像仓库等服务。\n第一步 创建数据卷\nk8s有个nodeSelector的功能，如果mysql由于单点故障漂移了node，原来的存储会自动重新挂载到新的node(这一步骤是在转移node之前做的). 这样避免了漂移node导致有状态服务不可用的情况。\naliyun支持k8s三种存储：网盘, NAS, OSS. 基于性能考虑，我们选择NAS。首先需要在NAS控制台右上角选择创建文件系统, 注意所创建的文件系统所在区域必须和k8s集群所在的区域在同一地域:然后在新生成的文件系统选择添加挂载点，如图已经有两个挂载点，还不是很清楚为什么不能再添加新的挂载点了。添加的挂载点必须和k8s集群在同一个VPC! 记住挂载地址后续创建k8s数据卷时用到。\n然后在k8s控制台右上角选中创建, 填上新创建的挂载点域名，点击确定数据卷就创建了：\n\n第二步 使用aliyun的镜像创建mysql\n在容器服务控制台点击使用镜像，选择mysql部署应用, 第一步填应用名称和选择命名空间：\n然后设置应用配置，注意在这里的环境变量必须设置MYSQL_ROOT_PASSWORD或MYSQL_ALLOW_EMPTY_PASSWORD或MYSQL_RANDOM_ROOT_PASSWORD之一，否则会创建失败。这个页面暂时没有找到设置k8s secret的地方，所以可能如编排模块那样使用secret。其它的资源和网络按需设置, 注意填上上一步创建的pvmysql-pv, 点击创建，然后在控制台应该能看到绿色的部署成功，若有错误，请点进容器组查看相关log。(插一句，如果使用k8s secret创建私密信息，也不能把Secret模板commit到git, 因为base64不是真的加密啊！)\n\n第三部，创建service\n现在创建的mysql还只能通过172docker的网段访问， 想要所有pods通过域名访问，则需要给mysql加一个k8s service.可以通过k8s控制台的服务创建，编排示例如下：\n\napiVersion: v1kind: Servicemetadata:  name: mysql  labels:    name: mysqlspec:  ports:    - port: 3306  selector:    app: app-mysql  # 注意，这里是pod的label\n\n\n这样，基本上容器集群mysql搭建完成了。登入master, 使用k8s端口映射可以验证一下:\n\n[root@iZuf626rqqtw2yxzeeeqtoZ ~]# kubectl port-forward `kubectl get pods|grep -v NAME|cut -d &quot; &quot; -f 1` 3306:3306Forwarding from 127.0.0.1:3306 -&gt; 3306[root@iZuf626rqqtw2yxzeeeqtoZ ~]# mysql -h 127.0.0.1 -u root -P 3306 -p Enter password: Welcome to the MariaDB monitor.  Commands end with ; or \\g.Your MySQL connection id is 4Server version: 5.7.18 MySQL Community Server (GPL)Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.MySQL [(none)]&gt; \n\n小结可能对阿里云的容器服务还不是很熟悉，感觉控制台有些细节还能够改进。本文还未涉及mysql的HA方案（可选Galera+StatefulSet) 。当然上生产，我们还需要进一步检验有状态服务到底适不适合容器化。\n","categories":["kubernetes"],"tags":["kubernetes, mysql"]},{"title":"在云上使用k8s动态PV","url":"/2018/03/11/kubernetes-2018-03-12-k8s-dynamic-volume-provision/","content":"\nhttps://v1-8.docs.kubernetes.io/docs/concepts/storage/dynamic-provisioning/\n\n前言动态卷配置允许按需创建存储卷。如果没有动态配置，管理员必须手动配置创建新的存储， 然后创建PV对象。动态卷配置的功能能让管理员不必预先配置存储。而是当用户需要的时候自动配置存储。它可以按用户需要的用量配置存储空间，不像静态那样可能出现超额分配用量。\n背景动态配置数据卷的实现是基于StorageClassAPI对象。管理员可以配置很多StorageClass对象，每一个都可以指定一个存储供应商。管理员可以给一个集群定义和暴露多个存储，每一个都有不同的参数配置。这样能减轻配置存储的复杂度，使用户简单选择存储服务。\n开启动态配置数据卷\n首先需要创建StorageClass对象, 可以指定使用哪个供应商和那些参数，如下例：\n\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: slowprovisioner: kubernetes.io/gce-pdparameters:  type: pd-standard---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: fastprovisioner: kubernetes.io/gce-pdparameters:  type: pd-ssd\n\n如果使用aliyun的NAS作为provisioner, 需要先安装aliyun nas的插件\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: alicloud-nasprovisioner: alicloud/nas---apiVersion: v1kind: ServiceAccountmetadata:  name: alicloud-nas-controller  namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata:  name: run-alicloud-nas-controllersubjects:  - kind: ServiceAccount    name: alicloud-nas-controller    namespace: kube-systemroleRef:  kind: ClusterRole  name: alicloud-disk-controller-runner  apiGroup: rbac.authorization.k8s.io---kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: alicloud-nas-controller  namespace: kube-systemspec:  replicas: 1  strategy:    type: Recreate  template:    metadata:      labels:        app: alicloud-nas-controller    spec:      tolerations:      - effect: NoSchedule        operator: Exists        key: node-role.kubernetes.io/master      - effect: NoSchedule        operator: Exists        key: node.cloudprovider.kubernetes.io/uninitialized      nodeSelector:         node-role.kubernetes.io/master: &quot;&quot;      serviceAccount: alicloud-nas-controller      containers:        - name: alicloud-nas-controller          image: registry.cn-hangzhou.aliyuncs.com/acs/alicloud-nas-controller:v1.8.4          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              value: alicloud/nas            - name: NFS_SERVER              value: 0cd8b4a576-mmi32.cn-hangzhou.nas.aliyuncs.com  # 指定创建的NAS挂载点            - name: NFS_PATH              value: /      volumes:        - name: nfs-client-root          nfs:            server: 0cd8b4a576-mmi32.cn-hangzhou.nas.aliyuncs.com   # 指定创建的NAS挂载点            path: /\n\n使用动态配置数据卷用户需要在PVC指定StorageClassName, 如下\napiVersion: v1kind: PersistentVolumeClaimmetadata:  name: claim1spec:  accessModes:    - ReadWriteOnce  storageClassName: fast  resources:    requests:      storage: 30Gi\n\n设置默认动态配置如果有个用户在PersistentVolumeClaim没有指定storageClassName， 那么可以配置默认的storageClassName使用。具体是在创建的storageClassName中annotationstorageclass.kubernetes.io/is-default-class\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"K8S使用私有镜像","url":"/2018/03/11/kubernetes-2018-03-12-k8s-pull-private-image/","content":"\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account\n\n前言如果想要k8s能够拉取使用私有镜像需要创建secret, 然后有两种方法指定secret, 拉取image。 一是在pod的编排模板指定imagePullSecret, 二是修改service account的配置，本文讲述第二种方法。\n第一步 创建imagePullSecret$ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAILsecret &quot;myregistrykey&quot; created.$ kubectl get secrets myregistrykeyNAME             TYPE                              DATA    AGEmyregistrykey    kubernetes.io/.dockerconfigjson   1       1d\n如果是使用aliyun的私有镜像，docker-server可是设置成vpc的server地址，这样省一点流量。\n第二步 修改serviceaccountkubectl patch serviceaccount default -p &#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;myregistrykey&quot;&#125;]&#125;&#x27;\n完成！\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"K8S管理容器的计算资源","url":"/2018/03/12/kubernetes-2018-03-13-k8s-manage-resource/","content":"\nhttps://v1-8.docs.kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n\n前言k8s可以给pod设置计算资源用量， 当遇到违反某些不满足性能的条件时，k8s会有一套机制来处理。限制资源在编排模拟中使用limits或requests.requests指定资源的最小要求，limits指定最大用量。详细关系是 0 &lt;= request &lt;=Node Allocatable和request &lt;= limit &lt;= Infinity。具体查看resource-qos\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"Kubernetes Deployment","url":"/2018/04/06/kubernetes-2018-04-08-k8s-deployments/","content":"k8s的部署Deploymentk8s的Deployment提供了pods和ReplicaSets的更新.只需在Deployment对象声明你想要的部署状态, Deployment控制器就会更新到需要的状态.所以这种方式是声明式的.除非第一次部署,k8s采用的部署方式是rolling update(滚动更新). 大致意思是保证服务可用的情况下, 创建一定量的新pods,然后删除一定量的pods,循环这些步骤直到部署完成.\n一个例子apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1kind: Deployment   # apiVersion, kind, metadata, spec这四个字段必须要metadata:  name: appwebspec:  selector:    matchLabels:  # 多个label做and处理. 部署时会自动加一个hash label, 用于区分部署版本      app: appweb      tier: backend      version: 1.0  replicas: 2  template:    metadata:      labels:        app: appweb        tier: backend        version: 1.0    spec:      containers:      - name: app        image:  registry-vpc.cn-shanghai.aliyuncs.com/web1.0        env:          - name: app_WEB_DATABASE_URL            valueFrom:              secretKeyRef:                name: appsecrets                key: app_WEB_DATABASE_URL          - name: WORKER_PROCESSES            value: &quot;2&quot;          - name: JOB_WORKER_URL            value: &quot;redis://redis-master:6379/0&quot;          - name: RAILS_ENV            value: &quot;production&quot;          - name: REDIS_CACHE_URL            value: &quot;redis://redis-master:6379/1&quot;        ports:          - containerPort: 7007        volumeMounts:          - name: app-pvc            mountPath: &quot;/app/public/&quot;        livenessProbe:          httpGet:            path: /login            port: 7007          initialDelaySeconds: 30          periodSeconds: 10          timeoutSeconds: 3        readinessProbe:          httpGet:            path: /login            port: 7007          initialDelaySeconds: 30          periodSeconds: 10          timeoutSeconds: 3      volumes:        - name: app-pvc          persistentVolumeClaim:            claimName: app-pvc\n部署时需要注意的地方:\nDeployment的模板使用label来找到对应的容器, 所以不建议修改label.如果必须修改请看label selector update\n在一个deploy过程中如果再进行另一个deploy,那么之前那个deploy的pod会立刻删除.\n每次部署都会创建一个部署版本, 如果部署完后又进行扩展(scale), 那么此时进行rollback将不会回滚扩展的部分.因为rollback只会根据(.spec.template)指定来回滚. 可用在每次更新部署的时候,指定--record把部署命令记录到部署更新版本记录里.\n查看部署历史kubectl rollout history deployment/appweb, 可以指定具体版本号kubectl rollout history deployment/appweb --revision=2\n回滚到上一个版本命令kubectl rollout undo deployment/appweb\n\n使用金丝雀(灰度)部署k8s的金丝雀部署其实就是在现有的版本上再部署只有一个pod的新deployment,用另一个label来区别两个deployment.并把这个新的pod加入原来的service.代码概要如下:\napiVersion: apps/v1  # old deploymentkind: Deploymentmetadata:  name: app-productionspec:  selector:    matchLabels:      env: production      app: web      .....---apiVersion: v1kind: Servicemetadata:  name: app-Servicespec:  selector:    app: web.....\napiVersion: apps/v1 # new deploymentkind: Deploymentmetadata:  name: app-canaryspec:  selector:    matchLabels:      app: web      env: canary      .....\n如果发布测试没有问题, 那么可以直接运行kubectl set image deployment/app-production app=registry-vpc.cn-shanghai.aliyuncs.com/web2.0滚动更新, 然后删除金丝雀部署.\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"本地rails系统迁移到阿里云容器服务小结","url":"/2018/04/10/kubernetes-2018-04-11-migrate-k8s-summary/","content":"\n实验性迁移小结\n\n迁移目标本文主要记录了将本地rails迁移到云上容器服务的过程, 包括如何部分重构本地应用代码和一些思考. 迁移本地应用的目的主要是为了能够将应用变得高可用和更好的扩展性, 以及为了更好的容器管理.\n原来的系统状况本地的rails系统是一个典型的数据库-网页应用, 原来采用docker compose构建部署, 用到的技术栈主要有, 结构图如下:\n\nrails5.0\nmysql5.7\nsidekiq\nredis\nnginx\n\n\n迁移后的系统迁移k8s后系统, 系统变得更加模块化(当然没有微服务化). 结构如下图:\n一些细节和原来本地的问题新的部署主要把原来多个进程从一个容器分离了出来, 用docker提倡的一个进程一个container; 把rake db:migrate等操作做成k8s的job; 把定时任务作为单独的cronJob, 避免重复运行(幂等性);并且把底层的存储用了云上的OSS和NAS. 由于是实验性的部署, 并没有使用managed service, 如rds和redis, 这部分还是自己搭建的, 以后会切换为云上的服务. 大致细节图:\n\n如上图, sidekiq和rails在同一个pod(还在考虑要不要分出来), 连接的redis使用的是helm部署(没有使用pv), mysql采用的官网StatefulSet例子 , 并且在集群前面搭建一个mycat,做读写分离, mysql底层的存储使用k8s的动态pv. mysql集群的问题是mycat不是HA, 动态scale slave需要配置mycat, 所以需要继续改进, 还可以是考虑vitess或galera做更好的mysql HA方案.\n本地问题 assets需要用nginx来处理. 务必做到开发,测试, 生产各个环境一致.必须是用k8s Secret来处理敏感信息,但是不要commit进代码库,可以直接使用的k8s控制台创建. dockerfile里面有些包只要构建的时候需要使用(minor issue).\n其它思考.\ngoogle墙的问题\n需要各种正向反向代理, 以便内网用户能够访问应用; 需要反向代理以便使用集团的各种服务,比如邮箱服务.\n由于本地gitlab网络不能出去, 所以需要搭一个自动构建&#x2F;推送镜像上云的流程.(走外网费流量)\n后续集成managed service(non-native k8s service)需要集成进k8s, 这里还没有经验.\n如何给每个开发设置kubectl权限. 现在是采用RBAC+Namespace来控制, 有一些笨拙,网上有推荐使用OpenID.\n部署自动化, 由于有了service, 进行金丝雀,蓝绿部署变得比较方便.不过还是需要手动干预,后续要考虑自动化.\nCI&#x2F;CD. 也是上一条思考相关. k8s集成jenkins看起来比较方便(推荐jenkinsfile), 不过考虑是不是看看其他集成方案.\n日志管理还没有深入研究.\n阿里云的Docker registry和helm仓库有点弱, 我想在控制台删个镜像都要用api.\n有了k8s, pods多了之后, 服务之间的依赖关系难以清晰的看出来. 如果将应用微服务化, 服务的治理会是个问题.\n开发需要使用新的工具, 如telepresence, helm,  Kubeval等. 用helm打包成chart可以方便的部署一整个应用.\n\n\n\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"AMQP-0-9-1模型总结","url":"/2019/02/28/middleware-2019-02-28-amqp-model/","content":"AMQP-0-9-1模型AMQP(Advanced Message Queue Protocol模型很简单, 就是publisher将消息发给exchanger中间人, 然后exchanger中间人按规则将消息的副本塞入队列中(这个过程叫binding), 接着中间人消息推送给订阅队列的消费者consumer或者消费者主动去拉取消息.\n当发布消息的时候, 发布者可以给消息设置一些消息的元信息, 然后中间人就会使用这些信息作为消息路由的规则.由于网络是不稳定的, AMQP有消息确认的概念: 消费者拿到消息后需要通知中间人确认拿到消息, 然后中间人会把队列中的消息删除, 否则中间人会重发消息给另一个消费者(如果存在). 如果消息没有被确认, 同时又不能重发消息(不存在另一个消费者), 那么消息是可以被返回给发送者或者丢弃的. 可以设置dead letter queue来处理消息消费失败的情况. \nAMQP是一个可编程协议queue/exchange/binding在AMQP中都是实体(entity), 所以AMQP的实体&#x2F;路由规则等是需要应用自己定义实现的. (就是再rabbitmq代码里定义)\nExchange和exchange类型Exchange接受生产者发送的消息, 然后根据路由规则将消息送给零个或多个队列. 路由规则取决于exchange type和binding. AMQP有4种交换类型:\n|-----------------+----------------|| name | default pre-declared names||-----------------+----------------|| direct(default) | (empty string) and amq.direct||-----------------+----------------|| fanout          |  amq.fanout    ||-----------------+----------------|| topic           | amq.topic      ||-----------------+----------------|| headers         | amq.match(amq.headers in rabbitmq)||-----------------+----------------|\nheaders exchange用的比较少, 说一下.  这种类型的中间人不使用routing key作为路由规则, 而是使用生产者在消息中的头部x-match定义的k-v值. \n队列queue是存放消息的buffer, 与exchange分享一些共同的属性, 同时有一些自己属性可以定义:\n\nName\nDurable (survive during restart)\nExclusive (only use by one consumer)\nAuto-delete (delete when last consumer unsubscribes)\nArguments (optional, )\n\n","categories":["middleware"],"tags":["rabbitmq,amqp"]},{"title":"Openshift Template","url":"/2018/01/03/openshift-2018-01-04-Openshift-template/","content":"Openshift的模板定义：模板是一组可以被参数化的对象，这组对象被处理后可以被用于创建服务，构建配置，部署配置。\n模板代码例子apiVersion: v1kind: Templatemetadata:  name: redis-template #1  annotations:    description: &quot;Description&quot;  #2    iconClass: &quot;icon-redis&quot;  #3    tags: &quot;database,nosql&quot;  #4objects:   #5- apiVersion: v1  kind: Pod  metadata:    name: redis-master  spec:    containers:    - env:      - name: REDIS_PASSWORD        value: $&#123;REDIS_PASSWORD&#125;   #6      image: dockerfile/redis      name: master      ports:      - containerPort: 6379        protocol: TCPparameters:  #7- description: Password used for Redis authentication  from: &#x27;[A-Z0-9]&#123;8&#125;&#x27;     generate: expression  name: REDIS_PASSWORDlabels:        redis: master\n\n模板名\n可选描述\n展示的icon（搜索”openshift-logos-icon”）\n这个模板的tag\n模板将会创建的对象\n模板处理的时候输入参数\n模板参数\n任意产生表达式\n所以对象将被打上标签\n\n复杂一点的json模板&#123;  &quot;kind&quot;: &quot;Template&quot;,  &quot;apiVersion&quot;: &quot;v1&quot;,  &quot;metadata&quot;: &#123;    &quot;name&quot;: &quot;postgresql-persistent&quot;,    &quot;creationTimestamp&quot;: null,    &quot;annotations&quot;: &#123;      &quot;openshift.io/display-name&quot;: &quot;PostgreSQL (Persistent)&quot;,      &quot;description&quot;: &quot;PostgreSQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/blob/master/9.5.\\n\\nNOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template.&quot;,      &quot;iconClass&quot;: &quot;icon-postgresql&quot;,      &quot;tags&quot;: &quot;database,postgresql&quot;,      &quot;template.openshift.io/long-description&quot;: &quot;This template provides a standalone PostgreSQL server with a database created.  The database is stored on persistent storage.  The database name, username, and password are chosen via parameters when provisioning this service.&quot;,      &quot;template.openshift.io/provider-display-name&quot;: &quot;Red Hat, Inc.&quot;,      &quot;template.openshift.io/documentation-url&quot;: &quot;https://docs.openshift.org/latest/using_images/db_images/postgresql.html&quot;,      &quot;template.openshift.io/support-url&quot;: &quot;https://access.redhat.com&quot;    &#125;  &#125;,  &quot;message&quot;: &quot;The following service(s) have been created in your project: $&#123;DATABASE_SERVICE_NAME&#125;.\\n\\n       Username: $&#123;POSTGRESQL_USER&#125;\\n       Password: $&#123;POSTGRESQL_PASSWORD&#125;\\n  Database Name: $&#123;POSTGRESQL_DATABASE&#125;\\n Connection URL: postgresql://$&#123;DATABASE_SERVICE_NAME&#125;:5432/\\n\\nFor more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/blob/master/9.5.&quot;,  &quot;labels&quot;: &#123;    &quot;template&quot;: &quot;postgresql-persistent-template&quot;  &#125;,  &quot;objects&quot;: [    &#123;      &quot;kind&quot;: &quot;Secret&quot;,      &quot;apiVersion&quot;: &quot;v1&quot;,      &quot;metadata&quot;: &#123;        &quot;name&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;,        &quot;annotations&quot;: &#123;          &quot;template.openshift.io/expose-username&quot;: &quot;&#123;.data[&#x27;database-user&#x27;]&#125;&quot;,          &quot;template.openshift.io/expose-password&quot;: &quot;&#123;.data[&#x27;database-password&#x27;]&#125;&quot;        &#125;      &#125;,      &quot;stringData&quot; : &#123;        &quot;database-user&quot; : &quot;$&#123;POSTGRESQL_USER&#125;&quot;,        &quot;database-password&quot; : &quot;$&#123;POSTGRESQL_PASSWORD&#125;&quot;      &#125;    &#125;,    &#123;      &quot;kind&quot;: &quot;Service&quot;,      &quot;apiVersion&quot;: &quot;v1&quot;,      &quot;metadata&quot;: &#123;        &quot;name&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;,        &quot;creationTimestamp&quot;: null,        &quot;annotations&quot;: &#123;          &quot;template.openshift.io/expose-uri&quot;: &quot;postgres://&#123;.spec.clusterIP&#125;:&#123;.spec.ports[?(.name==\\&quot;postgresql\\&quot;)].port&#125;&quot;        &#125;      &#125;,      &quot;spec&quot;: &#123;        &quot;ports&quot;: [          &#123;            &quot;name&quot;: &quot;postgresql&quot;,            &quot;protocol&quot;: &quot;TCP&quot;,            &quot;port&quot;: 5432,            &quot;targetPort&quot;: 5432,            &quot;nodePort&quot;: 0          &#125;        ],        &quot;selector&quot;: &#123;          &quot;name&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;        &#125;,        &quot;type&quot;: &quot;ClusterIP&quot;,        &quot;sessionAffinity&quot;: &quot;None&quot;      &#125;,      &quot;status&quot;: &#123;        &quot;loadBalancer&quot;: &#123;&#125;      &#125;    &#125;,    &#123;      &quot;kind&quot;: &quot;PersistentVolumeClaim&quot;,      &quot;apiVersion&quot;: &quot;v1&quot;,      &quot;metadata&quot;: &#123;        &quot;name&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;      &#125;,      &quot;spec&quot;: &#123;        &quot;accessModes&quot;: [          &quot;ReadWriteOnce&quot;        ],        &quot;resources&quot;: &#123;          &quot;requests&quot;: &#123;            &quot;storage&quot;: &quot;$&#123;VOLUME_CAPACITY&#125;&quot;          &#125;        &#125;      &#125;    &#125;,    &#123;      &quot;kind&quot;: &quot;DeploymentConfig&quot;,      &quot;apiVersion&quot;: &quot;v1&quot;,      &quot;metadata&quot;: &#123;        &quot;name&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;,        &quot;creationTimestamp&quot;: null      &#125;,      &quot;spec&quot;: &#123;        &quot;strategy&quot;: &#123;          &quot;type&quot;: &quot;Recreate&quot;        &#125;,        &quot;triggers&quot;: [          &#123;            &quot;type&quot;: &quot;ImageChange&quot;,            &quot;imageChangeParams&quot;: &#123;              &quot;automatic&quot;: true,              &quot;containerNames&quot;: [                &quot;postgresql&quot;              ],              &quot;from&quot;: &#123;                &quot;kind&quot;: &quot;ImageStreamTag&quot;,                &quot;name&quot;: &quot;postgresql:$&#123;POSTGRESQL_VERSION&#125;&quot;,                &quot;namespace&quot;: &quot;$&#123;NAMESPACE&#125;&quot;              &#125;,              &quot;lastTriggeredImage&quot;: &quot;&quot;            &#125;          &#125;,          &#123;            &quot;type&quot;: &quot;ConfigChange&quot;          &#125;        ],        &quot;replicas&quot;: 1,        &quot;selector&quot;: &#123;          &quot;name&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;        &#125;,        &quot;template&quot;: &#123;          &quot;metadata&quot;: &#123;            &quot;creationTimestamp&quot;: null,            &quot;labels&quot;: &#123;              &quot;name&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;            &#125;          &#125;,          &quot;spec&quot;: &#123;            &quot;containers&quot;: [              &#123;                &quot;name&quot;: &quot;postgresql&quot;,                &quot;image&quot;: &quot; &quot;,                &quot;ports&quot;: [                  &#123;                    &quot;containerPort&quot;: 5432,                    &quot;protocol&quot;: &quot;TCP&quot;                  &#125;                ],                &quot;readinessProbe&quot;: &#123;                  &quot;timeoutSeconds&quot;: 1,                  &quot;initialDelaySeconds&quot;: 5,                  &quot;exec&quot;: &#123;                    &quot;command&quot;: [ &quot;/bin/sh&quot;, &quot;-i&quot;, &quot;-c&quot;, &quot;psql -h 127.0.0.1 -U $POSTGRESQL_USER -q -d $POSTGRESQL_DATABASE -c &#x27;SELECT 1&#x27;&quot;]                  &#125;                &#125;,                &quot;livenessProbe&quot;: &#123;                  &quot;timeoutSeconds&quot;: 1,                  &quot;initialDelaySeconds&quot;: 30,                  &quot;tcpSocket&quot;: &#123;                    &quot;port&quot;: 5432                  &#125;                &#125;,                &quot;env&quot;: [                  &#123;                    &quot;name&quot;: &quot;POSTGRESQL_USER&quot;,                    &quot;valueFrom&quot;: &#123;                      &quot;secretKeyRef&quot; : &#123;                        &quot;name&quot; : &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;,                        &quot;key&quot; : &quot;database-user&quot;                      &#125;                    &#125;                  &#125;,                  &#123;                    &quot;name&quot;: &quot;POSTGRESQL_PASSWORD&quot;,                    &quot;valueFrom&quot;: &#123;                      &quot;secretKeyRef&quot; : &#123;                        &quot;name&quot; : &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;,                        &quot;key&quot; : &quot;database-password&quot;                      &#125;                    &#125;                  &#125;,                  &#123;                    &quot;name&quot;: &quot;POSTGRESQL_DATABASE&quot;,                    &quot;value&quot;: &quot;$&#123;POSTGRESQL_DATABASE&#125;&quot;                  &#125;                ],                &quot;resources&quot;: &#123;                  &quot;limits&quot;: &#123;                    &quot;memory&quot;: &quot;$&#123;MEMORY_LIMIT&#125;&quot;                  &#125;                &#125;,                &quot;volumeMounts&quot;: [                  &#123;                    &quot;name&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;-data&quot;,                    &quot;mountPath&quot;: &quot;/var/lib/pgsql/data&quot;                  &#125;                ],                &quot;terminationMessagePath&quot;: &quot;/dev/termination-log&quot;,                &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot;,                &quot;capabilities&quot;: &#123;&#125;,                &quot;securityContext&quot;: &#123;                  &quot;capabilities&quot;: &#123;&#125;,                  &quot;privileged&quot;: false                &#125;              &#125;            ],            &quot;volumes&quot;: [              &#123;                &quot;name&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;-data&quot;,                &quot;persistentVolumeClaim&quot;: &#123;                  &quot;claimName&quot;: &quot;$&#123;DATABASE_SERVICE_NAME&#125;&quot;                &#125;              &#125;            ],            &quot;restartPolicy&quot;: &quot;Always&quot;,            &quot;dnsPolicy&quot;: &quot;ClusterFirst&quot;          &#125;        &#125;      &#125;,      &quot;status&quot;: &#123;&#125;    &#125;  ],  &quot;parameters&quot;: [    &#123;      &quot;name&quot;: &quot;MEMORY_LIMIT&quot;,      &quot;displayName&quot;: &quot;Memory Limit&quot;,      &quot;description&quot;: &quot;Maximum amount of memory the container can use.&quot;,      &quot;value&quot;: &quot;512Mi&quot;,      &quot;required&quot;: true    &#125;,    &#123;      &quot;name&quot;: &quot;NAMESPACE&quot;,      &quot;displayName&quot;: &quot;Namespace&quot;,      &quot;description&quot;: &quot;The OpenShift Namespace where the ImageStream resides.&quot;,      &quot;value&quot;: &quot;openshift&quot;    &#125;,    &#123;      &quot;name&quot;: &quot;DATABASE_SERVICE_NAME&quot;,      &quot;displayName&quot;: &quot;Database Service Name&quot;,      &quot;description&quot;: &quot;The name of the OpenShift Service exposed for the database.&quot;,      &quot;value&quot;: &quot;postgresql&quot;,      &quot;required&quot;: true    &#125;,    &#123;      &quot;name&quot;: &quot;POSTGRESQL_USER&quot;,      &quot;displayName&quot;: &quot;PostgreSQL Connection Username&quot;,      &quot;description&quot;: &quot;Username for PostgreSQL user that will be used for accessing the database.&quot;,      &quot;generate&quot;: &quot;expression&quot;,      &quot;from&quot;: &quot;user[A-Z0-9]&#123;3&#125;&quot;,      &quot;required&quot;: true    &#125;,    &#123;      &quot;name&quot;: &quot;POSTGRESQL_PASSWORD&quot;,      &quot;displayName&quot;: &quot;PostgreSQL Connection Password&quot;,      &quot;description&quot;: &quot;Password for the PostgreSQL connection user.&quot;,      &quot;generate&quot;: &quot;expression&quot;,      &quot;from&quot;: &quot;[a-zA-Z0-9]&#123;16&#125;&quot;,      &quot;required&quot;: true    &#125;,    &#123;      &quot;name&quot;: &quot;POSTGRESQL_DATABASE&quot;,      &quot;displayName&quot;: &quot;PostgreSQL Database Name&quot;,      &quot;description&quot;: &quot;Name of the PostgreSQL database accessed.&quot;,      &quot;value&quot;: &quot;sampledb&quot;,      &quot;required&quot;: true    &#125;,    &#123;      &quot;name&quot;: &quot;VOLUME_CAPACITY&quot;,      &quot;displayName&quot;: &quot;Volume Capacity&quot;,      &quot;description&quot;: &quot;Volume space available for data, e.g. 512Mi, 2Gi.&quot;,      &quot;value&quot;: &quot;1Gi&quot;,      &quot;required&quot;: true    &#125;,    &#123;      &quot;name&quot;: &quot;POSTGRESQL_VERSION&quot;,      &quot;displayName&quot;: &quot;Version of PostgreSQL Image&quot;,      &quot;description&quot;: &quot;Version of PostgreSQL image to be used (9.2, 9.4, 9.5 or latest).&quot;,      &quot;value&quot;: &quot;9.5&quot;,      &quot;required&quot;: true    &#125;  ]&#125;","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"Openshift Commandline CheatSheet","url":"/2019/02/27/openshift-2018-01-03-openshift-commandline-shortcut/","content":"创建新的projectoc new-project postgres --display-name=&#39;postgres&#39; --description=&#39;postgres&#39;\n切换projectoc project myproject\n删除projectoc delete project myproject\n创建资源. 从json(yml)生成一个OpenStack可以用模板。oc create -f Filename.json(.yml)oc process -f file.json|oc create -f -  ##处理模板生产构建配置，然后创建资源\n\n创建app， 就是部署oc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [options]oc new-app --name=dbinit --strategy=docker https://github.com/devops-with-openshift/liquibase-example.git  ##将会从这个仓库拉代码，build on Dockerfile\n取消一个正在构建的appoc cancel-build buildname\n修改一个资源的配置oc patch dc postgresql -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;strategy&quot;:&#123;&quot;type&quot;:&quot;Recreate&quot;&#125;&#125;&#125;&#x27;\n\n设置应用配置oc set env dc postgresql POSTGRESQL_ADMIN_PASSWORD=password\n\n从docker镜像仓库导入最新的镜像信息oc import-image docker.io/busybox:latest --confirm ##把上游的镜像仓库镜像加入本地命名空间\n\n展示资源的信息oc get pods #展示pod资源的信息oc get rc redis  #展示replication controlleroc get -o wide pods  #展示详情oc get -o template pod myapp --template=&#123;&#123; .currentState.status &#125;&#125;\n\n设置trigger配置oc set triggers dc/registry --autooc set triggers bc/webapp --from-webhookoc set triggers bc/webapp --from-imagej=namespace/image:latest\n将模板转化为资源oc process -f template.json| oc create -f -\n导出资源，用于其它地方使用oc export service -o json  # 导出资源为jsonoc export svc --as-template=test # 导出所有服务，作为模板oc export Resource -l name=test # 导出资源，打上标签\n\npod同步容器内外文件oc rsync dir POD:dir\n查看pod日志oc logs $(oc get pods -l name=cats -o name)\n开启一个容器的shelloc get podsoc rsh mypod\n\n简写对应\n","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"Openshift Deployment","url":"/2018/01/02/openshift-2018-01-04-openshift-deployment/","content":"\nFor version v3.7; OS Mac; not recommend for production\n\n安装Openshift AllinOne.\n安装oc cli. 地址：https://github.com/openshift/origin/releases\n\nwget -c https://github.com//openshift/origin/releases/download/v3.7.0/openshift-origin-client-tools-v3.7.0-7ed6862-mac.ziptar -xvf openshift-origin-client-tools-v3.7.0-7ed6862-mac.zipmv openshift-origin-client-tools-v3.7.0-7ed6862-mac/oc /usr/local/bin\n\n启动Openshift, 使用预置配置, 挂出数据\n\noc cluster up --host-data-dir=&#x27;$REPLACE/oc/profiles/Devops/data&#x27; --host-config-dir=&#x27;$REPLACE/oc/profiles/Devops/config&#x27; --use-existing-config\n\n访问https://127.0.0.1:8443/login\n生产环境需要在linux下安装，并且需要考虑HA, openshift的安装脚本不支持single master升级到multiple master.\n\nOpenshift的部署叫replication controller.Openshift基于用户定义的模板，通过手动或者事件触发的方式开始部署。\n\n多种部署策略，支持模板配置部署方式\nRolling 是openshift默认部署策略，流程：\n执行pre lifecycle hook\n通过surge配置开始部署 \n通过最大不可用配置缩减旧的部署 \n重复扩展操作，直到新的部署达到想要的复制数，旧的部署慢慢将为0 \n执行post lifecycle hook \nOpenShift官网认证rolling策略不应该用于数据库部署，防止两个数据库同时运行，导致数据不一致\n\n\n触发部署方式Trigger，支持配置改变，镜像改变(image stream tag更新)，github, webhook等方式触发部署 oc describe dc myapp ##查看myapp配置 oc set triggers dc myapp  ## 查看myapp部署配置 oc set triggers dc myapp --remove-all  ##去除所有triggers, 但是config trigger auto=falseoc set triggers dc myapp --from-config  ##更新config自动部署oc set triggers dc myapp --from-config --remove ## 删除config\noc import-image docker.io/busybox:latest --confirm ##把上游的镜像仓库镜像加入本地命名空间oc set triggers dc myapp --from-image=welcome/busybox:latest --containers=myapp ##image 更新部署, 应用于myapp这个容器\nRecreate 策略, 和rolling在部署流程上不同。先删除旧的pod, 然后部署新的pod。\n执行pre lifecycle hook\n慢慢关闭上一个部署\n执行中间lifecycle hook\n部署新的应用\n执行post lifecycle hook\n\noc delete project welcomeoc new-project welcome --display-name=&#x27;Welcom&#x27; --description=&#x27;Welcome&#x27;oc new-app devopswithopenshift/welcome:latest --name=myappoc patch dc myapp -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;strategy&quot;:&#123;&quot;type&quot;:&quot;Recreate&quot;&#125;&#125;&#125;&#x27;oc set probe dc myapp --readiness --open-tcp=8080 --initial-delay-seconds=5 --timeout-seconds=5oc set probe dc myapp --liveness -- echo okoc expose svc myapp --name welcomeoc deploy myapp --latest\nCustom 订制\nlifecycle hooks. Openshift会另外起一个容器用于运行pre, mid, post hook.\n当前支持pod-based类型hook, execNewPod字段定义\npre hook在新镜像部署前运行\nmid hook只有recreate停止旧的pod后执行\npost hook在新镜像部署后执行\n\n\n\n\n支持事件触发部署\n自定义策略切换部署方式\n回滚\n自动可扩展AutoScaling\n\n针对openshift的特性，需要考虑应用的架构\n比如： session调整。是服务器session还是无状态，客户端缓存。\n\n部署的资源利用默认情况下，pod可以无限使用节点资源。当然也可以通过项目级别或者通过部署策略限制资源的使用。Openshift强制使用Cgroup控制CPU和内存使用。oc patch -n welcome --type=strategic dc myapp -p &#39;&#123;&quot;spec&quot;: &#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;containers&quot;:[&#123;&quot;name&quot;:&quot;myapp&quot;, &quot;resources&quot;:&#123;&quot;limits&quot;:&#123;&quot;cpu&quot;:&quot;100m&quot;,&quot;memory&quot;:&quot;256Mi&quot;&#125;&#125;&#125;]&#125;&#125;&#125;&#125;&#39;\n","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"What is OpenShift Resource","url":"/2018/01/07/openshift-2018-01-08-openshift-resource/","content":"\nOpenshift的资源定义类似Rest的资源\n\nOpenShift中的资源oc get -h 有一行说明Possible resources include builds, buildConfigs, services, pods, etc. To see a list of common resources, use &#39;oc get&#39;.. 所以任何构建，构建配置，服务，pod都是OpenShift的资源。由于Kubernetes采用Restful的架构，所以用rest的角度看待OpenShift资源应该更加合适，一个资源就是一个对象，有对应的类型、数据、和其它资源的关系，一组标准HTTP方法. 资源和OOP的对象类似，但是只有特定GET, PUT,POST等这几个标准HTTP方法。\n","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"OpenShift A/B Deployment","url":"/2018/01/08/openshift-2018-01-09-openshift-AB-deploy/","content":"OpenShift的AB部署AB test原理就不讲了。直接上代码\noc new-project cotd --display-name=&#x27;A/B Deployment Example&#x27;  --description=&#x27;A/B Deployment Example&#x27;oc new-app --name=&#x27;cats&#x27; -l name=&#x27;cats&#x27;  php:5.6~https://github.com/devops-with-openshift/cotd.git  -e SELECTOR=catsoc expose service cats --name=cats -l name=&#x27;cats&#x27;oc new-app --name=&#x27;city&#x27; -l name=&#x27;city&#x27;  php:5.6~https://github.com/devops-with-openshift/cotd.git -e SELECTOR=cities oc expose srv/city --name=city -l name=&#x27;city&#x27;","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"OpenStack ImageStream","url":"/2018/01/07/openshift-2018-01-08-openstack-imagestream/","content":"OpenShift的Image Stream按照官方的解释mage stream一个镜像流是一组tag的镜像. 这些镜像可以是来自：\n\nopenshift的私有镜像\n其它镜像流\n外部镜像仓库OpenShift的build,deployment组件可以监控image stream. 用于触发新的build或者deploy\n\n","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"OpenShift Annotation","url":"/2018/01/08/openshift-2018-01-09-openshift-annotation/","content":"OpenShift的AnnotationOpenshift的Annotation是一个键值对，用于机器识别等其它用途。它不像label适用于人类识别，所以可以存较大的值。\noc annotate route/ab haproxy.router.openshift.io/balance=roundrobin","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"OpenShift Blue-Green Deployment","url":"/2018/01/08/openshift-2018-01-09-openshift-bg-deployment/","content":"OpenShift的蓝绿部署使用oc patch route切换路由\noc new-project bluegreen --display-name=&quot;Blue Green Deployments&quot;  --description=&quot;Blue Green Deployments&quot;oc new-app https://github.com/devops-with-openshift/bluegreen#master  --name=blueoc expose service blue --name=bluegreen #注意route的名称oc new-app https://github.com/devops-with-openshift/bluegreen#green  --name=green##切换oc patch route/bluegreen -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;to&quot;:&#123;&quot;name&quot;:&quot;green&quot;&#125;&#125;&#125;&#x27;\n\n蓝绿最佳适用场景无状态的服务实现蓝绿部署较为轻松， 因为不需要考虑旧实例的长事务和数据的迁移和回滚。\n","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"Openshift Origin cluster部署配置要求和主机准备","url":"/2018/02/04/openshift-2018-02-05-prerequirement-deploy-a-cluster/","content":"\nhttps://github.com/openshift/openshift-ansible\n\n杂言官方提供的github ansible部署代码还是有一些坑的，首先不能用master分支的代码，master分支是他们的开发分支。所以需要选择某个release， 以下选择release-3.8作为说明。origin使用的kubernetes是1.7和docker1.12. 先说说配置要求和操作系统、网络等环境等要求, 这些要求覆盖所有节点。\n系统配置需求\nmaster，nodes, 外部etcd都有最小推荐配置. 大致上，master每1000pods需要额外1CPU和1.5GB内存。node的配置根据业务负载配置。\n\nCPU核数配置master和node默认使用系统所有的CPU, 可以设置环境变量GOMAXPROCS限制核数。\n开启SELinuxSELinux必须开启，配置文件/etc/selinux/config\nSELINUX=enforcingSELINUXTYPE=targeted\n\n使用OverlayFS文件系统节点和master必须同步时间ansible的环境变量设置为openshift_clock_enabled=true\n有些容器是previlige, 可以访问docker守护进程。这就等于这个容器对所有容器和镜像有root权限，openshift使用security context constraints 来控制容器的权限。\n容器的DNS由于docker容器不会认宿主机的&#x2F;etc&#x2F;hosts, 所以所有节点都要安装dnsmasq, 容器访问宿主机的dns，宿主机再查询dns nameserver. ansible剧本需要\nNM_CONTROLLED=yes  ##使用network manageropenshift_use_dnsmasq=true\n\n默认容器内的域名解析按如下顺序进行：\n使用宿主机的&#x2F;etc&#x2F;resolv.conf解析\n容器内/etc/origin/node/node-config.yaml有一条宿主机的IP作为dnsIP\n如果没有设置dnsIP， 默认值是kubernetes service IP。也就是pod中/etc/resolv.conf的第一个nameserver.\n\n\n宿主机使用DNS宿主机的域名解析配置取决于是否开启DHCP动态主机地址分配。如果没有开启使用静态ip地址，需要把DNS nameserver加入到NetworkManager;如果启用DHCP, NetworkManager会根据DHCP的配置自动分配DNS;或者在node-config.yaml手动加入dnsIP. 使用dig测试是否正确配置dns\n\n$ dig &lt;node_hostname&gt; @&lt;IP_address&gt; +short$ dig master.example.com @10.64.33.1 +short10.64.33.100\n\n联通的网络\nNetworkManager必须启用NM_CONTROLLED=yes，否则DNS配置会有问题。\nm-m, m-n节点需要开通一些端口，单节点master和多节点master的开放端口也不同。 OpenShift会自动配置一些iptables规则。)\n\n存储Openshift使用Kubernetes的持久卷Persistent Volume提供持久化存储。在安装Openshift完成后，可以根据具体需求，要求更多的存储资源。安装脚本里有提供相应的代码，支持的存储方式包括：NFS, GlusterFS, Ceph RBD, OpenStack Cinder, AWS Elastic Block Store (EBS), GCE Persistent Disks, and iSCSI.\n在云上安装openshift origin需要考虑：1. 配置安全组，开通部分端口2. 需要覆盖一些参数。通过下面指令得到正确的值。ansible-playbook  [-i /path/to/inventory] \\    ~/openshift-ansible/roles/openshift_facts/library/openshift_facts.py\n\n如果安装容器化的GlusterFS,需要考虑额外的一些配置https://docs.openshift.org/latest/install_config&#x2F;install&#x2F;prerequisites.html#prereq-containerized-glusterfs-considerations\n主机需要安装的包可能需要epel仓库， 自行解决\nyum install -y wget git net-tools bind-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct ansible pyOpenSSL docker-1.12.6yum update\n\n配置docker的容器和镜像存储镜像和容器在openshift被认为是ephemeral, 他们和持久层分开存储。对于RHEL7, 默认的docker存储后端是在loopback的thin pool，这个配置不适合生产环境，在生产环境下，必须使用thin pool逻辑卷。配置存储必须在创建镜像和容器前，否则数据将丢失。使用docker-storage-setup脚本可以配置thin pool和docker的存储驱动。这个脚本会读取&#x2F;etc&#x2F;sysconfig&#x2F;docker-storage-setup文件， 支持三种方式创建逻辑卷：\n\n使用另一存储块，推荐方式。需要新增一块存储块到主机。在&#x2F;etc&#x2F;sysconfig&#x2F;docker-storage-setup设置DEVS到新增的块，设置VG为新建的卷组名。运行脚本\n\ncat &lt;&lt;EOF &gt; /etc/sysconfig/docker-storage-setupDEVS=/dev/vdcVG=docker-vgEOF\n\n\n使用已有的逻辑卷组, 修改上述脚本指定VG&#x3D;.\n使用根文件系统剩余的卷空间, 直接运行脚本。查看存储配置, 重启docker.\n\ncat /etc/sysconfig/docker-storageDOCKER_STORAGE_OPTIONS=--storage-opt dm.fs=xfs --storage-optdm.thinpooldev=/dev/mapper/docker--vg-docker--poollvsLV          VG   Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convertdocker-pool rhel twi-a-t---  9.29g             0.00   0.12\n\n管理容器日志容器的日志（&#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;&#x2F;-json.log）大小增大可能会有问题，可以通过配置docker json-file 日志驱动来限制大小和日志数量。修改&#x2F;etc&#x2F;sysconfig&#x2F;docker, 然后重启。\nOPTIONS=&#x27;--insecure-registry=172.30.0.0/16 --selinux-enabled --log-opt max-size=1M --log-opt max-file=3&#x27;\n","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"Openshift中Kubernetes的基础设施","url":"/2018/03/04/openshift-2018-03-05-kubernetes-infrastrcture/","content":"前言主要是从Openshift Origin的Doc看到的, 对应k8s 1.7. 加上官网的Doc. \nMastersmasters上的组件含有API server, controller manager server和etcd(可以单独host)。 master管理k8s集群中的nodes，调度节点上的pods.\n\nAPI server. 可以是一个独立的进程运行。API server验证和配置pods、services和replication controller; 分配pods到节点，根据service的配置同步pod的信息。\nETCD. 存储主机的状态， 其他组件查看etcd的动态，使自身做相应状态变化。ETCD使用RAFT算法，所以最后以2n+1来部署。\nController Manager Server. 这个服务可以是单个进程，如果是多个进程那么就只有一个leader.\n监控etcd的复制状态变化，然后调用API做到相应的复制改变; \nnode控制器， 监控节点是否健康。\nendpoints控制器，控制endpoint对象植入。\n账号和token控制器\n\n\nscheduler（openshift 没有提到），k8s官网写了。scheduler用于还未在node创建的pod监控，然后调度一个node让pod运行。scheduler会考虑资源的各种要求，包括：软硬件，策略限制，affinity and anti-affinity specifications，数据地区，内部负载影响和期限。\nHAProxy. 如果master是HA, 默认采用HAProxy作为master api server负载均衡器. 当然也可以选择自定义的。\ncloud-controller-manager。 用于和云供应商服务交互，云供应商有如下服务：\n节点控制器\n路由控制器\n服务控制器（k8s service)\n卷控制器\n\n\n\nNodes节点提供容器的运行环境。每个节点都运行着master管理需要的服务和pods运行需要的服务（具体有Docker, kubelet, service proxy)Node对象定义\napiVersion: v1 kind: Node metadata:  creationTimestamp: null  labels:     kubernetes.io/hostname: node1.example.com  name: node1.example.com spec:  externalID: node1.example.com status:  nodeInfo:    bootID: &quot;&quot;    containerRuntimeVersion: &quot;&quot;    kernelVersion: &quot;&quot;    kubeProxyVersion: &quot;&quot;    kubeletVersion: &quot;&quot;    machineID: &quot;&quot;    osImage: &quot;&quot;    systemUUID: &quot;&quot;\n\nNodes中的Kubelet其实是一个agent. kubelet用来通过manifest控制node的container健康运行， kubelet不会管理不是k8s创建的容器。 container manifest是一个描述pod的yaml文件。manifest可以通过4种方式提供给kubelet:\n\n命令行指定的文件地址\nHttp endpoint\n监控etcd服务\nkubelet作为http服务，监听http请求，提交新的manifest\n\nNodes中的service proxy每一个节点有一个简单的网络proxy, 能够让TCP或UDP把流量导向后端pods\nNodes中的Addons附加插件Addons(小怪？)是实现集群功能的pods和service， 还有一些其他附加功能. addon对象在kube-system创建，它们是有命名空间的。\n\nDNS. Kubernetes的服务需要DNS服务器， 被k8s启动的容器会自动包含dns服务器\nUI界面\nContainer Resource Monitoring\nCluster-level Logging\n\n","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"Openshift的网路","url":"/2018/02/21/openshift-2018-02-22-openshift-network/","content":"\nhttps://docs.openshift.org/latest/architecture/networking/networking.html\n\n前言openshift的网路架构是建立在kubernetes的service上的。K8S的Service是一组pods和访问这些pods的逻辑抽象，service解耦了下游pods的网路变化。但是由于Service也会变化，所以openshift在master运行skyDNS来解决service网路的变化。\nSDNSoftware Define Networking软件定义集群网络使openshift的pods相互能够通信，SDN使用Open vSwitch(OVS)来管理配置网路。有三种SDN插件可以配置pods网路：\n\novs-subnet, 提供所有的pods直接相互通信\novs-multitenant, 能够提供项目级别的pods和service隔离。每一个项目都有一个VNID, 这个网路内项目之间的pods不能相互通信。VNID 0是一个例外，可以和所有pods通信，可以提供负载均衡等服务。Openshift正是使用br0的tag VNID功能，做到网络的隔离。\novs-networkpolicy, 这里用户可以自己定义网路规则。\n\n在master上，Openshift SDN分配网络给运行节点并且注册到etcd, SDN会给新的节点分配新的子网，可以定义的网段有10.128.0.0&#x2F;14, node节点定义在10.128.0.0&#x2F;23。SDN是不会给master配置集群网络访问，所以master节点是不能访问集群网络的，除非它也作为运行节点。\n在运行节点，SDN先在etcd注册，然后会创建三个网络装置: br0,tun0,vxlan_sys_4789\n\nbr0, ovs的网桥，pods会和它连在一起。SDN也会在这个网桥上配置非子网的流量进出规则。\ntun0, ovs的内部端口，在br0的port 2, 是子网的网关，负责pods访问外部网络。SDN会通过配置netfilter和路由规则使集群子网NAT访问外部网络。eth0 (in A’s netns) → vethA → br0 → tun0 → (NAT) → eth0 (physical device) → Internet\nvxlan_sys_4798, OVS的VXLAN，在br0的port 1, 提供容器访问远程节点。eth0 (in A’s netns) → vethA → br0 → vxlan0 → network → vxlan0 → br0 → vethB → eth0 (in B’s netns)\n\nSDN会对每个新的pod作4件事：\n\n分配子网内的一个新的ip给pod。\n连接宿主机边pod的veth网络接口到OVS的br0。\n在OVS数据库增加一条OpenFlow规则，让到新pod的流量导入正确的OVS端口。\n如果是多租户情况， 给流量打上标记VNID, 并且使其正确流向对应VNID. 没有匹配的VNID将会采用默认规则。\n\nSDN运行节点也会跟踪master节点对子网的更新, 当新的子网加入，br0会在新增一条Openflow规则，使vxlan0能够访问远程子网。\n","categories":["openshift","kubernetes"],"tags":["kubernetes, openshift"]},{"title":"Luigi基础概念","url":"/2018/07/19/python-2018-07-20-luiqi-base/","content":"基础模块想要构建一个基本的Luigi工作流, 需要创建Task和Target类, 还有Parameter类.使用这些类来定义任务的好处是在代码里定义依赖, 而不是使用DSL.\nTargetTarget是Task output返回的结果. Target类对应磁盘上的一个文件, HDFS上的一个文件或者某种checkpoint(比如数据库的条目). 理论上只需要实现exists方法,用于返回文件是否存在就可以实现这个类. Target有多个子类:LocalTarget, HdfsTarget, S3Target, ssh.RemoteTarget, ftp.RemoteTarget, mysqldb.MysqlTarget, redshift.RedshiftTarget, 所以基本上不需要自己subclass.Target类是对文件的映射, 如果只有一个target支持原子性操作, 也支持open()和Gzip. 多个targets需要用户保持文件的原子性操作.\nTaskTask是实际做任务的地方. 通过run(), output(), requires()设置任务的行为. Task通过其它Task产生的Targets作为输入, 结果产生也是Target.任务之间可以通过requires()指定依赖.每个任务通过output()指定输出, input()指定输入.\n\nrequires()返回本task需要的其它tasks, 可以是task对象或封装的dicts, lists, tuples. \n\n如果需要依赖外部task, 那么可以封装ExternalTask, 然后把这个task作为当前task的requires\n\n\nclass LogFiles(luigi.ExternalTask):    def output(self):        return luigi.contrib.hdfs.HdfsTarget(&#x27;/log&#x27;)\n\n\nrun()\n\nrun()函数是实际的任务运行地方, 如果有requires那么就会先解决依赖, 然后跑run的逻辑. input()会把requires的输出封装成targets, 用作run()的输入.\nclass TaskWithManyInputs(luigi.Task):    def requires(self):        return &#123;&#x27;a&#x27;: TaskA(), &#x27;b&#x27;: [TaskB(i) for i in xrange(100)]&#125;    def run(self):        f = self.input()[&#x27;a&#x27;].open(&#x27;r&#x27;)        g = [y.open(&#x27;r&#x27;) for y in self.input()[&#x27;b&#x27;]]\n\n\ntask的事件和回调\n\nluigi有事件系统能够注册事件回调, 然后使用自定义的task触发任务.\n@luigi.Task.event_handler(luigi.Event.SUCCESS)def celebrate_success(task):    &quot;&quot;&quot;Will be called directly after a successful execution       of `run` on any Task subclass (i.e. all luigi Tasks)    &quot;&quot;&quot;    ...@luigi.contrib.hadoop.JobTask.event_handler(luigi.Event.FAILURE)def mourn_failure(task, exception):    &quot;&quot;&quot;Will be called directly after a failed execution       of `run` on any JobTask subclass    &quot;&quot;&quot;    ...    luigi.run()\n\nParameterParameter可以给每个task增加参数, 用于定制化一些额外信息.\n\n使用@inherits, @requires来传递多个task直接的参数, 考虑如下问题:\n\nclass TaskA(luigi.ExternalTask):    param_a = luigi.Parameter()    def output(self):        return luigi.LocalTarget(&#x27;/tmp/log-&#123;t.param_a&#125;&#x27;.format(t=self))class TaskB(luigi.Task):    param_b = luigi.Parameter()    param_a = luigi.Parameter()    def requires(self):        return TaskA(param_a=self.param_a)class TaskC(luigi.Task):    param_c = luigi.Parameter()    param_b = luigi.Parameter()    param_a = luigi.Parameter()    def requires(self):        return TaskB(param_b=self.param_b, param_a=self.param_a)\n对上述代码,下游的task将会需要写上所有上游需要的参数, 这样就会产生参数爆炸, 如果想要简化参数, 可以是使用@inherits和requires\nimport luigifrom luigi.util import inheritsclass TaskA(luigi.ExternalTask):    param_a = luigi.Parameter()    def output(self):        return luigi.LocalTarget(&#x27;/tmp/log-&#123;t.param_a&#125;&#x27;.format(t=self))@inherits(TaskA)class TaskB(luigi.Task):    param_b = luigi.Parameter()    def requires(self):        t = self.clone(TaskA)  # or t = self.clone_parent()        return t@inherits(TaskB)class TaskC(luigi.Task):    param_c = luigi.Parameter()    def requires(self):        return self.clone(TaskB)\n\nLuigi的模式Luigi没有中间文件的概念, 所以如果两个依赖的任务运行一半失败, 中间结果将会被保留.\n如何触发多个任务在每个不相关的任务链的结尾加一个相同的dummy task, 这样只需要触发这个任务就会触发多个任务, 类似make.实际使用时, 在Luigi中使用WrapperTask来封装和唤起其它tasks就行了, 它不会有输出output.\nclass AllReports(luigi.WrapperTask):    date = luigi.DateParameter(default=datetime.date.today())    def requires(self):        yield SomeReport(self.date)        yield SomeOtherReport(self.date)        yield CropReport(self.date)        yield TPSReport(self.date)        yield FooBarBazReport(self.date)\n\nLuigi的执行模型luigi的执行模型很简单, 一个worker的进程执行所有tasks, 所以如果有成千上万个tasks, 扩展性将成为问题.\n调度luigi的调度由单独的luigid中心化管理, 多个worker执行run()时, 每次都会从依赖树从头向下遍历, 找到需要执行的task运行, 跳过已完成的task. 见gif\n","categories":["python"],"tags":["python"]},{"title":"Pandas的Groupby","url":"/2018/08/30/python-2018-08-31-pandas/","content":"前言一般需要对数据做分割&#x2F;处理&#x2F;合并的时候会使用groupby, groupby的意思类似sql语句的分组. 对一个DataFrame做分割、处理、合并的过程一般如下图所示， 通过这样的流程能做到聚合数据的能力。\n分割官方叫split, 是把数据依照某种条件分组. 对一个DataFrame使用groupby就达到了split.\nIn [1]: import pandas as pdIn [2]: df = pd.DataFrame(&#123;&#x27;A&#x27;: [&#x27;foo&#x27;,&#x27;bar&#x27;,&#x27;foo&#x27;,&#x27;bar&#x27;,&#x27;foo&#x27;,&#x27;bar&#x27;,&#x27;foo&#x27;,&#x27;foo&#x27;], &#x27;B&#x27;:[&#x27;one&#x27;,&#x27;one&#x27;,&#x27;two&#x27;,&#x27;three&#x27;,&#x27;two&#x27;,&#x27;two&#x27;,&#x27;one&#x27;,&#x27;three&#x27;],&#x27;C&#x27;:[332,42,12,31,21,1,3,14]&#125;)In [3]: dfOut[3]:      A      B    C0  foo    one  3321  bar    one   422  foo    two   123  bar  three   314  foo    two   215  bar    two    16  foo    one    37  foo  three   14In [4]: group_a=df.groupby(&#x27;A&#x27;)In [5]: group_aOut[5]: &lt;pandas.core.groupby.DataFrameGroupBy object at 0x7f78fb9cfc50&gt;In [52]: grouped.get_group(&#x27;A&#x27;)Out[52]:      A      B         C         D1  bar    one  0.254161  1.5117633  bar  three  0.215897 -0.9905825  bar    two -0.077118  1.211526\n\n处理apply对每个分割好的group运行某个(多个)函数. 在这一步我们可能会使用到如下方法:\n\naggregation的作用是统计组里的数据: 如平均值, 求和, 计数. 从下面的例子可以看出来, aggregation后得到一个以groupby参数作为groupname,并且以它为index的DataFrame; 如果是通过多个值groupby的那么结果是以MultiIndex为索引, 当然index是可以通过as_index来设置的(.reset_index()也可以得到相同效果). aggregation函数会排除NA值.\n\nIn [6]: group_a.agg(sum)   ## 注意, 这里如果传递[sum]将会对所有column做sumOut[6]:        CA       bar   74foo  382In [8]: group_a[&#x27;C&#x27;].agg(sum)  ## 注意, 这里传给agg, 如果是[sum], 返回的是Dataframe, 否则是SeriesOut[8]: Abar     74foo    382Name: C, dtype: int64In [9]: group_a_b=df.groupby([&#x27;A&#x27;,&#x27;B&#x27;])In [10]: group_a_b.agg(sum)Out[10]:              CA   B         bar one     42    three   31    two      1foo one    335    three   14    two     33In [15]: group_a_b=df.groupby([&#x27;A&#x27;,&#x27;B&#x27;], as_index=False).agg(sum)In [16]: group_a_bOut[16]:      A      B    C0  bar    one   421  bar  three   312  bar    two    13  foo    one  3354  foo  three   145  foo    two   33\n所有的Aggregation函数如下表, 它们返回的对象一般会减少维度:\nagg()是可以一次运作多个函数的, 注意下面这个函数是对于grouped Series, 即切出了一个series操作的, 返回的是dataframe. \nIn [30]: group_a[&#x27;C&#x27;].agg([sum, min])Out[30]:      sum  minA            bar   74    1foo  382    3\n如果对一个groupby Dataframe应用多个函数, 那么每个column都会有各自的聚合函数列:\nIn [63]: df.groupby(&#x27;A&#x27;).agg([sum, min,max])Out[63]:                      B              C                            sum  min  max  sum min  maxA                                             bar        onethreetwo  one  two   74   1   42foo  onetwotwoonethree  one  two  382   3  332###重命名列名In [65]: df.groupby(&#x27;A&#x27;).agg([sum, min,max]).rename(columns=&#123;&#x27;B&#x27;:&#x27;reName&#x27;&#125;)Out[65]:                    reName              C                            sum  min  max  sum min  maxA                                             bar        onethreetwo  one  two   74   1   42foo  onetwotwoonethree  one  two  382   3  332\nagg的函数也可以通过lambda把所有列改成标量值:\nIn [17]: df.groudby(&#x27;A&#x27;).agg(lambda x:1)Out[17]:      B  CA        bar  1  1foo  1  1\n通过给agg()传递dict可以对Dataframe的不同column应用不同的聚合. 这样得出的column没有顺序的, 想要有顺序需要使用OrderedDict\nIn [51]: df.groupby(&#x27;A&#x27;).agg(&#123;&#x27;B&#x27;: sum, &#x27;C&#x27;: min&#125;)Out[51]:      C                  BA                        bar  1        onethreetwofoo  3  onetwotwoonethree\n\n\ntransformationtransform方法把一个groupby对象做转变, 返回一个和grouped对象一样index的对象（这点和agg不同， agg通常返回维度减少的数据）. 这个transform实际传入的函数必须满足:\n返回和group chunk一样大小的对象, 或者返回的对象大小可以broadcastable到group chunk size(broadcasting是numpy的术语, 描述了numpy对待不同形状阵列时如何计算.)\n在group chunk上做一列一列操作. 实际使用的是chunk.apply\n不能直接(inplace)在group chunk上修改, group chunk需要认为是不可改变的, grouped.transform(lambda x: x.fillna(inplace=True))这样的直接修改可能带来意外结果.\n也可以直接对整个chunk做操作\n\n\n\n这里有个例子，讲述了如何使用transform减少处理的逻辑http://pbpython.com/pandas_transform.html\n\nfilteration使用filter函数可以返回原来对象的一个子集. filter的参数是个应用到整个组的函数， 其返回为True&#x2F;False.\n\nIn [115]: sf = pd.Series([1, 1, 2, 3, 3, 3])In [116]: sf.groupby(sf).filter(lambda x: x.sum() &gt; 2)Out[116]: 3    34    35    3dtype: int64In [117]: dff = pd.DataFrame(&#123;&#x27;A&#x27;: np.arange(8), &#x27;B&#x27;: list(&#x27;aabbbbcc&#x27;)&#125;)In [118]: dff.groupby(&#x27;B&#x27;).filter(lambda x: len(x) &gt; 2)Out[118]:    A  B2  2  b3  3  b4  4  b5  5  bIn [119]: dff.groupby(&#x27;B&#x27;).filter(lambda x: len(x) &gt; 2, dropna=False)Out[119]:      A    B0  NaN  NaN1  NaN  NaN2  2.0    b3  3.0    b4  4.0    b5  5.0    b6  NaN  NaN7  NaN  NaNIn [120]: dff[&#x27;C&#x27;] = np.arange(8)In [121]: dff.groupby(&#x27;B&#x27;).filter(lambda x: len(x[&#x27;C&#x27;]) &gt; 2)Out[121]:    A  B  C2  2  b  23  3  b  34  4  b  45  5  b  5\n\n\n合并combining是再把处理好的数据组合一起.\n","categories":["python"],"tags":["python"]},{"title":"Pandas的数据结构","url":"/2018/09/15/python-2018-09-16-pandas%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","content":"前言基本上, Pandas的Series沿袭了Numpy的设计, 是一维数组和数组的索引, 和Numpy不同的一点是这个一维数组可以是异构的,比如数字int和字符串混在一起, 这个时候数组的类型是object. 而DataFrame是为了处理表格和异构多维的数据.\nSeries可以用python的list初始化一个Series, 也可以用python的Dict初始化一个Series. 如果使用Dict那么序列的索引就是Dict的Key.\n&gt;&gt;&gt; d = &#123;&#x27;d&#x27;: 1, &#x27;e&#x27;:2, &#x27;a&#x27;:0&#125;&gt;&gt;&gt; pd.Series(d)a    0d    1e    2\n想要得到自己想要的顺序可以使用参数index排序Series, 下图的例子可以看到索引b, 但是它没有值会用NaN表示(可以使用isnull, notnull检测是否为空), 没有索引的值会被省略.\n&gt;&gt;&gt; pd.Series(d, index=[&#x27;e&#x27;,&#x27;b&#x27;,&#x27;d&#x27;])e    2.0b    NaNd    1.0dtype: float64\nPandas筛选比较强大的一点是直接可以在[]输入条件, 这点可能和python本身的语法有一点不同.\n&gt;&gt;&gt; obj2= pd.Series(d)&gt;&gt;&gt; obj2a    0d    1e    2dtype: int64&gt;&gt;&gt; obj2 [ obj2 &gt; 1]e    2dtype: int64\n两个Series是可以直接通过索引直接做计算的, 类似数据库的join. 计算子必须都需要有相同的索引才会有计算结果, 否则会出现NaN.\nIn [35]: obj3Out[35]:Ohio      35000Oregon    16000Texas     71000Utah       5000dtype: int64In [36]: obj4Out[36]:California   NaNOhio         35000Oregon       16000Texas        71000dtype: float64In [37]: obj3 + obj4Out[37]:California    NaNOhio          70000.0Oregon        32000.0Texas         142000.0Utah          NaNdtype: float64\n可以给Series起一个名字, 使用Series.name属性, 取个名字的好处是如果把Series放入Dataframe, 那么这个name就是Dataframe的列名.(DataFrame也会有index名称和column名称)\nIn [1]: s = pd.Series([&quot;A&quot;,&quot;B&quot;,&quot;C&quot;], name=&quot;foo&quot;)In [2]: sOut[2]: 0    A1    B2    CName: foo, dtype: objectIn [3]: pd.DataFrame(s)Out[4]:   foo0   A1   B2   C\n\n\nDataFramePandas的DataFrame代表了一组表格(一般是二维, 可以使用hierarchical indexing来表示更高的维度数组), 数据的底层存储如下图所示, 数据会按类型分成不同的块存储实际的数据. 可以看到数据被没有指向具体的列, 而是通过BlockManager这个类是来管理实际数据到列名的映射.有很多种方式构建DataFrame, 常见的一种是通过Dict; 如果想在创建的时候指定索引, 可以使用嵌入式dict.\nIn [44]: data = &#123;&#x27;state&#x27;: [&#x27;Ohio&#x27;, &#x27;Ohio&#x27;, &#x27;Ohio&#x27;, &#x27;Nevada&#x27;, &#x27;Nevada&#x27;, &#x27;Nevada&#x27;],            &#x27;year&#x27;: [2000, 2001, 2002, 2001, 2002, 2003],            &#x27;pop&#x27;: [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]&#125;In [45]: frame = pd.DataFrame(data)In [46]: frameOut[46]:   pop   state  year0  1.5    Ohio  20001  1.7    Ohio  20012  3.6    Ohio  20023  2.4  Nevada  20014  2.9  Nevada  20025  3.2  Nevada  2003&gt;&gt;&gt; pop = &#123;&#x27;Nevada&#x27;: &#123;2001: 2.4, 2002: 2.9&#125;,&#x27;Ohio&#x27;: &#123;2000: 1.5, 2001: 1.7, 2002: 3.6&#125;&#125;&gt;&gt;&gt; frame3 = pd.DataFrame(pop)&gt;&gt;&gt; frame3      Nevada  Ohio2000     NaN   1.52001     2.4   1.72002     2.9   3.6\n下图是所有的输入数据类型可以构建DataFrame:\nDataFrame可以指定column参数选择只要某几列, 也可以在column任意添加列名, 如何column这个字段赋予了不存在的列, 那么新加的这个列会显示NaN. 两者的区别在于使用属性访问时, 这个python的attribute必须符合python的规则.\n&gt;&gt;&gt; pd.DataFrame(data, columns=[&#x27;pop&#x27;,&#x27;jik&#x27;])   pop  jik0  1.5  NaN1  1.7  NaN2  3.6  NaN3  2.4  NaN4  2.9  NaN5  3.2  NaN\n想要取得DataFrame的某个列, 可以采用.columnName或切片的方式:\nIn [51]: frame2[&#x27;state&#x27;]Out[51]:one        Ohiotwo        Ohiothree      Ohiofour     Nevadafive     Nevadasix      NevadaName: state, dtype: objectIn [52]: frame2.yearOut[52]:one      2000two      2001three    2002four     2001five     2002six      2003Name: year, dtype: int64\n通过DataFrame.loc[]可以选取某几行, 这个loc用的是[] ,而不是(), 和python的语法又有些区别. 一般使用loc[]定位字符串(label)索引, iloc[]定位数字(number)索引.\n&gt;&gt;&gt; frame.iloc[3]pop         2.4state    Nevadayear       2001Name: 3, dtype: object\n可以使用Series给DataFrame赋值, Series将成为DataFrame的一个列, 列值必须和DataFrame的长度一致, 缺失的值将会NaN.\n&gt;&gt;&gt; val = pd.Series([-1.2, -1.5, -1.9], index=[&#x27;two&#x27;,&#x27;four&#x27;,&#x27;five&#x27;])&gt;&gt;&gt; frame2[&#x27;debt&#x27;] = val&gt;&gt;&gt; frame2       year   state  pop  debtone    2000    Ohio  1.5   NaNtwo    2001    Ohio  1.7  -1.2three  2002    Ohio  3.6   NaNfour   2001  Nevada  2.4  -1.5five   2002  Nevada  2.9  -1.9six    2003  Nevada  3.2   NaN\nDateFrame列赋值还有一个快捷判断的语法, 可以一行代码先判断再赋值\n&gt;&gt;&gt; frame2[&#x27;new_col&#x27;] = frame2.state == &#x27;Ohio&#x27;&gt;&gt;&gt; frame2       year   state  pop  debt  new_colone    2000    Ohio  1.5   NaN     Truetwo    2001    Ohio  1.7  -1.2     Truethree  2002    Ohio  3.6   NaN     Truefour   2001  Nevada  2.4  -1.5    Falsefive   2002  Nevada  2.9  -1.9    Falsesix    2003  Nevada  3.2   NaN    False\n列的删除: 使用del df[clo]DataFrame的values属性会返回一个二维ndarray的数组. \n&gt;&gt;&gt; frame3.valuesarray([[nan, 1.5],       [2.4, 1.7],       [2.9, 3.6]])\n\nIndex对象Index对象负责存储轴标签axis label和其它元信息如索引名称, 任何数组或其它序列的标签在转化成Series或DataFrame的时候会被转成Index.\n&gt;&gt;&gt; lables = pd.Index(np.arange(3))&gt;&gt;&gt; lablesInt64Index([0, 1, 2], dtype=&#x27;int64&#x27;)&gt;&gt;&gt; obj4 = pd.Series([1.4,3,1], index=lables)&gt;&gt;&gt; obj40    1.41    3.02    1.0dtype: float64&gt;&gt;&gt; obj4.index is lablesTrue\npandas的索引对象是不可变的, 既可以把index当成数组, 可以当成集合, 注意这个集合不同于python的set, 集合是可以重复的.\n&gt;&gt;&gt; dup_index = pd.Index([&#x27;foo&#x27;,&#x27;foo&#x27;,&#x27;bar&#x27;])&gt;&gt;&gt; dup_indexIndex([u&#x27;foo&#x27;, u&#x27;foo&#x27;, u&#x27;bar&#x27;], dtype=&#x27;object&#x27;)\nIndex对象的一些常用方法:\n","categories":["python"],"tags":["python, pandas"]},{"title":"Pandas的常用函数","url":"/2018/09/16/python-2018-09-17-pandas-common-methods/","content":"前言Pandas有一些常用方法, 可以作为回调函数用于其它函数.\npandas.DataFrame.reindexreindex方法用于把DataFrame按新的索引转化, 同时可选填充空值或者其他方法(如ffill, 按前项填充). 默认情况下, 重新索引过的DataFrame会复制原来DataFrame, 除非索引没有变或者设置copy=False.\n&gt;&gt;&gt; frame5 = pd.DataFrame(np.arange(9).reshape((3,3)), index=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;], columns=[&#x27;Ohio&#x27;,&#x27;Texas&#x27;,&#x27;California&#x27;])&gt;&gt;&gt; frame5   Ohio  Texas  Californiaa     0      1           2b     3      4           5c     6      7           8&gt;&gt;&gt; frame5 = frame5.reindex([&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;])&gt;&gt;&gt; frame5   Ohio  Texas  Californiaa   0.0    1.0         2.0b   3.0    4.0         5.0c   6.0    7.0         8.0d   NaN    NaN         NaN\n\n丢弃某些行或列对于DataFrame, 想要删除某行或列是很方便的, 直接通过drop([])方法可以实现, 默认删除行, 如果需要删除列, 可以设置参数axis=1.\n&gt;&gt;&gt; frame5   Ohio  Texas  Californiaa   0.0    1.0         2.0b   3.0    4.0         5.0c   6.0    7.0         8.0d   NaN    NaN         NaN&gt;&gt;&gt; frame5.drop([&#x27;a&#x27;])   Ohio  Texas  Californiab   3.0    4.0         5.0c   6.0    7.0         8.0d   NaN    NaN         NaN&gt;&gt;&gt; frame5.drop([&#x27;Ohio&#x27;], axis=1)   Texas  Californiaa    1.0         2.0b    4.0         5.0c    7.0         8.0d    NaN         NaN\n","categories":["python"],"tags":["python, pandas"]},{"title":"SQL Server的触发器","url":"/2021/01/31/database-SQL-Server-2021-02-01-SQL-Server-Trigger/","content":"SQL Server的Trigger简介SQL Server主要有三种trigger: DDL Trigger, DML Trigger, Logon Trigger.下面将分别介绍。SQL Server的Logon Trigger是触发器在用户建立会话后将触发对应的存储过程。更加具体一点是在用户真正建立会话之前，认证成功之后的阶段。认证失败的时候触发器是不会触发的。我们可以使用logon trigger来审计和控制服务端会话，比如追溯登入活动，限制特定账号的登入会话数。实际上logon trigger对应于AUDIT_LOGIN事件，AUDIT_LOGIN可以用于Event Notifications。Event Notifications和Trigger的主要区别是，触发器是同步的，Event Notification是异步的。\nDDL Trigger是在各种DDL(Data Definition Language)事件时触发的存储过程。这些事件主要包括事务语句：CREATE, ALTER, DROP, GRANT, DENY, REVOKE, UPDATE STATISTICS. 某些系统存储过程运行DDL类似的操作也可以触发DDL触发器。 一般我们使用DDL Trigger来：\n\n防止数据库结构改变\n结构变化需要对应一些改变\n记录结构改变\n\nDML Trigger是当DML(Data Manipulation Language)事件发生时触发的存储过程。DML事件包含: INSERT, UPDATE, DELETE语句。DML trigger可以被用来保证业务规则和数据完整性, 尤其在底层一些contraints不能很好满足需要的时候。\nLogon Trigger触发器的执行顺序如果在LOGON事件定义了多个触发器， 触发器执行的先后顺序是可以在sp_settriggerorder中定义的。SQL Server不能保证其它触发器的执行顺序。\n管理事务在SQL Server真正触发trigger前，SQL Server会创建一个隐式的事务，此时事务计数为1.在所有logon trigger完成执行，事务才会commit. 当logon trigger执行完，事务计数为0时，SQL Server会报错。什么时候logon trigger会将事务计数置0呢？一般有两种情况，ROLLBACK TRANSACTION和不正确的COMMIT TRANSACTION数(每次commit会减一计数).在logon trigger使用ROLLBACK时需要注意：\n\n任何rollback前的数据修改将被回滚，包括在同一个事件之前已执行的触发。后续的trigger不会再触发。\n注意，当前triggerROLLBACK后面的语句将会继续执行，数据修改将不会被回滚。\n\nDDL TriggerDDL Trigger的类型DDL Trigger有两种类型： 事务型(Transact-SQL)和通用语言型(Common Language Runtime). 事务型的DDL Trigger应对服务级别或者数据库级别的事件，如修改服务配置(ALERT SERVER CONFIGUATION)或删除表(DROP TABLE);CLR Trigger是在.net写的assembly代码。\nDDL Trigger的范围DDL Trigger触发对应的事件可以是当前数据库或者当前服务器，取决于具体事件。数据库范围的DDL Trigger存储在数据库上, 运行select * sys.triggers视图可以看到数据库层面的DDL Trigger;服务器层面的DDL Trigger存储在master数据库，运行select * from sys.server_triggers可以查看服务器层面的DDL Trigger。\nDML TriggerDML Trigger的类型DML Trigger有三种trigger：AFTER Trigger,INSTEAD OF Trigger和CLR Trigger.\n\nAfter Trigger是在INSERT, UPDATE, MERGE, DELETE语句运行后触发的触发器。如果发生constraint, 这个trigger是不会触发的。\nINSTEAD OF trigger覆盖标准的触发器语句，所以可以用来处理错误或者在insert, update, delete之前做数值检查。这个触发器主要的两个优势是能让不能修改的视图支持更新，另一个优势是可以让你的批处理部分处理成功，部分处理失败。\nCLR Trigger\n\nAFTER触发器和INSTEAD OF触发器的区别:![](&#x2F;images&#x2F;2021-02&#x2F;comparison of after&amp;instead of trigger.png)\n","categories":["database"],"tags":["database, trigger"]},{"title":"SQL Server的存储过程","url":"/2021/01/31/database-SQL-Server-2021-2-1-SQL-Server%E7%9A%84%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/","content":"简介SQL Server 的存储过程是微软基于 ANSI SQL 的扩展，和其他数据库的过程类似是一组或多组事务性 SQL 语句。它的特点是：\n\n接受多个输入参数，可以返回多个值。\n包含编程语句用于操作数据库，包括调用其他存储过程。\n返回调用成功与否的状态。\n\n使用存储过程的好处\n减少服务端、客户端的网络流量， 存储过程的代码是一次性批量传输给服务端，不像普通语句是一行一次传输给客户端，这样减少了网络流量。\n更安全， 不同的用户和客户端可以运行存储过程操作底层的数据库对象，即便这个用户没有底层对象的权限。（减少了单独对用户的授权的便捷性，但是更安全了？）EXECUTE AS语句用来作为执行过程用户。存储过程是可以被加密的。\n复用代码\n更容易维护\n更好的性能，因为第一次运行后执行计划会被保留。\n\n存储过程的类型\n自定义 用户自定义的存储过程可以用于所有数据库(除了 Resource 数据库).\n临时 临时存储过程也是一种用户自定义过程，存储在tempdb库中，它的生命周期就是连接的时间。临时存储过程又分为local和global, 它们的区别在于名字、可见性和可用性。local存储过程以#开头命名，只对当前连接用户可见，连接关闭将删除过程。global存储过程以##命名开头，这个过程创建后对所有用户可见，当最后一个会话关闭后将删除过程。\n系统 系统存储过程存储在Resource数据库，逻辑上以sys表出现。另外msdb数据库也有系统存储过程存在dbo表中，用于调度警告和任务。一般系统存储过程以sp_命名开头。用户可以扩展系统存储过程，一般以xp_命名开头，见官网资料 [https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/general-extended-stored-procedures-transact-sql?view=sql-server-2017](General Extended Stored Procedures)\n扩展自定义 用户可以使用 C 语言来创建外部 DLL,不过为了将来的兼容性不建议使用。\n\n","categories":["database"],"tags":["database, stored procedures"]},{"title":"SQL Server2019智能查询和性能优化简介","url":"/2021/02/04/database-SQL-Server-2021-02-05-SQL-SERVER-2019/","content":"简介SQL Server2019 主要的新特性可以用官方的一张图说明：\nSQL Server Intelligent Performance2019 版本主要在几方面提供查询性能优化：\n\nIntelligent Query Processing智能查询处理\nLightweight Query Profiling轻量级查询分析\nSequential Key Insert Performance顺序键插入性能优化\nIn-Memory Database\nHybrid Buffer Pool混合缓冲池\nMemory Optimized Tempdb Metadata内存优化的 Tempdb 元数据\nPersistent Memory Support持久化内存支持\n\n\n\nIntelligent Query Processing使用智能查询处理，用户可以不改动应用层代码，只需将 T-SQL 跑在兼容等级 150 的 MSSQL 数据库上(这意味着想要充分使用这个特性，只能使用 2019 版本和云上 Azure, 2017 有部分功能)。MSSQL 的兼容等级见下表:\nIntelligent Query Processing主要包含如下特性：几个优化技术简介：\n\nAdaptive QP 自适应查询处理， 150 会使用这个技术自适应地选择 join,交错执行Interleaved execution,内存授予反馈Memory Grant Feedback.\nTable Variable Deferred Compilation表变量延迟编译， 这个技术会传输表的实际行数估计到后续的执行计划，用于优化执行计划和整体性能。\nBatch Mode on RowStore批量模式\nScalar UDF Inlining\nApproximate QP\n\n修改数据库的兼容等级：alter database tobereplaceddb set compatibility_level=150\nQuery Store 性能分析首先开启查询存储SET QUERY_STORE = ON (OPERATION_MODE = READ_WRITE), 使用 SSMS 从左边栏目可以看到资源消耗最大的几个查询：\n\n运行如下 SQL, 从右边的查询存储分析一下不同兼容等级的查询性能和执行计划。\nUSE WideWorldImportersGOCREATE or ALTER PROCEDURE [Sales].[CustomerProfits]ASBEGIN-- Declare the table variableDECLARE @ilines TABLE(       [InvoiceLineID] [int] NOT NULL primary key,        [InvoiceID] [int] NOT NULL,        [StockItemID] [int] NOT NULL,        [Description] [nvarchar](100) NOT NULL,        [PackageTypeID] [int] NOT NULL,        [Quantity] [int] NOT NULL,        [UnitPrice] [decimal](18, 2) NULL,        [TaxRate] [decimal](18, 3) NOT NULL,        [TaxAmount] [decimal](18, 2) NOT NULL,        [LineProfit] [decimal](18, 2) NOT NULL,        [ExtendedPrice] [decimal](18, 2) NOT NULL,        [LastEditedBy] [int] NOT NULL,        [LastEditedWhen] [datetime2](7) NOT NULL)-- Insert all the rows from InvoiceLines into the table variableINSERT INTO @ilines SELECT * FROM Sales.InvoiceLines-- Find my total profile by customerSELECT TOP 1 COUNT(i.CustomerID) as customer_count, SUM(il.LineProfit) as total_profitFROM Sales.Invoices iINNER JOIN @ilines ilON i.InvoiceID = il.InvoiceIDGROUP By i.CustomerIDENDGO-- Pull these pages into cache to make the comparison fair based on a warm buffer pool cacheSELECT COUNT(*) FROM Sales.InvoicesGO-- Step 2: Run the stored procedure under dbcompat = 130USE masterGOALTER DATABASE wideworldimporters SET compatibility_level = 130GOUSE WideWorldImportersGOSET NOCOUNT ONGOEXEC [Sales].[CustomerProfits]GO 25SET NOCOUNT OFFGO-- Step 3: Run the same code under dbcompat = 150USE masterGOALTER DATABASE wideworldimporters SET compatibility_level = 150GOUSE WideWorldImportersGOSET NOCOUNT ONGOEXEC [Sales].[CustomerProfits]GO 25SET NOCOUNT OFFGO-- Step 4: Restore dbcompat for WideWorldImportersUSE masterGOALTER DATABASE wideworldimporters SET compatibility_level = 130GO\n\n\n![](/images/2021-02/query_store_130.png)\n![](/images/2021-02/store_query_150.png)\n\n\n我们可以看到上面第一张图是 130 兼容性使用nested loop join, 虽然只用了一行 join, 但是性能不如 150 兼容性的Adaptive join自适应选择 join 方式（此例用了 hash join+clustered index scan); 同时用了Table Variable Deferred Compilation和Batch mode for rowstore技术。\nTempdbTempdb系统数据库可以作为共享资源用于临时表和临时表变量。 由于是共享的就存在竞争的问题，访问和修改 GAM(Global Allocation Map), SGAM(Shared Global Allocation Map)和 PFS(Page Free Space)可能存在竞争问题。存在系统表的临时表元信息也存在相同问题。MSSQL 采用page latch页锁的方式来物理上保护并发临时表的 page 访问, 但是有锁就意味着可能有等待。MSSQL2019 采用 tempdb 有如下优化：\n\n多文件 partition 先在物理上减轻 page 访问压力， 每个文件会以相同的增量同步增大\n临时表和临时变量会变缓存\nallocation page锁协议改善\n减少 tempdb 的日志文件 IO 开销\ntempdb 中所有的 allocation 使用同一的 extents\n对于主要fileground, AUTOGROW_ALL_FILES属性不能被修改，默认打开\n\n开启内存优化的tempdb\nSELECT SERVERPROPERTY(&#x27;IsTempdbMetadataMemoryOptimized&#x27;)ALTER SERVER CONFIGURATION SET MEMORY_OPTIMIZED TEMPDB_METADATA = ON;\n","categories":["database"],"tags":["database"]},{"title":"MongoDB的架构","url":"/2018/09/23/database-mongodb-2018-09-24-mongo-%E6%9E%B6%E6%9E%84/","content":"前言最近尝试用MongoDB替换Mysql， 由于Mysql的写能力限制， 32C128G的实例同时写入1M的数据CPU飙升， 更不用说如果是并发的情况。MongoDB通过sharding能够很好地解决写能力扩展的问题, 故作一篇网上大神博客的小结. MongoDB的架构大致如下图所示(v3.2):\n与RBDMS的主要区别MongoDB和RDBMS的主要区别在于：\n\nRBDMS的数据记录是平的， 而MongoDB的数据单元Document是可以嵌入的，比如一个CSV文件，一个字段一个值; 但是mongo可以一个字段可以存储多个值如:&#123;&#39;key&#39;: [v1,v2]&#125;, 通过不同组合还可以生成更复杂的数据结构。\nRBDMS所有的数据结构schema必须预定义， MongoDB不需要预定义, Document可以存储任何结构的数据。\nMongoDB是没有join查询操作的。 RBDMS数据库设计的核心讲究Normalization, 数据越结构化，冗余越少越好。 而MongoDB鼓励denormalized, 通过数据冗余做到join查询。\nMongoDB的数据一致性需要客户端来维护， mongo没有ACID的Isolation概念， 一个并发客户端可能读到另一个并发用户修改的数据。\n同样地, transaction事务也是不存在，（注意， 4.0这个版本支持ACID的事务）。MongoDB原子性操作只能做到document级别。而正是移除关系型数据库部分特性， mongo才能做到更好的扩展性和轻量级， 这两条正适合用于处理大数据。\n\n查询处理Mongo的Collection可以认为是关系型数据库的table， Document可以认为是关系型数据库的records. 并且不需要预创建数据库和collection, 如下是一些基本操作, \nIn [1]: import pymongoIn [2]: p = &#123;&#x27;first&#x27;:&quot;Dave&quot;, &#x27;lastname&#x27;: &quot;He&quot;&#125;In [3]: client = pymongo.MongoClient()In [4]: db = client.test_databaseIn [6]: db.person.insert_one(&#123;&#x27;first&#x27;:&#x27;NN&#x27;, &#x27;lastname&#x27;:&#x27;HE&#x27;&#125;)Out[6]: &lt;pymongo.results.InsertOneResult at 0x21f9710&gt;In [7]: db.person.find_one()Out[7]: &#123;u&#x27;_id&#x27;: ObjectId(&#x27;5ba858bc91046605e83667dd&#x27;), u&#x27;first&#x27;: u&#x27;Dave&#x27;, u&#x27;lastname&#x27;: u&#x27;He&#x27;&#125;\n可以创建索引加速查询, MongoDB的索引以Btree的数据结构存储, 所以支持范围查询. 由于document本身就是一棵树, 索引可以嵌套到document下层里的某个值. 也可以创建符合索引, 如db.person.ensureIndex(&#123;lastname:1, firstname:1&#125;). 索引也可以是单键多值array.\nIn [5]: db.person.index_information()Out[5]: &#123;u&#x27;_id_&#x27;: &#123;u&#x27;key&#x27;: [(u&#x27;_id&#x27;, 1)], u&#x27;ns&#x27;: u&#x27;test_database.person&#x27;, u&#x27;v&#x27;: 2&#125;&#125;\nmongo默认给_id创建索引, 也可以给一个字段建新的索引, 创建索引可以是前台线下模式或者后台在线模式, 如果是线下模式, 如果是多副本集,需要考虑这些副本索引做到滚动更新.\nIn [6]: r = db.person.create_index([(&#x27;lastname&#x27;, pymongo.ASCENDING)], unique=True)In [8]: db.person.index_information()Out[8]: &#123;u&#x27;_id_&#x27;: &#123;u&#x27;key&#x27;: [(u&#x27;_id&#x27;, 1)], u&#x27;ns&#x27;: u&#x27;test_database.person&#x27;, u&#x27;v&#x27;: 2&#125;, u&#x27;lastname_1&#x27;: &#123;u&#x27;key&#x27;: [(u&#x27;lastname&#x27;, 1)],  u&#x27;ns&#x27;: u&#x27;test_database.person&#x27;,  u&#x27;unique&#x27;: True,  u&#x27;v&#x27;: 2&#125;&#125;\n但开始查询时如果有多个查询条件, mongo总是先尝试使用单个最佳索引找到符合的数据集, 然后根据后续的其它条件迭代查询.那么如果一个collection的多索引是如何配合来加速查询的呢?当一个查询执行时, mongo会给每个索引创建一个执行计划, 每个索引轮流执行查询, 直到所有索引执行完查询, mongo记录下最快执行查询的索引,后续就会使用这个索引查询, 直到后续一定量的数据更新, 才会重新执行上述流程.针对执行计划每次查询只用一个索引的特点, 查询时注意查询条件, 并且创建主键以外的复合索引来加速查询至关重要! 同时也要注意索引的开小, 删除不必要的索引.\n存储模型MongoDB通过内存mapping文件把存储在磁盘上数据文件直接映射到内存byte array, 数据的访问逻辑是通过指针算法实现的. 每个collection存在单独的一个namespace文件(记录元信息)和多个可扩展(extent)数据文件.每个collection的数据被组织在extent文件中, 每个extent是一段连续的磁盘空间,使用双向链表连接extent. extent包含了多个document, 每一个document也会和其相邻的document相连, 实际数据就以Bson的格式存储在上图中的DocRecord. 一个extent会指向所包含的document链表的头和尾(当然extent和extent之间也是连起来的). \n任何修改数据会直接在原地进行. 为了预防数据修改后大小超过了原来记录的分配的空间, 整条记录会被移动到一个更大的区域(附带一些填充占位的字节, 这些填充的字节当作缓存空间用于将来可以用于放置更大的空间, 那么具体填充多少字节, 这就需要每个collection有一个统计值, 专门记录修改的统计数据), 原来空出来的空间将会被释放, 有一张表会记录free list的大小.基于上述移动空间的设计, 我们可以预见数据会变得片段化, 所以mongo需要周期性地运行compact命令, 会把数据移动到连续的空间内, 以提高IO性能.这个操作通常需要在线下进行, 副本集则需要滚动更新.\n索引是通过Btree实现的, 每个Btree的节点含有一个key数字和指向左边子节点的指针.\n数据的更新和事务更新一条记录是在原数据上直接做修改:\nIn [6]: db.person.update_one(&#123;&#x27;first&#x27;:&#x27;Dave&#x27;&#125;, &#123;&quot;$set&quot;:&#123;&quot;first&quot;:&#x27;David&#x27;&#125;&#125;)In [8]: db.person.find_one()Out[8]: &#123;u&#x27;_id&#x27;: ObjectId(&#x27;5ba858bc91046605e83667dd&#x27;), u&#x27;first&#x27;: u&#x27;David&#x27;, u&#x27;lastname&#x27;: u&#x27;He&#x27;&#125;\nmongo写的时候可以指定多种方式(policy), 以表示写是否成功了, 如数据已经在磁盘持久化了; 或者数据已经在多个副本集传递(可以通过getLastError拿到执行返回值). 另外, mongo是可以给副本集打tag, 所以可以基于tag制定写策略.\n由于所有从mongo读到的数据都是过去的快照, 读到的数据可能已经被其它客户修改, 所以如果有一致性要求, 每次修改数据前可以先读一次验证数据, 然后再做修改.mongo可以在修改前验证条件, 使用findAndModifyapi更新数据. 如果是pymongo那么update会帮做验证条件, 不需要额外做这一步.\nmongo没有事务的概念(4.0以前), 对每个document的操作都是原子性的, 但是如果多个document被修改, 原子性是不能保证的.所以客户端需要自行实现多文件更新原子性.一个常见的技术是: 首先创建一个独立的文档(叫做transaction), 把所有需要更新的文档连接起来; 然后把所有需要更新的文档反链到transaction; 接着如下图做二段式提交完成事务.\n复制模型Mongdb通过副本集(replica set)做到高可用和读查询的负载均衡(没有做到写负载均衡), 通过把数据复制到多个服务器做到冗余. 具体包括一个主要DB和多个二级DB. 为了数据一致性, 所有修改都会在主DB操作然后异步复制到副本. 副本集之间的节点通过心跳同步状态, 如果有一个节点失去心跳(挂了), 那么就会失去membership. 如果后面恢复了那么这个节点会重新加入集群, 通过和主DB通信,获取changelog恢复到最新的数据. 如果主DB的changelog没有覆盖这个节点挂掉到恢复的全部日志, 那么需要重新导入主DB的所有数据.如果主DB挂掉了, 那么集群将进行选举, 通过节点优先级,运行时间等条件从二级DB中选出新的主DB, 由于二级DB采用异步复制的形式, 这个新选出来的主DB可能不是最新的数据版本.\n客户端驱动库需要实现找到主DB的功能, 在开始连接数据库后, 客户端需要发出isMaster的指令从当前集群中找到主DB和所有副本, 然后客户端会把大部分请求发给主DB, 部分读请求发给二级DB; 这个isMaster的指令会周期性的发送, 以便同步当前集群信息. 所有挂掉的节点, 客户端会强制中断连接.\nMongoDB有一个特殊的二级DB叫Delayed Slave, 它是延迟一段时间同步主数据库. 主要用来恢复短期误删的的数据.在读取查询时, 可以指定从二级DB读取数据, 这样读到的数据可能不是最新的, 但是可以使用这个特性做到查询的负载均衡. 客户端可以ping二级DB, 挑选最快的节点查询数据.\n分片模型想要做到写负载均衡, 需要使用MongoDB的sharding. 在sharding配置中, 一个collection可以通过partition key被分割到多个chunks内(一组key的范围), 多个chunks被分配到不同的shards, 每个shards都是副本集(replica set). 通过sharding, mongo可以存储无限量的数据, 这点在大数据应用场景十分重要.在sharding模式下, 客户端需要连接到MongoS, 它是一个路由服务器把客户端的请求发送到合适的分片. 对于insert/delete/update修改操作请求, 如果包含partition key, 那么基于chunk/shard映射表信息(从config server得到,并且本地缓存), 路由服务器可以找到对应chunk的节点服务器. 如果是读查询, 路由服务器会检测partition key是否包含在查询的部分条件中, 如果是那么就可以找到对应的主或二级shard; 但是也有可能查询条件不含有partition key, 那么路由会把请求发给每个shard.如果查询需要排序, 同时partition key在排序的条件中, 那么路由会一次按partition key排序shards; 如果不包含partition key, 路由服务器会把请发给每个shard, 然后做merge-sort.另一方面, 路由服务器需要保障每个shards中的数据chunks大致一样多. 当不平衡的条件被检测到, 路由器会通知chunk较多的shard触发迁移工作. 这个迁移工作是线上进行的, 数据迁移时进行多次delta检查, 直到最后数据迁移完毕, 目标服务器会通知config server新的shard信息, 通知源服务发送StaleConfigException给路由服务器, 让路由器重新读取配置信息. 后面的某个时间点, 源服务器旧的shard数据将会被删除. 如果在迁移的过程中发生高频率的更新操作, 那么迁移会停止,路由服务器会选择新的chunk去迁移数据.\n","categories":["database"],"tags":["mongo"]},{"title":"Aliyun RDS for Mysql","url":"/2017/11/12/database-mysql-2017-11-13-Aliyun-Mysql/","content":"RDS文档小结默认部署主备架构且提供了容灾、备份、恢复、监控、迁移等方面的全套解决方案\n账号模式\nrds账户分为经典模式和高权限模式，5.7只有高权限模式。\n\n它们之间的区别是能不能直接通过sql创建账户，见下图.\n&lt;img src&#x3D;”http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/pic/26186/cn_zh/1510133360436/%E5%9C%A8%E4%B8%8D%E5%90%8C%E8%B4%A6%E5%8F%B7%E6%A8%A1%E5%BC%8F%E4%BD%BF%E7%94%A8%E5%AF%B9%E6%AF%94.png“ alt&#x3D;”Drawing” width&#x3D;”400)\n\n注意！！ 升级了高权限就不能回滚经典模式了！\n\n\nRds的一些使用限制\n不提供root或sa账户\n修改部分参数限制\n通过命令行和图形界面进行逻辑备份，控制台和api进行物理备份。还原亦然。\n使用命令行和图形界面进行逻辑导入，mysql命令行和数据迁移服务进行数据迁移。\n存储引擎：只支持InnoDb和TokuDB。\n默认主备复制双节点，无需手动搭建，用户不能访问slave。5.7只有基础版本，没有主备，支持原生Json。\n只能通过api和控制台进行重启。\n普通账户不可以自定义授权，5.7不支持创建普通账户权限。\n\n高可用服务\n阿里云的rds高可用采用Detection, Repair, Notice等模块组成， 主要保障数据链路服务的可用性。\nDetection: 通过HA判断主备节点是否正常，能够排除网络抖动，30秒完成异常切换操作。\nRepair: 维护主备节点的复制关系和修复。\nNotice: 负责通知HA主备节点的状态变化。 \n多可用区可以承受机房级别的故障，多可用采用半同步复制方案，响应时间可能比单可用长。\n高可用策略：\nrds有两个高可用策略，RTO(recovery time objective）和RPO(recovery point objective)\n三种复制方式：\n\n\n异步复制, 主库提交，响应应用，再向slave异步复制。\n强同步， 主备复制完成后返回响应。\n半同步， 正常主备强同步，当主向备复制发生问题时，主退化为异步复制。\n\n\n\n备份服务\n通过Backup, Recovery, Storage模块提供数据的离线备份，转储和恢复\n通过Backup备节点压缩数据到oss\n通过Recovery可以恢复备份文件到目标节点：  - 回滚主节点  - 修复备节点  - 创建只读实例\n通过Storage 复制备份文件的上传、转储，下载。\n\n从本地迁移数据到云上\n云上的数据库账号需要和本地一致\n支持DTC, FTP, mysqldump\n使用DTS迁移可以实现不停应用，平滑迁移。\nDTS支持结构迁移、全量迁移、增量迁移。迁移过程中有数据变更可以开启增量迁移。DTS迁移文档\n迁移过程中不支持DDL\n结构迁移不支持event迁移\n增量迁移本地需要开启binlog，binlog_format为row。如果本地 MySQL 为5.6版本时，它的 binlog_row_image还须设置为full。\n\n\nmysqldump doc\n\n容灾\nRDS 通过数据传输服务（DTS）实现主实例和异地灾备实例之间的实时同步。主实例和灾备实例均搭建主备高可用架构，当主实例所在区域发生突发性自然灾害等状况，主节点（Master）和备节点（Slave）均无法连接时，可将异地灾备实例切换为主实例，在应用端修改数据库链接地址后，即可快速恢复应用的业务访问。\n多收DTS的费用\n灾备实例不支持备份设置、备份恢复、数据迁移、数据库管理、申请外网访问地址、修改连接地址功能。\n\nmysql和oss搭配使用\n多类数据存储解决方案\n\n数据库性能测试\n使用测压SysBench0.5\n准备数据\n\nsysbench --num-threads=32 --max-time=3600 --max-requests=999999999 --test= oltp.lua --oltp-table-size=10000000--oltp-tables-count=64 --db-driver=mysql --mysql-table-engine=innodb--mysql-host= XXXX --mysql-port=3306 --mysql-user= XXXX --mysql-password= XXXX prepare\n\n压测性能\n\nsysbench --num-threads=32 --max-time=3600 --max-requests=999999999 --test= oltp.lua --oltp-table-size=10000000--oltp-tables-count=64 --db-driver=mysql --mysql-table-engine=innodb --mysql-host= XXXX --mysql-port=3306--mysql-user= XXXX --mysql-password= XXXX run\n\n清理环境\n\nsysbench --num-threads=32 --max-time=3600 --max-requests=999999999 --test= oltp.lua --oltp-table-size=10000000--oltp-tables-count=64 --db-driver=mysql --mysql-table-engine=innodb --mysql-host= XXXX --mysql-port=3306--mysql-user= XXXX --mysql-password= XXXX cleanup\nDMS在线数据库管理工具重启的坑！\n由于数据库重启时，rds会自动更新小版本，如果出现不兼容后果自负！所以重启前必须先购买一个新的实例进行兼容测试。\n\n数据压缩\n5.6使用TokuDB支持存储压缩数据\n\n主备切换\n主备切换会有闪断，应用需要重连。实际体验小于1分钟完成切换， 5.7没有备。\n\n数据复制方式\n复制方式包括： 强同步，半同步，异步\n强同步需要3台以上节点，一个事务包含同步完成大部分节点\n半同步在同步出现异常时，退化为异步同步。\n异步可能会引起数据不一致。\n\nCloudDBA监控服务\n实时性能监控\n会话诊断和终止\n分析慢sql\nCloudDBA仅适用于公共云华北1、华北2、华东1、华东2、华南1地域的MySQL 5.5和MySQL 5.6版本的实例监控的metrics:\nIOPS\nlatency\nthroughput\nqueue depth\n\nmysql5.6的读写分离\n读写分离和主实例、读实例的区别，后者单独有连接地址，业务逻辑选择进行连接。读写分离是一个统一的地址，程序自动进行读写分流。\n\n用户只需要购买读实例，可以免费试用读写分离\n&lt;img src&#x3D;”http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/pic/51073/cn_zh/1505467133787/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E5%9C%B0%E5%9D%80.png“ alt&#x3D;”Drawing” width&#x3D;”400)\n\n\n数据备份和恢复\n数据备份oss+日志oss总量&gt;实例空间50%时，将收费。\n误删数据该如何恢复？ 使用克隆实例按 按备份集 或 按时间点 两种方式复制出一个新的实例，进入克隆实例导出sql,再进入主实例导入sql。若数据较多可以使用DTS.\n不建议使用覆盖性恢复\n\n虚机自建mysql和RDS性能对比\n云数据库是可能比自建数据库慢的。见对比ECS自建数据库与RDS性能时的注意事项\n\n数据加密\nSSL\nTDE 指定参与加密的数据库或者表。这些数据库或者表中的数据在写入到任何设备（磁盘、SSD、PCIE 卡）或者服务（OSS、OAS）前都会进行加密.\n\n其它技术运维问题\n其他问题\n\n影响数据库存储性能的方面\n计算实例的配置\nIO的性能\n负载需求\n\n","categories":["database"],"tags":["mysql"]},{"title":"InnoDB for Mysql 5.6","url":"/2017/12/05/database-mysql-2017-12-06-InnoDB/","content":"Mysql architecture\nInnoDB High Level Overview\n\nwhat is in the buffer pool\nindex page \ndata page\nredo page\ninsert buffer\nadaptive hash index\nlock info\ndata dictionary\n\n\nthe default page size is 16KB\ninnodb_buffer_pool_size and innodb_buffer_pool_instances\nselect pool_id, pool_size, free_buffers, database_pages from information_schema.innodb_buffer_pool_stats; #show buffer pool stats.\ninnodb_buffer_pool_instances divides the buffer pool into specified number of separate regions. each with its own LRU list and data structures. The advantage is to reduce contention during concurrent memeory.\nbuffer instance param will only take effect when innodb_buffer_pool_size is larger than 1GB, and recommend use multiple Gigebytes\n\n\nfrom memory point of view, innodb has three types of page: clean page, dirty page and free page.\nclean page, data in memory and in disk is identical\ndirty page, not identical\nfree page, not used page\n\n\ninnodb logically has data page type:\n\nselect distinct page_type from information_schema.innodb_buffer_page_lru where;-------page_type-------------SYSTEMINODEIBUF_INDEXINDEXIBUF_BITMAPTRX_SYSTEMUNDO_LOGFILE_SPACE_HEADERBLOB\n\nLRU list, Free list and Flush list**\n\ninnodb use list to manage different pages.\nLRU list saves already read page. Use midpoint tech to save lastest read page into LRU list. \nfree list has all not used free page, when server starts, all the page are in free list, and move to LRU.\nflush list contains all the dirty page. Use checkpoint to flush dirty data into disk. Dirty page may exist both in flush list and LRU list.\ninnodb_old_blocks_pct and innodb_old_blocks_time\n\nvariables like '%innodb_old_block%'``` # innodb_old_blocks_pct sets percertage of old list, innodb_old_blocks_time sets the midpoint page will be in the hot part of LRU list after certain time.- ```show engine innodb status;``` #show the page info ``` bash----------------------BUFFER POOL AND MEMORY----------------------Total large memory allocated 2198863872Dictionary memory allocated 776332Buffer pool size   131072Free buffers       124908Database pages     5720Old database pages 2071Modified db pages  910Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 4, not young 00.10 youngs/s, 0.00 non-youngs/sPages read 197, created 5523, written 50600.00 reads/s, 190.89 creates/s, 244.94 writes/sBuffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/sLRU len: 5720, unzip_LRU len: 0 I/O sum[0]:cur[0], unzip sum[0]:cur[0]\n\n\nselect * from information_schema.innodb_buffer_pool_stats # can also show the buffer pool stats.\nbuffer poll hit rate is the rate hit in memory not in disk, the higher the better\nselect * from information_schema.innodb_buffer_page_lru where compressed_size &lt;&gt; 0 # compressed lru page\n\n\nredo log buffer\n show variables like &#x27;innodb_log_buffer_size&#x27;;innodb_log_buffer_size\t16777216\nthree conditions that flush data into disk\n\nevery 1s master thread trigger\nevery transaction\nredo buffer free page less than 1&#x2F;2\n\n\nDirty page checkout mechanization\n\nwrite-ahead-log write to log before write page\n\nthe problem to solve:\n\nreduce database recover time\nflush data when buffer pool is full\nrefresh dirty page when redo log is out of work\n\n\nInsert buffer\n\nprimary key is the unique identification of the row, usually application insert rows based on auto-increased primary key.  if we insert primary key that is random, we may not get sequence data in disk\n\n\n","categories":["database"],"tags":["mysql"]},{"title":"高性能mysql笔记","url":"/2017/11/14/database-mysql-2017-11-15-high-performance-mysql/","content":"sql执行大致流程\n客户端发送一条查询给服务器。\n服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。\n服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划。\nMySQL根据优化器生成的执行计划，再调用存储引擎的API来执行查询。\n将结果返回给客户端。\n\nmysql的架构\nmysql采用存储和计算分离的架构, 是一个典型的单进程多线程模型数据库.\nmysql可以开启线程池，处理客户端的请求。\nselect语句会在缓存中先查找，没有hit才会到解释器。\n解释器创建内部数据结构并做优化，最后才到存储器API。优化决策时用户可以使用hint关键字来影响mysql的决策过程，也可能使用explain来看看mysql是怎么决策的。\n优化器会查询存储引擎提供一些信息来帮助优化。\n\n启动一个mysql实例一般命令行启动mysql需要指定basedir, datadir, user, log-error, pid-file, socket等参数, basedir下面放置项目二进制可执行文件和库文件,pid文件和socket等文件. datadir目录下放置server log文件, innodb相关文件, 数据文件如.ibd数据和索引文件, .frm对象结构文件等.\n/usr/local/mysql/libexec/mysqld --basedir=/usr/local/mysql --datadir=/usr/local/mysql/var --user=mysql --log-error=error.log \\--pid-file=/usr/local/mysql/var/mysql.pid --socket=/tmp/mysql.sock --port=3306\n\nmysql的并发控制\nmysql在服务层和存储层都做了并发控制， 一般使用锁来控制并发。\n存储引擎有自己的锁策略和颗粒度，但是服务层的策略高于存储层。比如alter table 使用应用层的表锁，忽略存储层锁机制。\n行锁只有在存储层实现, 一般通过多版本并发控制MVCC, 提升并发性能。简单原理是一个事务看到的是某一时刻数据的备份。\n可以显式地加锁：\nselect ... lock in share mode;\nselect ... for update;\n\n\n\nMVCC(Multi-Version concurrent control)\n实现机制： 基于某个时间点的快照。非阻塞度，行锁写\nInnoDB在每个行记录后面保存两个隐藏的列(时间上根据mysql版本不同，有更多的隐藏列, 5.7是3个字段）， 一个行的创建时间，一个是过期时间。时间都是系统版本号。\n在repeartable read隔离等级下， mvcc的操作:\nselect:    只会查到当前事务版本号前面或等于当前版本号（事务修改过）的行。    删除的行也会作比较。\ninsert:    插入一条数据，加2个隐藏的列\ndelete:    删除一行，在删除列加入当前系统版本\nupdate:    插入一条新的记录，把老的记录删除行更新系统版本。\n\n\n\nInnoDB\n它是被设计为处理大量短时事务。\n使用next-key locking实现防止幻读。\n\n事务只有在存储引擎实现\n需要格外小心事务中混合了事务型和非事务型表\n\nACID是一个事务的标准特征\nmysql主要关心两个隔离等级，Read committed和repeatable read。 read committed也是nonrepeatable read.repeatable read是mysql默认隔离等级，保证同一个事务多次读取同样记录结果一致。\nInnoDB使用不同locking strategy来实现不同隔离等级.\n在多个事务里可能出现死锁现象, innoDB处理的方式是将最少行级排它锁进行回滚。\n\n隔离等级下的幻读和脏读假设有一张表: t_1(id primary key, name) 有三条数据.幻读现象会在这种情况出现, A事务先执行:select * from t_1 where id=4然后B事务执行insert into t_1 values (4, &#39;Hanny&#39;)如果此时隔离等级是read committed,那么事务A再执行select * from t_1 where id=4就会读到这条数据, 出现所谓的幻读. 如果此时的隔离等级是repeatable read那么A事务再执行select将不会读到id=4的数据.脏读现象出现在A事务读取了B事务未提交的更新, 如A事务start transaction; insert into t_1 values (4, &#39;hanny&#39;), B事务select * from t_1 where id=4读到了数据, 就是脏读. 这种情况一般出现在隔离等级在read uncommitted的时候.\n和隔离等级密切相关的各种锁(V5.7)\nrecord lock记录锁, select ... lock in share mode/select ... for update会加记录锁, 锁定的这行不能被另一个事务做insert, update, delete. 记录锁会锁定有索引的记录, 如果表没有定义索引, innodb会隐含创建hidden clustered index. 在RR隔离等级下, select ... lock in share mode/select ... for update加上where唯一索引作为唯一查询条件, 可以实现RR.\n\n\n使用事务日志可以提高存储引擎修改表数据效率\n做法类似redis, 仅仅持久化事务日志，在内存中更新数据，然后后台再慢慢写入磁盘。\n\nMysql提供两种事务性存储引擎InnoDB, NBD cluster\n自动提交auto-commit： 默认开启，即便不是显式开启，每个查询还是会默认进行事务。\n\nMysql的复制方式\n主库记录二进制日志，备库将主库日志复制到中继日志（relay log)后，重放二进制日志。同一时间点主备数据可能不一致。\n\n设计mysql备份方案\n逻辑备份恢复太慢，采用ExtraBackup快照备份是物理备份较好的选择。\n\n保留多个备份集\n\n定期恢复\n\nexpire_logs_bin 设置足够长，保留二进制日志文件用于基于时间点的恢复。\n\n监控和检查备份是否正常？监控恢复需要耗费多少资源和时间？\n\n选择在线备份，可能是导致mysql服务中断\n\n逻辑备份导出的文件要么是sql要么是类似csv的文本；物理备份就是直接复制原始文件。\n\n逻辑备份的优点：\n\n\n逻辑备份可以消除底层存储引擎的影响  2. 如果内存保存着正确数据但是磁盘坏了，不能复制一个正确的物理备份， 仍可能导出一个正常的逻辑备份。\n\n\n逻辑备份的缺点：\n\n\n需要消耗cpu，恢复时间较长，需要建index等。\nASCII形式的数据可能比原始数据大\n恢复时可能由于bug或者浮点表示问题，无法保证还原一模一样的数据。\n\n\n物理备份的优点：\n\n\n恢复快速， 不需要执行任何sql或构建索引。\n\n\n物理备份的缺点：\n\n\n原始文件比逻辑备份大\n可能不总是夸平台\n\n\n使用check tables或mysqlcheck 检查恢复操作。\n\n\n推荐的备份方案\n先一周使用一次使用物理备份，启动mysql实例，运行mysqlcheck, 然后在服务器负载低时周期性地mysqldump执行逻辑备份，30分钟备份一次bin-log,热备份完flush logs。\n\n需要备份什么？\n二进制文件, InnoDB事务日志\n代码，如存储过程\n配置\n服务器配置\n操作系统配置\n\n增量备份存在中间增量出错，导致整个备份不可用的风险备份中如果要保持数据一致性\n使用InnoDB，能够保证一个事务内数据一致备份到另处。但是如果应用逻辑写的不对，导致本应该是一个事务到了两个事务，备份在两个事务之中可能数据不一致。\nmysqldump –single-transaction 在InnoBD开始dump开启事务，隔离等级必须是repeatable read. 但是dump时不能执行ALTER TABLE, CREATE TABLE, DROP TABLE, RENAME TABLE, TRUNCATE TABLE。To dump large tables, combine the –single-transaction option with the –quick option.\n\n使用LVM镜像做mysql备份的基本思路\n获取读锁\n将缓存中的数据写到磁盘\n建立快照\n释放读锁\n\nLVM CoW原理\n给一个卷打一个快照，只记录元信息，当源卷发生写，把需要改变的那部分数据在未改变前复制到快照。这样，读取快照时，既能保证拍照时的数据一致，又能省时间性能。\nLVM快照的一些限制：\n所有文件必须在同一个逻辑卷（分区）\n需要有足够空间\n\n\n\nMysql的索引\n索引在mysql也叫key， mysql有单列索引和多列索引。多列索引根据左序排列。\n索引的类型：\nB-Tree索引， MyISAM使用压缩的索引指向数据的物理位置；innodb使用索引指向数据的主键。\n\n\n一个典型的B+树索引![](.&#x2F;images&#x2F;B+tree.png” width&#x3D;”450” height&#x3D;”450”&gt;\n对于下面这个多列索引：索引根据建表时指定key的值，按大小排序\n\ncreate table people (last_name VARCHAR(50), not NULL,first_name VARCHAR (50), not NULL ,dob DATE , not NULL ,gender enum(&#x27;m&#x27;, &#x27;f&#x27;) not NULL ,key(last_name, first_name, dob));\n![](.&#x2F;images&#x2F;multi-col-index.png” width&#x3D;”450” height&#x3D;”450”&gt;\n\n对于多列索引，B+tree适合的查询方式有：\n完全匹配\n最左列完全匹配，仅适用于第一个column,即last_name\n键值范围, 仅适用于第一个column，即last_name值的范围\n键值前缀, 仅适用于第一个column，即last_name begin with\n完全匹配一个列，范围匹配另一个列\n仅仅查询索引\n\n\n\n","categories":["database"],"tags":["mysql"]},{"title":"A look at InnoDB for Mysql 5.7","url":"/2017/12/11/database-mysql-2017-12-12-InnoDB5-7/","content":"InnoDB的优势\nDML 遵循ACID\n行级锁，使用MVCC保证读一致\n使用主键索引管理磁盘上的数据，加快查找速度\n使用外键维持数据的完整性\n\n\nInnoDB表 最佳实践\n主键的设定：把查询用的最多的column， 并且不怎么改动的设成主键；如果没有可以设置自增列为主键。\n如果是基于一个列进行多表联查，使用join. 并且把join的列设置成外键，数据类型也要一致。这么做是因为外键会给列加索引，外键也会把删除和更新传到相关表。\n关闭autocommit(可能是log flush的影响，待查)\n把一系列的DML语句放在一个事务中。\n不要使用lock tables语句。可以使用select .. for update 只针对某些row锁定更新。\n开启innodb_file_per_table， 这样表的数据和索引是单独文件存在。\n根据需求，可以使用ROW_FORMAT=COMPRESSED来压缩数据。\n\nInnoDB 和 ACID 模型\n使用innodb, mysql将十分接近ACID模型，避免软硬件的失效带来的灾难。\nACID就不说了\n\nMVCC\ninnodb使用rollback segment的数据结构来管理版本信息。主要服务回退和一致性读。\ninnodb 会在每行后面加三个隐藏字段\n6B DB_TRX_ID 最新事务ID. 这个字段里面有个位标记行是否被删除\n7B DB_ROLL_PTR roll pointer 指向rollback segment的undo log记录。 更新一条记录时，undo log 会记录如何回滚这个更新的记录。\n6B DB_ROW_ID 行ID, 递增。如果聚簇索引是innodb自动生成的，那么索引里包含row id. 否则DB_ROW_ID不会再任何索引里出现。\n\n\n在rollback segment中的undo log 分为insert和update undo logs. insert undo log 在事务回滚时就不需要了，但是update undo log需要在MVVC保持consistent read后，才能丢弃。    \n在innodb, delete sql不会立即删除物理数据；仅在update undo log被purge的时候才会删除行数据和对应的索引\npurge是一个定时垃圾处理undo log的后台任务，可以有一个或多个线程。purge是滞后操作的，所以批量删除数据太多，可能引起rollback segment过大，这时可以增加线程innodb_max_purge_lag\n多版本和二级索引\n\n","categories":["database"],"tags":["mysql"]},{"title":"InnoDB Buffer Pool for Mysql 5.7","url":"/2017/12/15/database-mysql-2017-12-16-innodb-5-7-buffer-pool/","content":"缓存池\n内存池用page来分割，pages会连接成链表，方便管理。\n内存池最小单位是page， 一个page默认是16KB. 一个page可以有一行以上数据， 如果只有一行，而且没有填满page，innodb会用指针类型的数据结构填满page。可以压缩数据让page包含更多行数据，对于Blob或者text类型的大column, 使用压缩技术innodb会单独存储这一列的数据，以便减少不必要的查询开销。\n一般缓存池最多会占80%的物理内存，InnoDB使用变异的LRU算法，维持缓存中数据的热度。\nselect * from information_schema.innodb_buffer_page 可以查看所有缓冲池的元信息，这个查询可能影响性能。\n\nchange buffer\nchange buffer 是一个特殊的数据结构，用于缓存那些辅助索引页的修改，而这些页又不在buffer pool中。由于辅助索引非唯一，insert,delete,update更改辅助索引页并不是按序进行的。所以需要先把他们缓存起来，当辅助索引读入buffer的时候，一并merge到buffer,这样减少了随机访问磁盘I&#x2F;O.\nchange buffer 包含insert buffer, delete buffer, purge buffer\nchange buffer 既是缓存，又是tablespace的一部分。\nshow variables like innodb_change_buffering 这个变量可以设置inserts, deletes, purges,all,none.\nnone    Do not buffer any operations.\ninserts    Buffer insert operations.\ndeletes    Buffer delete marking operations; strictly speaking, the writes that mark           index records for later deletion during a purge operation.\nchanges    Buffer inserts and delete-marking operations.\npurges    Buffer the physical deletion operations that happen in the background.\nall    The default. Buffer inserts, delete-marking operations, and purges.\n- 下面这个状态可以看到具体change buffer操作次数，insert表示insert buffer, delete mark表示delete buffer, delete表示purge buffer;    ```sql  show engine innodb status;  -------------------------------------  INSERT BUFFER AND ADAPTIVE HASH INDEX  -------------------------------------  Ibuf: size 1, free list len 0, seg size 2, 0 merges  merged operations:   insert 0, delete mark 0, delete 0  discarded operations:   insert 0, delete mark 0, delete 0  Hash table size 34673, node heap has 0 buffer(s)  Hash table size 34673, node heap has 0 buffer(s)  Hash table size 34673, node heap has 0 buffer(s)  Hash table size 34673, node heap has 0 buffer(s)  Hash table size 34673, node heap has 1 buffer(s)  Hash table size 34673, node heap has 0 buffer(s)  Hash table size 34673, node heap has 0 buffer(s)  Hash table size 34673, node heap has 0 buffer(s)  0.00 hash searches/s, 0.00 non-hash searches/s\n\n\n\n\n查看IBUF_INDEX和IBUF_BITMAP占全部page的比例  SELECT (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGEWHERE PAGE_TYPE LIKE &#x27;IBUF%&#x27;) AS change_buffer_pages,(SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE) AS total_pages,(SELECT ((change_buffer_pages/total_pages)*100))AS change_buffer_page_percentage;\nchange_buffer_pages | total_pages | change_buffer_page_percentage|--- | --- | ---21 | 8191 | 0.2564 |\n\n","categories":["database"],"tags":["mysql"]},{"title":"MySQL 5.7 性能调优","url":"/2018/06/26/database-mysql-2018-06-27-mysql5-7%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","content":"前言最近在使用new relic监控发现有个sql update花了30s, 于是开启了数据库优化的路径…\n优化原则数据库性能优化有两个层面: 数据库本身层面和硬件层面. 两个层面的优化原则各不相同\n\n数据库本身优化原则:\n\n\n表结构是否合理? 更新较多的应用表设计时需要设置多表但是较少的列. 查询较多的应用应该设置较少的表但是列较多.\n索引是否设置合理?\n存储引擎是否选择合理\n列的类型是否选择合理, 较少的磁盘需求意味着较少的I&#x2F;O.\n并发控制的锁策略是否合理?\n用于缓存的内存是否设置合理?\n\n\n硬件优化原则:\n\nSQL语句优化优化SQL时主要需要考虑的问题有:\n\n是否在where语句后面的列是否加了索引, mysql5.7是会给外键自动加索引的,但是在使用join或外键查询时还是最好explain一下,看看执行计划.\n分步调试查询的每一部分.\n对于大表, 尽量减少全表查询\nANALYZE TABLE, 尽量保持statistics table统计表更新, 让优化器使用这些统计信息去构建更高效的执行计划.\n需要学习调优技巧, 如索引技巧, 各种存储引擎的参数配置.\n\n开启慢sql查询日志mysql可以开启慢sql查询的log, 只要sql执行超过long_query_time时间, 同时影响超过min_examined_row_limit行数, 这个慢sql就会被记录. \nMySQL [(none)]&gt; show variables like &#x27;long_query_time&#x27;;+-----------------+-----------+| Variable_name   | Value     |+-----------------+-----------+| long_query_time | 10.000000 |  //对于文件精细度是毫秒, 对于表精细度是秒+-----------------+-----------+MySQL [(none)]&gt; show variables like &#x27;min_examined_row_limit&#x27;;+------------------------+-------+| Variable_name          | Value |+------------------------+-------+| min_examined_row_limit | 0     |+------------------------+-------+\n默认情况下, admin的管理查询语句是不会被记录在log, 但是可以被log_slow_admin_statements和log_queries_not_using_indexes修改配置.慢log记录的执行时间是从获取锁的时间开始了, 直到运行完语句然后释放所有锁.所以获取锁的执行时间是不会被记录.默认情况下, 慢查询log会记录所有使用非索引查询的慢语句, 这可能会导致log变得很大, 所以可以设置log_queries_not_using_indexes忽略这些使用非索引查询的语句. 或者使用log_throttle_queries_not_using_indexes来限制每分钟记录多少条非索引慢查询记录. mysql server使用如下控制参数顺序来控制一个查询语句是否写入慢查询log:\n\n是否是管理sql语句, 或者开启了log_slow_admin_statements\nlong_query_time必须被满足,或者log_queries_not_using_indexes被使能, 非索引查询语句也会被写入log\n满足min_examined_row_limit行数\n那些满足log_throttle_queries_not_using_indexes的查询log_timestamps控制了写入慢查询log文件的时区, 但是不会影响通用查询log和写入数据库的log.在cache中满足慢语句的sql是不会写入log, 默认情况下复制的从机也不会记录log.收集完log后, 可以使用mysqldumpslow来查看.所以为了开启慢查询log, 在my.cnf设置如下代码:\n\n[mysqld]slow_query_log                  = 1slow_query_log_file             = /var/log/mysql/slow.loglong_query_time                 = 5\n\n增加最大客户端连接数max_connections指定允许的同时连接客户端, 默认是151(实际最大连接数会+1, 多出来的是给admin使用). 改变这个值会影响mysqld的文件描述符的需求. 如果所需的fd不能满足,server会自动减少这个值.所以一味增大连接数但是不改变table_open_cache可能不能提升性能, 同时还需要确保table_open_cache的值能被操作系统满足.关于mysql如何打开和关闭表 为了提升性能, 每一个client session会单独打开一个table. table_open_cache和max_connections系统变量决定了最大数量文件可以被打开. 比如如果有200个并发连接, 每个连接最大的表join是N, 那么起码应该设置table_open_cache为200*N + 额外的一定数量. 那么什么时候mysql会关闭不使用的表呢? 会有如下情况表会被关闭:\n\ncache满了, 新的线程想要打开新的table但是不在cache中\n当前cache中条目超过了table_open_cache设置, 不再被线程使用table将会被关闭.\n使用flush的命名. 可以是flush table&#x2F;mysqladmin flushtables&#x2F;mysqladmin refresh.如果cache满了, mysql怎么分配的table cache呢? 这时候会分配临时cache, 具体是怎么做的还未知.查询当前表打开的状态, open_tables和opened_tables, 如果opened_tables &gt; table_open_cache应该考虑增大table_open_cache.\n\nMySQL [(none)]&gt; show global status like &#x27;open%tables%&#x27;;+---------------+-------+| Variable_name | Value |+---------------+-------+| Open_tables   | 1352  || Opened_tables | 1457  |+---------------+-------+MySQL [(none)]&gt; show variables like &#x27;table_open_cache&#x27;;+------------------+-------+| Variable_name    | Value |+------------------+-------+| table_open_cache | 2000  |  +------------------+-------+\n\n增加线程缓存mysql可以缓存线程, 用于下次用户连接. thread_cache_size默认是-1, 通过8 + (max_connections / 100)计算出. 如果threads_created&#x2F;connections过大, 可以考虑增大thread_cache_size.\n通用和结构化系统变量mysql支持一种结构化的变量类型, 用于控制key cache的参数. 结构化的系统变量有两个特点:\n\n它的值由一些系统的参数组成\n每一个结构化变量类型都可能有多个实例, 每个实例有不同的名称和不同的资源.一个结构化的key cache变量有如下组件组成, 这些参数可能和MyISAM密切相关:\nkey_buffer_size\nkey_cache_block_size\nkey_cache_division_limit\nkey_cache_age_threshold\n\n配置Innodb_buffer_pool_size","categories":["database"],"tags":["mysql"]},{"title":"Graph Thinking","url":"/2021/03/07/database-neo4j-2021-03-08-graph-thinking/","content":"When to start use graph?当我们需要使用图数据库时, 需要考虑三个问题:\n\n图数据库是否比关系型数据库更优?\n如何把我的数据看做图?\n如何给图的原型结构建模?\n\n关系vs图形从存储和数据检索的角度看, 关系型数据库将数据以三范式的形式存储, 将真实世界的实体与表映射; 而图数据库关注存储和检索的关系, 这些关系代表了真实世界实体的连接, 比如: 一个人认识另一个人, 人住在什么地方, 人拥有东西等等. 其实两个系统都可以表示实体和关系, 但是关注点和优化点不同. 他们底层的数学算法也不同, 一个是关系型代数, 一个是图论.\n图论的思想?图是由节点(Vertex)和边(Edge)组成的，Vertex代表数据实体，Edge代表实体之间的关系, edge是有方向的(单项与双向)。如果把节点当作名词，边当作动词，那么它们分别都有形容词和副词来修饰它们，即属性。\n\nAdjacency: 如果两个节点通过边相连，那么它们是临近的。\nNeighborhood: 对于节点v, 所有与之相连的节点形成节点v的邻居， 称N(v).\nDistance: 从一个节点走到另一个节点所需的边数称为距离。\nDegree: 程度描述了一个节点链接的边数。图论中，只有程度为1的节点成为叶。如果有很高的程度（&gt;100000edges)称为超级节点。\nMultiplicity: 描述节点间有不同属性的edges.\n\n为什么不直接用关系型数据库?直接回答：当需要突出数据的关系时使用图数据库, 尤其数据的结构复杂时。图数据建模和关系型数据建模很相似，主要的区别在于考虑实体之间的关系。图技术突出了关系优先的数据结构。在物理存储方面，图数据库直接存储概念数据模型(Conceptual data model), 不像关系型数据库做三范式处理。具体处理上，图数据库把节点的边直接组成数据结构存储在一起，这样可以方便地寻址某个节点的关系。当新增一个需求时, 关系型数据库往往需要改变数据结构, 增加表, 新增sql语句. 而图数据库只需要增大原型(schema)和插入数据.\n","categories":["database"],"tags":["graph database"]},{"title":"SQL Server的主数据服务MDS简介","url":"/2021/01/28/database-SQL-Server-2021-01-29-SQL-Server%E7%9A%84MDS/","content":"关于主数据管理系统MDM(Master Data Management)是从企业各个数据源采集数据，使用统一的标准和业务流程构建单一数据视图，并且将主数据分发, 作为企业内部其他系统依赖的数据金标准。\n主数据系统一般需要收集如下信息：\n\n人(客户，供应商，雇员，患者等)\n物(产品，业务单元，部件，装备等)\n地点(地址，仓储，地理区域等)\n抽象(账户，合同，时间等)\n\nMDM 一般的做法流程是：\n\n从不同的数据源导入到 staging 数据库\n把 staged 的数据和领域属性映射，再做标准化和归一化，清洗，应用业务规则，富集数据，最后打上版本\n通过 API 把数据发布给相关用户\n\n另外，主数据的修改需要记录， 方便审计。\n主数据管理系统一般具有的功能\nDomain. 能够把主数据和领域分开和关联。\nRepository/Entity. 一个仓库或实体定义主数据的结构。\nAttributes. 仓库的属性\nAttribute Groups. 基于业务领域，有方法可以把相似类型的属性分组在一起。\nBusiness Processes. 一个好的业务流程可以方便治理数据\nBusiness Rules. 业务规则是限制某个属性取值的范围\nPermissions. 鉴权授权，典型 MDM 的角色有：Data stewards(数据管家)，approvers(审批人)，requesters(发起人)\nUI.\nWeb Services.\nData Publish. 推送的机制把数据送到订阅的用户。\nData Quality.\n\n基于上述特点，典型的 MDM 架构应该是：\nMaster Data Services 简介基于 SQLServer 的 MDS 可以用来管理组织的主数据，使用WCF(Windows Communication Foundation)提供 SOA 接口，并且可以通过 Excel 分享这些信息。\n在 MDS 中，Model 是主数据结构的最高级别容器。你可以创建 model 来管理相似数据组，比如管理所有线上产品。一个 Model 可以是一个 Entity 或者多个 Entities. 比如一个产品 model 包含产品，颜色和风格， 颜色 entity 包含所有的颜色。Entity 可以有两种属性：free-form和domain-based属性，free-form可以直接用于描述 entity，domain-based需要通过一个 domain 的 entity 来表现(就是 entity 的一个属性是另一个 entity)。\nMDS 是一个典型的三级架构：数据库层、服务层、WEB&#x2F;Add-in 层。MDS 一般要和 DQS 和 SSIS 结合使用，用于数据的集成和ETL。\nMDS 的组件\nConfiguration Manager, 配置管理工具用于配置数据库和 web 应用。\nMaster Data Manager, Web 应用用于管理任务，配置接口和视图。\nMDSModelDeploy.exe, 部署工具。\nMDS Web Service, SOA.可以在这使用代码来配置Master Data Manager.\nAdd-in for Excel, excel 插件。\n\nMDS 使用流程创建一个 model –&gt; 创建多个 entities –&gt; 创建domain-based entities –&gt; 创建free-form entities –&gt; 创建属性组 –&gt; 导入 entities 数据 –&gt; 使用业务逻辑确保数据质量 –&gt; 创建层级结构 –&gt; 创建显式层级如需 –&gt; 把组聚为集合如需 –&gt; 创建自定义元数据 –&gt; 给 model 打个版本号 –&gt; 创建订阅视图 –&gt; 配置权限\n","categories":["database"],"tags":["database, Master data"]},{"title":"Mysql 索引总结","url":"/2019/06/29/database-mysql-2019-06-29-mysql-index/","content":"mysql索引的作用和意义当我们使用sql语句查询时往往要加where, 使用索引我们可以快速查找到满足where条件的行. \nmysql如何使用索引mysql大部分索引使用B-tree, 例如(PRIMARY KEY, UNIQUE, INDEX, FULLTEXT); 空间数据类型使用R-tree; 内存表还支持hash索引, InnoDB使用反向列表(inverted list)作为FULLTEXT的索引.\nmysql会在做如下操作时用到索引:\n\n使用索引快速找到满足where条件语句的行.\n如果在查找时有多个索引选择, mysql使用那个能找到最少数据行的索引.\n如果使用多列索引, 最左边的列将被用于优化.\n在执行join其它表的时候\n计算有索引的列min(),max()时\n当使用最左边的索引排序和分组表时\n\n如何使用主键索引作为查询条件使用最多的列可以设置为主键(Primary key). 主键有附属索引, 用于提升查询性能.主键是不能为空的,所以性能也比较好. 如果表比较大, 同时又不知道选哪个列作为索引时, 可以创建一个自增的列作为索引. 用这个列作为其他表的外键方便join.\n如何使用外键优化表查询有时候如果有一个张大表, 可以把这张大表分成若干小表, 把不太常用的字段放在一起, 通过外键和主表关联, 这样子表既有主键用于快速查询又可以做join操作. 查询也可能使用更少的内存和IO, 因为相应的数据列都已经物理上在一起了.\n普通列索引最常见的索引是单列索引, mysql把列的值复制一份在数据结构, 用于快速查询.大部分数据结构采用B-tree, 可以快速定位到单个值, 一组值或值的范围. 在sql where语句对应=, &gt;, &lt;=, BETWEEN, IN等等.\n每个存储引擎的定义了每张表的索引最大值和最大长度. 基本上, 所有存储引擎至少支持16个索引和单个索引256字节以上.\nTEXT&#x2F;BLOB的索引Index Prefixes指在文本类型上创建索引时, 可以指定这个字段的开头一部分N个字节作为索引, 这样索引的长度将被限制, 特别适用于TEXT&#x2F;BLOB这样没有长度的字段上.\ncreate table test (blob_col BLOB, index(blob_col(10)))\n如果一个查询超出index prefixes的长度, 超出的部分将会被排除.\n多列索引mysql可以在多个列上创建索引. 一个索引最多可以有16个列组成. 使用多列索引的好处是你只需要创建一个索引,就可以享受到多个查询条件快速命中的优点.使用多列作为索引查询时, mysql可以检查索引中的所有列, 也可以只检查第一个列, 头二个列, 或头三个列.所以正确的定义复合索引的顺序可以加速好几种情况的查询.一个多列索引可以被当成是一个排序的数组, 索引的值是多个列的值CONCAT后的hash, 等同于没有多列索引时, 自己创建一个hash几个列值的字段, 这样就可以快速查询:\nselect * from table where hash_col=MD5(CONCAT(val1, val2))and col1=val1 and col2=val2;\n\n假设有如下表:\ncreate table test (  id int not null,  last_name char(30) not null,  first_name char(30) not null,  primary key(id),  index name (last_name, first_name));\nname索引是last_name和first_name的组合索引. 所以各种last_name和first_name的组合查询都可以使用到索引. 但是只有last_name可以作为单独查询条件, 因为只有最左边的索引值才能被优化器(optimizer)使用. 索引如下都是会使用到索引:\nselect * from test where last_name = &#x27;Widenius&#x27;;select * from test where last_name = &#x27;Widenius&#x27; and first_name=&#x27;Micheal&#x27;;select * from test where last_name = &#x27;Widenius&#x27; and first_name &gt;= &#x27;M&#x27; and first_name &lt; &#x27;N&#x27;;select * from test where last_name = &#x27;Widenius&#x27; and (first_name = &#x27;Micheal&#x27; or first_name=&#x27;Monty&#x27;);\n但是如下查询是不会使用到name索引的:\nselect * from test where first_name=&#x27;Micheal&#x27;;select * from test where last_name=&#x27;Widenius&#x27; or first_name = &#x27;Micheal&#x27;;\n\n单列索引和多列索引的区别假设有如下查询:\nselect * from table_name where col1=val1 and col2=val2;\n如果有一个由col1和col2组成的多列索引, 那么满足条件的行会快速被找到. 那么如果col1和col2分别采用单列索引那么优化器将会尝试合并索引,或者使用单个索引, 具体使用哪个索引取决于哪个索引能够排除更多的行.\n多列索引最左列优化多列索引中最左列会被优化器用于快速查找行, 假设有一个三列索引(col1, col2, col3), 那么(col1), (col1, col2)和(col1, col2, col3)这样的查询组合将会使用到索引.\n检查索引的使用正确性使用explain查看sql语句的执行计划, 可以了解索引是否被正确使用. 如果怀疑优化器没有按最佳的执行计划执行语句, 可以ANALYZE TABLE更新表的统计.\n","categories":["database"],"tags":["mysql"]}]